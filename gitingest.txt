Directory structure:
└── yanyepeng-cmbfscnn/
    ├── README.md
    ├── MANIFEST.in
    ├── PKG-INFO
    ├── setup.cfg
    ├── setup.py
    ├── cmbfscnn/
    │   ├── __init__.py
    │   ├── CMBFS.py
    │   ├── CMBFS_mode.py
    │   ├── CNN_models.py
    │   ├── get_power_sperctra.py
    │   ├── main.py
    │   ├── namaster.py
    │   ├── plottor.py
    │   ├── Results.py
    │   ├── simulate_sky_map.py
    │   ├── spherical.py
    │   └── utils.py
    ├── examples/
    │   ├── __init__.py
    │   ├── main.py
    │   └── Tutorial.py
    ├── pysm/
    │   ├── __init__.py
    │   ├── common.py
    │   ├── components.py
    │   ├── nominal.py
    │   ├── pysm.py
    │   ├── template/
    │   │   ├── __init__.py
    │   │   └── emissivity.txt
    │   └── test/
    │       ├── __init__.py
    │       ├── test_common.py
    │       ├── test_components.py
    │       ├── test_mpi_readmap.py
    │       └── test_pysm.py
    └── Utils_ILC/
        ├── __init__.py
        ├── smoothingTo.py
        └── ILC/
            ├── __init__.py
            ├── ILCbase.py
            └── needlet_data/
                └── needlet.csv

================================================
FILE: README.md
================================================
# CMBFSCNN: Cosmic Microwave Background Foreground Subtraction with Convolutional Neural Network



CMBFSCNN is  a method for component separation of CMB using CNN (convolutional neural network). It can efficiently remove the foregrounds of CMB temperature and polarization.





## Attribution

If you use this code or  find this code useful in your research,  please cite the following papers:

* CMBFSCNN: Cosmic Microwave Background Polarization Foreground Subtraction with Convolutional Neural Network, Ye-Peng Yan, Si-Yu Li, Guo-Jian Wang, Zirui Zhang, Jun-Qing Xia, 2024, arXiv:[2406.17685](https://arxiv.org/abs/2406.17685)
* Recovering Cosmic Microwave Background Polarization Signals with Machine Learning, Ye-Peng Yan, Guo-Jian Wang, Si-Yu Li, Jun-Qing Xia, 2023, ApJ, 947, 29. [doi:10.3847/1538-4357/acbfb4](https://iopscience.iop.org/article/10.3847/1538-4357/acbfb4)
* Recovering the CMB Signal with Machine Learning, Guo-Jian Wang, Hong-Liang Shi, Ye-Peng Yan, Jun-Qing Xia, Yan-Yun Zhao, Si-Yu Li, and Jun-Feng Li, 2022, ApJS, 260, 13. [doi:10.3847/1538-4365/ac5f4a](https://iopscience.iop.org/article/10.3847/1538-4365/ac5f4a)





## Requirements

Install the following packages:

- [PyTorch](https://pytorch.org/)
- [CUDA](https://developer.nvidia.com/cuda-downloads)
- [PySM](https://github.com/bthorne93/PySM_public): No need to install. It is included in CMBFSCNN.
- [CAMB](https://github.com/cmbant/CAMB)
- [Healpy](https://github.com/healpy/healpy)
- [NaMaster](https://github.com/LSSTDESC/NaMaster)







## Contributors

* Ye-Peng Yan
* Guo-Jian Wang
* Si-Yu Li

## Installing

You can install CMBFSCNN by using:

```
$ git clone https://github.com/yanyepeng/CMBFSCNN.git
$ cd CMBFSCNN
$ python setup.py install
```



## Quick Start

You can directly run the code by using

```
cd examples
python main.py 'config'
```

You can also modify the configuration file according to your needs.



In script `Tutorial.ipynb`, we provide a demo for use. 



The computational process of CMBFSCNN is divided into five steps: 

1) Simulating sky radiation data: We use the [PySM](https://github.com/bthorne93/PySM_public) software package to simulate foreground components, CMB, instrument beam, and white noise. We can simulate a large number of multi-frequency sky maps and divide them into training, validation, and testing sets. In this code, we also provide the computation of ILC noise for CMB polarization, which is smaller than the noise of each frequency band and can reduce the noise level in the CNN output.

2)  Data preprocessing: It involves transforming the healpix sky maps into two-dimensional flat sky maps for CNN processing. 

   ![figure1](images/figure1.png)

3) Establishing and training the CNN model. 

4) Predicting CMB maps from  the polluted sky-maps and calculating the relevant power spectra. 

5) Evaluating the CMBFSCNN method at the sky map and power spectra levels.






================================================
FILE: MANIFEST.in
================================================
recursive-include pysm/template *
recursive-include Utils_ILC/ILC/needlet_data *
recursive-include images/ *




================================================
FILE: PKG-INFO
================================================
Metadata-Version: 2.1
Name: cmbfscnn
Version: 0.0.1
Summary: a component separation of CMB using CNN 
Author: Ye-Peng Yan
Author-email: yanyepengphy@163.com
License: MIT
Platform: UNKNOWN
Requires-Python: >=3.8

a python command tool for camel case




================================================
FILE: setup.cfg
================================================
[egg_info]
tag_build = 
tag_date = 0




================================================
FILE: setup.py
================================================
# -*- coding: utf-8 -*-



import os
from setuptools import setup, find_packages

VERSION = '0.0.1'
def find_data_files(directory):
    """
    Using glob patterns in ``package_data`` that matches a directory can
    result in setuptools trying to install that directory as a file and
    the installation to fail.

    This function walks over the contents of *directory* and returns a list
    of only filenames found.
    """

    strip = os.path.dirname(os.path.abspath(__file__))

    result = []
    for root, dirs, files in os.walk(directory):
        for filename in files:
          filename = os.path.join(root, filename)
          result.append(os.path.relpath(filename, strip))

    print("\n".join(result))
    return result
#print('+++++++', find_data_files('pysm/template/'))
setup(name='cmbfscnn',
      version=VERSION,
      description="a component separation of CMB using CNN ",
      long_description='a python command tool for camel case',
      author='Ye-Peng Yan',
      author_email='yanyepengphy@163.com',
      license='MIT',
      packages=['cmbfscnn', 'pysm', 'examples','Utils_ILC','pysm/template','pysm/test','Utils_ILC/ILC'],
      include_package_data=True,
      zip_safe=False,
      python_requires = '>=3.8',)
      #data_files=[('pysm',find_data_files('pysm/template/')), ('Utils_ILC/ILC',find_data_files('Utils_ILC/ILC/needlet_data'))]
      #)






================================================
FILE: cmbfscnn/__init__.py
================================================

# -*- coding: utf-8 -*-

#from .CNN_models import CMBFSCNN_level3, CMBFSCNN_level4, UNet
#from .generate_data import Data_preprocessing, Load_data, Simulator_data
#from .utils import utils
#from .CMBFS_mode import Foreground_subtraction_model, Calculate_power_spectra,Result_analysis
#from .Results import Plot_results
#from .spherical import spherical




================================================
FILE: cmbfscnn/CMBFS.py
================================================

# -*- coding: utf-8 -*-


import numpy as np
from . import CMBFS_mode as Cmo
from . import generate_data as gd
from . import Results as Re

class CMBFSCNN(object):
    def __init__(self, config):
        self.config = config
        self._get_config

    @property
    def _get_config(self):
        for key, value in self.config.items():
            setattr(self, key, value)

        self.data_index = [np.arange(self.N_sky_maps[0]), np.arange(self.N_sky_maps[1], np.sum(self.N_sky_maps[0:2])),
                           np.arange(np.sum(self.N_sky_maps[0:2]), np.sum(self.N_sky_maps))]
        self.noise_index = [np.arange(i) for i in self.N_noise_maps]


    def data_simulation(self):

        Sim_data = gd.Simulator_data(freqs=self.freqs, beams=self.beams, noise_level=self.sens,
                                  is_half_split_map=self.is_half_split_map, using_ilc_cmbmap=self.using_ilc_cmbmap,
                                  Nside=self.nside, config_random=self.data_config_random, save_dir=self.save_data_dir,
                                  is_polarization_data=self.is_polarization_data)
        Sim_data.simulator_sky_map(np.sum(self.N_sky_maps))

        for data_type, N_noise in zip(self.dataset_type, self.N_noise_maps):
            Sim_data.simulator_noise_map(N_noise_map = N_noise, dataset_type=data_type)

        for k, data_type in enumerate(self.dataset_type):

            Sim_data.get_observed_map(index_sky_map = self.data_index[k], index_noise_map = self.noise_index[k],
                                      dataset_type = data_type)

    def cal_ilc_noise(self):
        Sim_data = gd.Simulator_data(freqs=self.freqs, beams=self.beams, noise_level=self.sens,
                                  is_half_split_map=self.is_half_split_map, using_ilc_cmbmap=self.using_ilc_cmbmap,
                                  Nside=self.nside, config_random=self.data_config_random, save_dir=self.save_data_dir,
                                  is_polarization_data=self.is_polarization_data)

        if self.using_ilc_cmbmap:
            for k, data_type in enumerate(self.dataset_type):
                Sim_data.mult_process_get_ilc_noise(N_sample = self.N_sky_maps[k], N_mult = self.ILC_N_threads, out_freq = self.output_freq, mask = self.ilc_mask,
                                                    dataset_type=data_type)
            for k, data_type in enumerate(self.dataset_type):
                if data_type == 'traing_set':
                    Sim_data.get_data_CMB_ilcnoise(index_sky_map=self.data_index[k],
                                                   index_ilcnoise_map=self.data_index[k],
                                                                                dataset_type=data_type,
                                                                                out_beam=self.output_beam)
                else:
                    Sim_data.get_data_CMB_ilcnoise(index_sky_map = self.data_index[k],
                                                   index_ilcnoise_map = self.noise_index[k],
                                                   dataset_type = data_type, out_beam = self.output_beam)


    def data_preprocessing(self):
        Data_prep = gd.Data_preprocessing(save_dir = self.save_data_dir, padding = self.padding, is_half_split_map = self.is_half_split_map,
                                       nside = self.nside, block_number = self.block_n, full_sky_map = self.is_fullsky,using_ilc_cmbmap=self.using_ilc_cmbmap)
        for k, data_type in enumerate(self.dataset_type):
            Data_prep.mult_process_get_flatmap_from_spheremap(N_mult = self.N_threads_preprocessing, N_sample = self.N_sky_maps[k], dataset_type = data_type)


    def _training_cnn(self, component = 'Q', using_loss_fft = True, repeat_n = 3):
        FSM_Qmap = Cmo.Foreground_subtraction_model(data_dir = self.save_data_dir, freqs = self.freqs, output_freq = self.output_freq,
                                                component = component, full_sky_map = self.is_fullsky,
                                                is_half_split_map = self.is_half_split_map, using_ilc_cmbmap = self.using_ilc_cmbmap,
                                                result_dir = self.save_result_dir)
        FSM_Qmap.net_train(batch_size = self.batch_size, learning_rate = self.learning_rate, num_train = self.N_sky_maps[0],
                           num_validation = self.N_sky_maps[1], device_ids = self.device_ids,
                           iteration = self.iteration,
                           CNN_model = self.CNN_model, using_loss_fft = using_loss_fft, repeat_n = repeat_n)
        FSM_Qmap.plot_train_records()  # Plot Training History

    def training_cnn(self):
        for st in self.component:
            self._training_cnn(st)


    def get_predicted_maps(self):
        RA = Cmo.Result_analysis(data_dir = self.save_data_dir, full_sky_map = self.is_fullsky, map_block = self.block_n, padding = self.padding,
                             using_ilc_cmbmap = self.using_ilc_cmbmap, nside = self.nside,
                             is_half_split_map = self.is_half_split_map, result_dir = self.save_result_dir, freqs = self.freqs,
                             output_freq = self.output_freq)
        for st in self.component:
            RA.prediction(num_testset = self.N_sky_maps[2], is_return=False, comp = st)

        RA.get_true_CMB_map(index_sky_map = self.data_index[2])

    def calculate_power_spectra(self, nlb=5, Dl = True):
        CPS = Cmo.Calculate_power_spectra(result_dir = self.save_result_dir, is_half_split_map = self.is_half_split_map,
                                      beams_arcmin = self.beams, output_beam_arcmin = self.output_beam, N_sample = self.N_sky_maps[2],
                                      nside = self.nside, component = self.component)

        if 'T' in self.component:
            CPS.get_true_CMB_PS(nlb=nlb, Dl=Dl, aposize=None, EB_power=False)  # get true CMB TT and TT power spectra
            CPS.cal_cmb_T_PS(nlb=nlb, Dl=Dl, aposize=None, EB_power=False)  # get output CMB TT and TT power spectra
            if self.is_half_split_map:
                CPS.cal_cmb_T_cross_PS(nlb=nlb, Dl=Dl, aposize=None,
                                       EB_power=False)  # get output CMB TT and TT power spectra after denoising step
        if 'Q'in self.component and 'U'in self.component:
            CPS.get_true_CMB_PS(nlb = nlb, Dl = Dl, aposize = None, EB_power = False)  # get true CMB QQ and UU power spectra
            CPS.get_true_CMB_PS(nlb = nlb, Dl = Dl, aposize = None, EB_power = True)  # get true CMB EE and BB power spectra
            CPS.cal_cmb_Q_or_U_PS(nlb = nlb, Dl = Dl, aposize = None, EB_power=False)  # get output CMB QQ and UU power spectra
            CPS.cal_cmb_Q_or_U_cross_PS(nlb = nlb, Dl = Dl, aposize = None,
                                        EB_power = False)  # get output CMB QQ and UU power spectra after denoising step
            CPS.cal_cmb_E_B_PS(nlb = nlb, Dl = Dl, aposize = None, EB_power = True)  # get output CMB EE and BB power spectra
            CPS.cal_cmb_E_B_cross_PS(nlb = nlb, Dl = Dl, aposize = None,
                                     EB_power = True)  # get output CMB EE and BB power spectra after denoising step

    def _plot_results(self):
        PR = Re.Plot_results(result_dir = self.save_result_dir, is_half_split_map = self.is_half_split_map)
        PR.plot_predicted_sphere_map()
        PR.plot_predicted_flat_map()
        PR.plot_recovered_CMB_QU_PS()
        PR.plot_recovered_CMB_EB_PS()


    def run_CMBFSCNN(self):
        self.data_simulation()
        if self.using_ilc_cmbmap:
            self.cal_ilc_noise()
        self.data_preprocessing()
        self.training_cnn()
        self.get_predicted_maps()
        self.calculate_power_spectra()
        self._plot_results()

























================================================
FILE: cmbfscnn/CMBFS_mode.py
================================================

# -*- coding: utf-8 -*-



import torch
from . import utils
from . import CNN_models as Cm
from . import generate_data as gd
import numpy as np
from tqdm import tqdm
import torch.nn as nn
import matplotlib.pyplot as plt


class Foreground_subtraction_model(gd.Data_preprocessing):
    def __init__(self, data_dir='DATA/', freqs = np.array([]), output_freq = 220, component ="Q", full_sky_map = False,
                 is_half_split_map=True, using_ilc_cmbmap=True, result_dir = 'DATA_results/'):

        if not data_dir is None:
            self._save_dir = data_dir
            self._cread_file_name
        super(gd.Data_preprocessing, self).__init__()
        self.result_dir = result_dir
        self.freqs = freqs
        self.output_freq = output_freq
        self.component = component
        self.full_sky_map = full_sky_map
        self.is_half_split_map = is_half_split_map
        self.using_ilc_cmbmap = using_ilc_cmbmap
        self._creat_file_n
        self.l1_loss = torch.nn.L1Loss()

    @property
    def output_freq_index(self):
        return np.where(self.freqs==self.output_freq)[0][0]


    @property
    def _creat_file_n(self):
        dir_key = {}
        dir_key['records_Tmap_dir'] = self.result_dir + 'training_records_T_comp/'
        dir_key['records_Qmap_dir'] = self.result_dir+'training_records_Q_comp/'
        dir_key['records_Umap_dir'] = self.result_dir + 'training_records_U_comp/'
        dir_key['output_Tmap_dir'] = self.result_dir + 'output_T_comp/'
        dir_key['output_Qmap_dir'] = self.result_dir + 'output_Q_comp/'
        dir_key['output_Umap_dir'] = self.result_dir + 'output_U_comp/'

        for key, value in dir_key.items():
            setattr(self, key, value)



    def LrDecay(self, iteration, ith_iteration, lr=0.1, lr_min=1e-7):
        gamma = (lr_min / lr) ** (1. / iteration)
        return lr * gamma ** ith_iteration


    def loss_fft(self, cmb_false, cmb_true, lambda_l=0.5):
        cmb_false_fft = torch.abs(torch.fft.fftn(cmb_false, dim=(2, 3))) / cmb_true.shape[3]
        cmb_true_fft = torch.abs(torch.fft.fftn(cmb_true, dim=(2, 3))) / cmb_true.shape[3]
        return lambda_l * self.l1_loss(cmb_true_fft, cmb_false_fft)

    def loss_function(self, output, target, using_loss_fft = True):
        if using_loss_fft:
            return self.l1_loss(output, target) + self.loss_fft(output, target, lambda_l = 1)
        else:
            return self.l1_loss(output, target)



    def cnn_m(self, model_name):
        if model_name == 'CMBFSCNN_level3':
            self._net = Cm.CMBFSCNN_level3(in_channels = len(self.freqs), out_channels = 1, n_feats = 16)
        elif model_name == 'CMBFSCNN_level4':
            self._net = Cm.CMBFSCNN_level4(in_channels = len(self.freqs), out_channels = 1, n_feats = 16)
        elif model_name == 'UNet':
            self._net = Cm.UNet(in_channels = len(self.freqs), out_channels = 1)
        else:
            print("Error: Please set model_name correctly. model_name = 'CMBFSCNN_level3' or 'CMBFSCNN_level4' or UNet ")

    def MAD(self, out, target):
        return np.mean(abs(out-target))

    def _load_batch_data(self, index_sample, data_tyep = 'training_set', batch_size=None, half_sp = 1, comp=None):
        '''

        :param index_sample: map index in data_set
        :param data_tyep: dataset_type = 'traing_set' or 'validation_set' or 'testing_set'
        :return:
        '''
        if comp is None:
            comp = self.component
        if batch_size is None:
            batch_size = self.batch_size

        if batch_size >len(index_sample):
            batch_index = np.random.choice(index_sample, batch_size, replace=True)  # Note: replace=False
        else:
            batch_index = np.random.choice(index_sample, batch_size, replace=False)
        if data_tyep == 'training_set':
            if self.full_sky_map:
                if self.using_ilc_cmbmap:
                    cmb_dir = getattr(self, 'dir_cmb_ilc_flat_{}'.format(data_tyep[0:3]))
                else:
                    cmb_dir = getattr(self, 'dir_cmb_obs_flat_{}'.format(data_tyep[0:3]))
                total_dir = getattr(self, 'dir_total_obs_flat_{}'.format(data_tyep[0:3]))
            else:
                if self.using_ilc_cmbmap:
                    cmb_dir = getattr(self, 'dir_cmb_ilc_flat_block_{}'.format(data_tyep[0:3]))
                else:
                    cmb_dir = getattr(self, 'dir_cmb_obs_flat_block_{}'.format(data_tyep[0:3]))
                total_dir = getattr(self, 'dir_total_obs_flat_block_{}'.format(data_tyep[0:3]))
            load = gd.Load_data(map_nums=batch_index, component=comp,
                                               input_dir=total_dir, target_dir=cmb_dir,
                                               output_freq=self.output_freq_index)
            cmb_batch, total_batch = load.data()
        else:
            if self.full_sky_map:
                if self.is_half_split_map:
                    if self.using_ilc_cmbmap:
                        cmb_dir = getattr(self, 'dir_cmb_ilc_flat_{}_{}'.format(data_tyep[0:3],half_sp))
                    else:
                        cmb_dir = getattr(self, 'dir_cmb_obs_flat_{}_{}'.format(data_tyep[0:3], half_sp))
                    total_dir = getattr(self, 'dir_total_obs_flat_{}_{}'.format(data_tyep[0:3],half_sp))
                else:
                    if self.using_ilc_cmbmap:
                        cmb_dir = getattr(self, 'dir_cmb_ilc_flat_{}'.format(data_tyep[0:3]))
                    else:
                        cmb_dir = getattr(self, 'dir_cmb_obs_flat_{}'.format(data_tyep[0:3]))
                    total_dir = getattr(self, 'dir_total_obs_flat_{}'.format(data_tyep[0:3]))
            else:
                if self.is_half_split_map:
                    if self.using_ilc_cmbmap:
                        cmb_dir = getattr(self, 'dir_cmb_ilc_flat_block_{}_{}'.format(data_tyep[0:3], half_sp))
                    else:
                        cmb_dir = getattr(self, 'dir_cmb_obs_flat_block_{}_{}'.format(data_tyep[0:3], half_sp))
                    total_dir = getattr(self, 'dir_total_obs_flat_block_{}_{}'.format(data_tyep[0:3], half_sp))
                else:
                    if self.using_ilc_cmbmap:
                        cmb_dir = getattr(self, 'dir_cmb_ilc_flat_block_{}'.format(data_tyep[0:3]))
                    else:
                        cmb_dir = getattr(self, 'dir_cmb_obs_flat_block_{}'.format(data_tyep[0:3]))
                    total_dir = getattr(self, 'dir_total_obs_flat_block_{}'.format(data_tyep[0:3]))
            load = gd.Load_data(map_nums=batch_index, component=comp,
                                               input_dir=total_dir, target_dir=cmb_dir,
                                               output_freq=self.output_freq_index)
            cmb_batch, total_batch = load.data()

        return cmb_batch, total_batch


    def net_train(self, batch_size=12, learning_rate=0.01, num_train=1000, num_validation = 200, device_ids = [0,1], iteration=3e4,
                  CNN_model='CMBFSCNN_level3', using_loss_fft=True, repeat_n=3):
        self.records_dir = getattr(self, 'records_{}map_dir'.format(self.component))
        self._creat_file(self.records_dir)
        iteration = int(iteration)
        self.batch_size = batch_size
        if iteration<100:
            ns = 2
        elif iteration<1000:
            ns = 10
        elif iteration<10000:
            ns = 100
        elif iteration>10000:
            ns = 500
        device = torch.device("cuda:{}".format(device_ids[0]) if torch.cuda.is_available() else "cpu")
        self.cnn_m(CNN_model)
        self._net = nn.DataParallel(self._net.to(device), device_ids=device_ids)
        optimizer = torch.optim.Adam(self._net.parameters(), lr=learning_rate, weight_decay=0)
        pbar = tqdm(total=iteration, miniters=2)
        pbar.set_description('Training CNN model for {} map'.format(self.component))
        train_loss, valid_loss, train_MAD, valid_MAD, ite = [], [], [], [], []
        index_tra_samp = np.arange(num_train)
        index_val_samp = np.arange(num_validation)
        for ith_iteration in range(1, iteration + 1):
            input, target = self._load_batch_data(index_sample = index_tra_samp, data_tyep = 'training_set')
            input, target = input.to(device), target.to(device)

            for t in range(repeat_n):
                out = self._net(input)

                loss = self.loss_function(out, target,using_loss_fft = using_loss_fft)
                optimizer.zero_grad()  #
                loss.backward()  #
                optimizer.step()  #
            new_lr = self.LrDecay(iteration=iteration, ith_iteration=ith_iteration, lr=learning_rate)
            optimizer.param_groups[0]['lr'] = new_lr
            pbar.update(1)
            if ith_iteration % ns == 0:
                input_valid, target_valid = self._load_batch_data(index_sample = index_val_samp, data_tyep='validation_set')
                input_valid = input_valid.to(device)
                self._net.eval()
                output_valid = self._net(input_valid)
                output_valid = output_valid.data.cpu()
                val_loss = self.loss_function(output_valid, target_valid,using_loss_fft = using_loss_fft)
                ite.append(ith_iteration)
                train_loss.append(loss.item())
                valid_loss.append(val_loss.item())
                tra_MAD = self.MAD(out.data.cpu().numpy(), target.cpu().numpy())
                val_MAD = self.MAD(output_valid.numpy(), target_valid.numpy())
                train_MAD.append(tra_MAD)
                valid_MAD.append(val_MAD)
                self._net.train()
                print(
                    "Iteration: [%d/%d] training loss: %.6f validation_loss: %.6f mean MAD in train set: %.6f mean MAD in valid set: %.6f " %
                    (ith_iteration + 1, iteration, loss.item(), val_loss.item(), tra_MAD, val_MAD))

                torch.save(self._net, self.records_dir+'net_%s.pkl' % (ith_iteration))

                tra_log = {'iter': ite, 'train_loss': train_loss, 'valid_loss': valid_loss, 'train_MAD': train_MAD,
                          'valid_MAD': valid_MAD}
                utils.save_pkl(tra_log, self.records_dir + 'train_log_%s' % (ith_iteration))

        torch.save(self._net, self.records_dir + 'net.pkl')
        utils.save_pkl(tra_log, self.records_dir + 'train_log')


    def _predictor(self, input_test, target_test, net = None):
        net.eval()
        out = net(input_test)
        out = out.cpu()
        out = torch.squeeze(out, 0)
        out = torch.squeeze(out, 0).detach().numpy()
        target_test = torch.squeeze(target_test, 0)
        target_test = torch.squeeze(target_test, 0)
        target_test = target_test.detach().numpy()
        return out, target_test




    def plot_train_records(self, train_log = None, train_log_dir = None):
        if train_log is None:
            if train_log_dir is None:
                train_log_dir = self.records_dir + 'train_log'
            train_log = utils.load_pkl(train_log_dir)
        self.records_dir = getattr(self, 'records_{}map_dir'.format(self.component))
        iter, train_loss, valid_loss, train_MAD, valid_MAD  = train_log['iter'], train_log['train_loss'], train_log['valid_loss'], train_log['train_MAD'], train_log['valid_MAD']
        fig, axs = plt.subplots(1, 2, figsize=(16, 8))
        ax1 = axs[0]
        ax1.scatter(iter, train_loss, c='r', s=3, lw=2)
        ax1.plot(iter, train_loss, c='r', lw=2, label='training loss')
        ax1.scatter(iter, valid_loss, c='c', s=3, lw=2)
        ax1.plot(iter, valid_loss, c='c', lw=2, label='validation loss')
        ax1.legend(loc='best', fontsize=16)
        ax1.set_xlabel('Iteration', fontsize=18)
        ax1.set_ylabel('Loss Function', fontsize=18)
        ax1.tick_params(labelsize=16)
        ax1.spines['bottom'].set_linewidth(1.5)
        ax1.spines['left'].set_linewidth(1.5)
        ax1.spines['top'].set_linewidth(1.5)
        ax1.spines['right'].set_linewidth(1.5)
        ax1.tick_params(which='major', length=8, direction='in', width=1.5)
        ax1.tick_params(which='minor', length=4, direction='in', width=1.5)
        ax1.tick_params(which='both', width=1.5, right='on')
        ax1.tick_params(which='both', width=1.5, top='on')

        ax2 = axs[1]
        ax2.scatter(iter, train_MAD, c='r', s=3, lw=2)
        ax2.plot(iter, train_MAD, c='r', lw=2, label='training MAD', )
        ax2.scatter(iter, valid_MAD, c='k', s=3, lw=2)
        ax2.plot(iter, valid_MAD, c='k', lw=2, label='validation MAD')
        ax2.legend(loc='best', fontsize=16)
        ax2.set_xlabel('Iteration', fontsize=18)
        ax2.set_ylabel('Mean Absolute Deviation', fontsize=18)
        ax2.tick_params(labelsize=16)
        ax2.spines['bottom'].set_linewidth(1.5)
        ax2.spines['left'].set_linewidth(1.5)
        ax2.spines['top'].set_linewidth(1.5)
        ax2.spines['right'].set_linewidth(1.5)
        # ax1.axis['bottom', 'left'].label.set_fontsize(16)
        # ax1.axis['bottom', 'left'].major_ticklabels.set_fontsize(16)
        ax2.tick_params(which='major', length=8, direction='in', width=1.5)
        ax2.tick_params(which='minor', length=4, direction='in', width=1.5)
        ax2.tick_params(which='both', width=1.5, right='on')
        ax2.tick_params(which='both', width=1.5, top='on')

        plt.savefig(self.records_dir+'plot_acc.png')
        # plt.show()


class Result_analysis(Foreground_subtraction_model):
    def __init__(self, data_dir='DATA/',  full_sky_map = False, map_block = 'block_0', padding = True, using_ilc_cmbmap=False,
                 is_half_split_map=True, result_dir = 'DATA_results/',freqs = np.array([]),  output_freq = 220, nside=512):
        if not data_dir is None:
            self._save_dir = data_dir
            self._cread_file_name
        self.result_dir = result_dir
        self.full_sky_map = full_sky_map
        self.is_half_split_map = is_half_split_map
        self.map_block  = map_block
        self.padding = padding
        self.freqs = freqs
        self.output_freq = output_freq
        self.using_ilc_cmbmap = using_ilc_cmbmap
        self.nside = nside
        self._creat_file_n


    def predictor(self, net_dir, num_testset=300, comp='Q'):

        net_dir = net_dir + 'net.pkl'
        net = torch.load(net_dir)
        if self.is_half_split_map:
            pbar = tqdm(total=num_testset, miniters=1)
            pbar.set_description('Predicting the CMB {} map'.format(comp))
            for n  in range(num_testset):

                input_test_1, target_test_1 = self._load_batch_data(index_sample=[n], data_tyep='testing_set', batch_size = 1, half_sp = 1, comp =comp)
                input_test_2, target_test_2 = self._load_batch_data(index_sample=[n], data_tyep='testing_set',
                                                                     batch_size=1, half_sp=2, comp = comp)
                if n==0:
                    out_1, target_1 = self._predictor(input_test_1, target_test_1, net)
                    out_2, target_2 = self._predictor(input_test_2, target_test_2, net)
                    out_1, target_1, out_2, target_2 = out_1[None,:], target_1[None,:], out_2[None,:], target_2[None,:]
                else:
                    out_1i, target_1i = self._predictor(input_test_1, target_test_1, net)
                    out_2i, target_2i = self._predictor(input_test_2, target_test_2, net)
                    out_1 = np.r_[out_1, out_1i[None,:]]
                    out_2 = np.r_[out_2, out_2i[None,:]]
                    target_1 = np.r_[target_1, target_1i[None,:]]
                    target_2 = np.r_[target_2, target_2i[None,:]]
                pbar.update(1)
            return out_1, out_2, target_1, target_2

        else:
            for n in range(num_testset):
                pbar = tqdm(total=num_testset, miniters=1)
                pbar.set_description('Predicting the CMB {} map'.format(comp))
                input_test, target_test = self._load_batch_data(index_sample=[n], data_tyep='testing_set',
                                                                 batch_size=1,  comp=comp)
                if n == 0:
                    out, target = self._predictor(input_test, target_test, net)
                    out, target = out[None,:], target[None,:]
                else:
                    out_i, target_i = self._predictor(input_test, target_test, net)
                    out = np.r_[out, out_i[None,:]]
                    target = np.r_[target, target_i[None,:]]
                pbar.update(1)
            return out, target

    def prediction(self, num_testset=300, comp = 'TQU', is_get_sphere_map = True, is_return = False):

        for str in comp:
            net_dir = getattr(self, 'records_{}map_dir'.format(str))
            output_dir = getattr(self, 'output_{}map_dir'.format(str))
            self._creat_file(output_dir)
            if self.is_half_split_map:
                out_1, out_2, target_1, target_2 = self.predictor(net_dir = net_dir, num_testset = num_testset, comp = str)
                np.save(output_dir + 'predicted_CMB_' + str + '_map_half_1.npy', (out_1).astype(np.float32))
                np.save(output_dir + 'predicted_CMB_' + str + '_map_half_2.npy', (out_2).astype(np.float32))
                np.save(output_dir + 'target_CMB_' + str + '_map_half_1.npy', (target_1).astype(np.float32))
                np.save(output_dir + 'target_CMB_' + str + '_map_half_2.npy', (target_2).astype(np.float32))
                if is_get_sphere_map:
                    out_s1 = self.sphere_from_flat_map(flat_map=out_1, is_fullsky = self.full_sky_map, is_padding = self.padding, block = self.map_block)
                    out_s2 = self.sphere_from_flat_map(flat_map=out_2, is_fullsky=self.full_sky_map,
                                                      is_padding=self.padding, block=self.map_block)
                    target_s1 = self.sphere_from_flat_map(flat_map=target_1, is_fullsky=self.full_sky_map,
                                                      is_padding=self.padding, block=self.map_block)
                    target_s2 = self.sphere_from_flat_map(flat_map=target_2, is_fullsky=self.full_sky_map,
                                                      is_padding=self.padding, block=self.map_block)
                    np.save(output_dir + 'predicted_sphere_CMB_' + str + '_map_half_1.npy', (out_s1).astype(np.float32))
                    np.save(output_dir + 'predicted_sphere_CMB_' + str + '_map_half_2.npy', (out_s2).astype(np.float32))
                    np.save(output_dir + 'target_sphere_CMB_' + str + '_map_half_1.npy', (target_s1).astype(np.float32))
                    np.save(output_dir + 'target_sphere_CMB_' + str + '_map_half_2.npy', (target_s2).astype(np.float32))
                if is_return:
                    if is_get_sphere_map:
                        return out_1, out_2, target_1, target_2, out_s1, out_s2, target_s1, target_s2
                    else:
                        return out_1, out_2, target_1, target_2
            else:
                out, target = self.predictor(net_dir=net_dir, num_testset=num_testset, comp=str)
                np.save(output_dir + 'predicted_CMB_' + str + '_map.npy', (out).astype(np.float32))
                np.save(output_dir + 'target_CMB_' + str + '_map.npy', (target).astype(np.float32))
                if is_get_sphere_map:
                    out_s = self.sphere_from_flat_map(flat_map=out, is_fullsky=self.full_sky_map,
                                                       is_padding=self.padding, block=self.map_block)
                    target_s = self.sphere_from_flat_map(flat_map=target, is_fullsky=self.full_sky_map,
                                                          is_padding=self.padding, block=self.map_block)
                    np.save(output_dir + 'predicted_sphere_CMB_' + str + '_map.npy', (out_s).astype(np.float32))
                    np.save(output_dir + 'target_sphere_CMB_' + str + '_map.npy', (target_s).astype(np.float32))
                if is_return:
                    if is_get_sphere_map:
                        return out, target, out_s, target_s
                    else:
                        return out, target

    def get_sphere_predicted_map(self, comp = 'TQU',num_testset=300):
        for str in comp:
            output_dir = getattr(self, 'output_{}map_dir'.format(str))
            self._creat_file(output_dir)
            try:
                out_1 = np.load(output_dir + 'predicted_CMB' + str + '_map_half_1.npy')
            except:
               self.prediction(self, num_testset=num_testset, comp = str, is_get_sphere_map = True, is_return = False)

    def get_true_CMB_map(self, index_sky_map):
        pbar = tqdm(total=len(index_sky_map), miniters=1)
        pbar.set_description('Geting the True CMB map')

        for n, i in enumerate(index_sky_map):
            if n==0:
                cmb = np.load(self.dir_cmb + 'cmb' + str(i) + '.npy')[self.output_freq_index,:]
                cmb = cmb[None,:]
            else:
                cmb_i = np.load(self.dir_cmb + 'cmb' + str(i) + '.npy')[self.output_freq_index,:]
                cmb = np.r_[cmb, cmb_i[None,:]]
            pbar.update(1)
        if cmb.shape[1] ==2:
            output_Qdir = getattr(self, 'output_Qmap_dir')
            output_Udir = getattr(self, 'output_Umap_dir')
            self._creat_file(output_Qdir)
            self._creat_file(output_Udir)
            np.save(output_Qdir + 'true_cmb_Q_map' + '.npy', (cmb[:,0,:]).astype(np.float32))
            np.save(output_Udir + 'true_cmb_U_map' + '.npy', (cmb[:,1, :]).astype(np.float32))
        else:
            output_Tdir = getattr(self, 'output_Tmap_dir')
            output_Qdir = getattr(self, 'output_Qmap_dir')
            output_Udir = getattr(self, 'output_Umap_dir')
            self._creat_file(output_Tdir)
            self._creat_file(output_Qdir)
            self._creat_file(output_Udir)
            np.save(output_Tdir + 'true_cmb_T_map' + '.npy', (cmb[:, 0, :]).astype(np.float32))
            np.save(output_Qdir + 'true_cmb_Q_map' + '.npy', (cmb[:, 1, :]).astype(np.float32))
            np.save(output_Udir + 'true_cmb_U_map' + '.npy', (cmb[:, 2, :]).astype(np.float32))




class Calculate_power_spectra(Result_analysis):
    def __init__(self,  result_dir = 'DATA_results/',is_half_split_map=True, component = 'Q',
                 beams_arcmin = np.array([]), output_beam_arcmin = 220, N_sample=10, nside = 512):
        # super(Calculate_power_spectra, self).__init__()
        self.is_half_split_map= is_half_split_map
        self.result_dir = result_dir
        self._creat_file_n
        self.beams = beams_arcmin
        self.output_beam = output_beam_arcmin
        self.N_sample = N_sample
        self.nside = nside
        self.component = component
        self._creat_file(self.save_PS_dir)
        self._creat_ps_file

    @property
    def save_PS_dir(self):
        return self.result_dir + 'output_PS/'

    @property
    def _creat_ps_file(self):
        dir_key = {}
        Tfile_dir = getattr(self, 'output_Tmap_dir')
        Qfile_dir = getattr(self, 'output_Qmap_dir')
        Ufile_dir = getattr(self, 'output_Umap_dir')
        dir_key['true_T_dir'] = Tfile_dir + 'true_cmb_T_map' + '.npy'
        dir_key['true_Q_dir'] = Qfile_dir + 'true_cmb_Q_map' + '.npy'
        dir_key['true_U_dir'] = Ufile_dir + 'true_cmb_U_map' + '.npy'
        dir_key['output_T_dir'] = Tfile_dir + 'predicted_sphere_CMB_T' + '_map.npy'
        dir_key['output_Q_dir'] = Qfile_dir + 'predicted_sphere_CMB_Q' + '_map.npy'
        dir_key['output_U_dir'] = Ufile_dir + 'predicted_sphere_CMB_U' + '_map.npy'
        dir_key['output_T_dir_1'] = Tfile_dir + 'predicted_sphere_CMB_T' + '_map_half_{}.npy'.format(1)
        dir_key['output_T_dir_2'] = Tfile_dir + 'predicted_sphere_CMB_T' + '_map_half_{}.npy'.format(2)
        dir_key['output_Q_dir_1'] = Qfile_dir + 'predicted_sphere_CMB_Q' + '_map_half_{}.npy'.format(1)
        dir_key['output_Q_dir_2'] = Qfile_dir + 'predicted_sphere_CMB_Q' + '_map_half_{}.npy'.format(2)
        dir_key['output_U_dir_1'] = Ufile_dir + 'predicted_sphere_CMB_U' + '_map_half_{}.npy'.format(1)
        dir_key['output_U_dir_2'] = Ufile_dir + 'predicted_sphere_CMB_U' + '_map_half_{}.npy'.format(2)
        dir_key['target_Q_dir'] = Qfile_dir + 'target_sphere_CMB_Q' + '_map.npy'
        dir_key['target_T_dir'] = Tfile_dir + 'target_sphere_CMB_T' + '_map.npy'
        dir_key['target_U_dir'] = Ufile_dir + 'target_sphere_CMB_U' + '_map.npy'
        dir_key['target_T_dir_1'] = Tfile_dir + 'target_sphere_CMB_T' + '_map_half_{}.npy'.format(1)
        dir_key['target_T_dir_2'] = Tfile_dir + 'target_sphere_CMB_T' + '_map_half_{}.npy'.format(2)
        dir_key['target_Q_dir_1'] = Qfile_dir + 'target_sphere_CMB_Q' + '_map_half_{}.npy'.format(1)
        dir_key['target_Q_dir_2'] = Qfile_dir + 'target_sphere_CMB_Q' + '_map_half_{}.npy'.format(2)
        dir_key['target_U_dir_1'] = Ufile_dir + 'target_sphere_CMB_U' + '_map_half_{}.npy'.format(1)
        dir_key['target_U_dir_2'] = Ufile_dir + 'target_sphere_CMB_U' + '_map_half_{}.npy'.format(2)

        dir_key['true_output_T_dir'] = self.save_PS_dir + 'true_cmb_T_PS_nlb_{}.npy'
        dir_key['true_output_Q_dir'] = self.save_PS_dir+'true_cmb_Q_PS_nlb_{}.npy'
        dir_key['true_output_U_dir'] = self.save_PS_dir + 'true_cmb_U_PS_nlb_{}.npy'

        dir_key['save_output_Q_dir_1'] = self.save_PS_dir + 'output_cmb_Q_PS_nlb_{}_half_1.npy'
        dir_key['save_output_Q_dir_2'] = self.save_PS_dir + 'output_cmb_Q_PS_nlb_{}_half_2.npy'
        dir_key['save_output_T_dir_1'] = self.save_PS_dir + 'output_cmb_T_PS_nlb_{}_half_1.npy'
        dir_key['save_output_T_dir_2'] = self.save_PS_dir + 'output_cmb_T_PS_nlb_{}_half_2.npy'
        dir_key['save_output_U_dir_1'] = self.save_PS_dir + 'output_cmb_U_PS_nlb_{}_half_1.npy'
        dir_key['save_output_U_dir_2'] = self.save_PS_dir + 'output_cmb_U_PS_nlb_{}_half_2.npy'

        dir_key['save_target_T_dir_1'] = self.save_PS_dir + 'target_cmb_T_PS_nlb_{}_half_1.npy'
        dir_key['save_target_T_dir_2'] = self.save_PS_dir + 'target_cmb_T_PS_nlb_{}_half_2.npy'
        dir_key['save_target_Q_dir_1'] = self.save_PS_dir + 'target_cmb_Q_PS_nlb_{}_half_1.npy'
        dir_key['save_target_Q_dir_2'] = self.save_PS_dir + 'target_cmb_Q_PS_nlb_{}_half_2.npy'
        dir_key['save_target_U_dir_1'] = self.save_PS_dir + 'target_cmb_U_PS_nlb_{}_half_1.npy'
        dir_key['save_target_U_dir_2'] = self.save_PS_dir + 'target_cmb_U_PS_nlb_{}_half_2.npy'

        dir_key['save_output_cros_T_dir'] = self.save_PS_dir + 'output_cmb_T_cross_PS_nlb_{}.npy'
        dir_key['save_output_cros_Q_dir'] = self.save_PS_dir + 'output_cmb_Q_cross_PS_nlb_{}.npy'
        dir_key['save_output_cros_U_dir'] = self.save_PS_dir + 'output_cmb_U_cross_PS_nlb_{}.npy'

        dir_key['save_output_EB_dir'] = self.save_PS_dir + 'output_cmb_EB_PS_nlb_{}.npy'
        dir_key['save_target_EB_dir'] = self.save_PS_dir + 'target_cmb_EB_PS_nlb_{}.npy'

        dir_key['save_true_EB_dir'] = self.save_PS_dir + 'true_cmb_EB_PS_nlb_{}.npy'

        dir_key['save_output_cros_EB_dir'] = self.save_PS_dir + 'output_cmb_EB_cross_PS_nlb_{}.npy'


        for key, value in dir_key.items():
            setattr(self, key, value)

    def __power_for_Nsample_from_spheremap(self,data_Q1_dir=None, data_Q2_dir = None,  data_U1_dir=None, data_U2_dir = None,
                                            nlb=5, Dl=True, aposize=None, EB_power=False):
        beam = utils.get_Bl(self.output_beam, nside=self.nside)

        Dl_gene = utils.Get_power(data_Q1_dir=data_Q1_dir, data_Q2_dir=data_Q2_dir, data_U1_dir=data_U1_dir, data_U2_dir=data_U2_dir, block_n=None, nside=self.nside,
                            beam_file=beam, nlb=nlb, Dl=Dl, aposize=aposize)
        Dl = Dl_gene.get_N_power_from_spheremap(N_sample=np.arange(self.N_sample), EB_power=EB_power)
        return Dl

    def get_true_CMB_PS(self, nlb=5, Dl=True, aposize=None, EB_power=False, half=1):
        if 'T' in self.component:
            Dl_true_T = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self, 'true_T_dir'), nlb=nlb, Dl=Dl,
                                                                aposize=aposize, EB_power=False)

            np.save(getattr(self, 'true_output_T_dir').format(nlb), Dl_true_T)

        else:
            if EB_power:
                Dl_true_EB = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self, 'true_Q_dir'), data_U1_dir=getattr(self, 'true_U_dir'),
                                                                     nlb=nlb, Dl=Dl,
                                                                    aposize=aposize, EB_power=EB_power)
                np.save(getattr(self, 'save_true_EB_dir').format(nlb), Dl_true_EB)
            else:
                Dl_true_Q = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self, 'true_Q_dir'), nlb=nlb, Dl=Dl,
                                                                    aposize=aposize, EB_power=EB_power)
                Dl_true_U = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self, 'true_U_dir'), nlb=nlb, Dl=Dl,
                                                                    aposize=aposize,
                                                                    EB_power=EB_power)
                np.save(getattr(self, 'true_output_Q_dir').format(nlb), Dl_true_Q)
                np.save(getattr(self, 'true_output_U_dir').format(nlb), Dl_true_U)


    def cal_cmb_Q_or_U_PS(self, nlb=5, Dl=True, aposize=None, EB_power=False, half=1):
        Qfile_dir = getattr(self, 'output_Qmap_dir')
        Ufile_dir = getattr(self, 'output_Umap_dir')


        if self.is_half_split_map:
            output_Q_dir = Qfile_dir + 'predicted_sphere_CMB_Q' + '_map_half_{}.npy'.format(half)
            output_U_dir = Ufile_dir + 'predicted_sphere_CMB_U' + '_map_half_{}.npy'.format(half)
            target_Q_dir = Qfile_dir + 'target_sphere_CMB_Q' + '_map_half_{}.npy'.format(half)
            target_U_dir = Ufile_dir + 'target_sphere_CMB_U' + '_map_half_{}.npy'.format(half)

            save_output_Q_dir = self.save_PS_dir + 'output_cmb_Q_PS_nlb_{}_half_{}.npy'.format(nlb,half)
            save_output_U_dir = self.save_PS_dir + 'output_cmb_U_PS_nlb_{}_half_{}.npy'.format(nlb,half)
            save_target_Q_dir = self.save_PS_dir + 'target_cmb_Q_PS_nlb_{}_half_{}.npy'.format(nlb,half)
            save_target_U_dir = self.save_PS_dir + 'target_cmb_U_PS_nlb_{}_half_{}.npy'.format(nlb,half)

        else:
            output_Q_dir = Qfile_dir + 'predicted_sphere_CMB_Q' + '_map.npy'
            output_U_dir = Ufile_dir + 'predicted_sphere_CMB_U' + '_map.npy'
            target_Q_dir = Qfile_dir + 'target_sphere_CMB_Q' + '_map.npy'
            target_U_dir = Ufile_dir + 'target_sphere_CMB_U' + '_map.npy'

            save_output_Q_dir = self.save_PS_dir + 'output_cmb_Q_PS_nlb_{}.npy'.format(nlb)
            save_output_U_dir = self.save_PS_dir + 'output_cmb_U_PS_nlb_{}.npy'.format(nlb)
            save_target_Q_dir = self.save_PS_dir + 'target_cmb_Q_PS_nlb_{}.npy'.format(nlb)
            save_target_U_dir = self.save_PS_dir + 'target_cmb_U_PS_nlb_{}.npy'.format(nlb)

        Dl_output_Q = self.__power_for_Nsample_from_spheremap(data_Q1_dir=output_Q_dir, nlb=nlb, Dl=Dl, aposize=aposize,
                                                            EB_power=EB_power)
        Dl_output_U = self.__power_for_Nsample_from_spheremap(data_Q1_dir=output_U_dir, nlb=nlb, Dl=Dl,
                                                              aposize=aposize,
                                                              EB_power=EB_power)
        Dl_target_Q = self.__power_for_Nsample_from_spheremap(data_Q1_dir=target_Q_dir, nlb=nlb, Dl=Dl,
                                                              aposize=aposize,
                                                              EB_power=EB_power)
        Dl_target_U = self.__power_for_Nsample_from_spheremap(data_Q1_dir=target_U_dir, nlb=nlb, Dl=Dl,
                                                              aposize=aposize,
                                                              EB_power=EB_power)
        np.save(save_output_Q_dir, Dl_output_Q)
        np.save(save_output_U_dir, Dl_output_U)
        np.save(save_target_Q_dir, Dl_target_Q)
        np.save(save_target_U_dir, Dl_target_U)

    def cal_cmb_T_PS(self, nlb=5, Dl=True, aposize=None, EB_power=False, half=1):
        Qfile_dir = getattr(self, 'output_Tmap_dir')

        if self.is_half_split_map:
            output_T_dir = Qfile_dir + 'predicted_sphere_CMB_T' + '_map_half_{}.npy'.format(half)
            target_T_dir = Qfile_dir + 'target_sphere_CMB_T' + '_map_half_{}.npy'.format(half)

            save_output_T_dir = self.save_PS_dir + 'output_cmb_T_PS_nlb_{}_half_{}.npy'.format(nlb,half)
            save_target_T_dir = self.save_PS_dir + 'target_cmb_T_PS_nlb_{}_half_{}.npy'.format(nlb,half)

        else:
            output_T_dir = Qfile_dir + 'predicted_sphere_CMB_T' + '_map.npy'
            target_T_dir = Qfile_dir + 'target_sphere_CMB_T' + '_map.npy'

            save_output_T_dir = self.save_PS_dir + 'output_cmb_T_PS_nlb_{}.npy'.format(nlb)
            save_target_T_dir = self.save_PS_dir + 'target_cmb_T_PS_nlb_{}.npy'.format(nlb)

        Dl_output_Q = self.__power_for_Nsample_from_spheremap(data_Q1_dir=output_T_dir, nlb=nlb, Dl=Dl, aposize=aposize,
                                                            EB_power=EB_power)

        Dl_target_Q = self.__power_for_Nsample_from_spheremap(data_Q1_dir=target_T_dir, nlb=nlb, Dl=Dl,
                                                              aposize=aposize,
                                                              EB_power=EB_power)
        np.save(save_output_T_dir, Dl_output_Q)
        np.save(save_target_T_dir, Dl_target_Q)

    def cal_cmb_T_cross_PS(self, nlb=5, Dl=True, aposize=None, EB_power=False):

        Dl_output_T = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self,'output_T_dir_1'), data_Q2_dir=getattr(self,'output_T_dir_2'), nlb=nlb, Dl=Dl, aposize=aposize,
                                                              EB_power=EB_power)

        np.save(getattr(self,'save_output_cros_T_dir').format(nlb), Dl_output_T)



    def cal_cmb_Q_or_U_cross_PS(self, nlb=5, Dl=True, aposize=None, EB_power=False):

        Dl_output_Q = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self,'output_Q_dir_1'), data_Q2_dir=getattr(self,'output_Q_dir_2'), nlb=nlb, Dl=Dl, aposize=aposize,
                                                              EB_power=EB_power)
        Dl_output_U = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self,'output_U_dir_1'), data_Q2_dir=getattr(self,'output_U_dir_2'), nlb=nlb, Dl=Dl,
                                                              aposize=aposize,
                                                              EB_power=EB_power)
        np.save(getattr(self,'save_output_cros_Q_dir').format(nlb), Dl_output_Q)
        np.save(getattr(self,'save_output_cros_U_dir').format(nlb), Dl_output_U)

    def cal_cmb_E_B_PS(self, nlb=5, Dl=True, aposize=None, EB_power=True):

        Dl_output_EB = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self, 'output_Q_dir_1'),
                                                               data_U1_dir=getattr(self, 'output_U_dir_1'),nlb=nlb,
                                                              Dl=Dl, aposize=aposize,
                                                              EB_power=EB_power)
        Dl_tar_EB = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self, 'target_Q_dir_1'),
                                                               data_U1_dir=getattr(self, 'target_U_dir_1'), nlb=nlb,
                                                               Dl=Dl, aposize=aposize,
                                                               EB_power=EB_power)

        np.save(getattr(self,'save_output_EB_dir').format(nlb), Dl_output_EB)
        np.save(getattr(self,'save_target_EB_dir').format(nlb), Dl_tar_EB)

    def cal_cmb_E_B_cross_PS(self, nlb=5, Dl=True, aposize=None, EB_power=True):

        Dl_output_EB = self.__power_for_Nsample_from_spheremap(data_Q1_dir=getattr(self, 'output_Q_dir_1'),
                                                              data_Q2_dir=getattr(self, 'output_Q_dir_2'), data_U1_dir=getattr(self, 'output_U_dir_1'),
                                                              data_U2_dir=getattr(self, 'output_U_dir_2'),nlb=nlb,
                                                              Dl=Dl, aposize=aposize,
                                                              EB_power=EB_power)

        np.save(getattr(self,'save_output_cros_EB_dir').format(nlb), Dl_output_EB)








































================================================
FILE: cmbfscnn/CNN_models.py
================================================

# -*- coding: utf-8 -*-

import torch
import torch.nn as nn



class CMBFSCNN_level3(nn.Module):
    def __init__(self, in_channels = 10, out_channels=1, n_feats=16):
        super(CMBFSCNN_level3, self).__init__()
        self.adnet = BRDNet(n_feats=n_feats, in_channel = in_channels*2)
        self.unet_enconder_patch1 = Encoder(in_channels = in_channels)
        self.unet_deconder_patch1 = Decoder(n_feat = 256, out_channels = in_channels)
        self.unet_enconder_patch2 = Encoder(in_channels = 2 *in_channels)
        self.unet_deconder_patch2 = Decoder(n_feat=256*2, out_channels = in_channels)

        self.unet_enconder = Encoder(in_channels=2 * in_channels)
        self.unet_deconder = Decoder(n_feat=256 * 3, out_channels = out_channels)
        self.cov = nn.Sequential(
            nn.Conv2d(2, out_channels, kernel_size=3, padding=1, stride=1))
        self.over_pix = 64

    def forward(self, x):
        H = x.size(2)
        W = x.size(3)

        #  Patches for Stage 3
        x3l_img = x[:, :, :, 0:int(W / 2)+self.over_pix]
        x3r_img = x[:, :, :, int(W / 2)-self.over_pix:W]

        #  Patches for Stage 2
        x2ltop_img = x[:, :, 0:int(H / 2)+self.over_pix, 0:int(W / 2)+self.over_pix]
        x2rtop_img = x[:, :, 0:int(H / 2)+self.over_pix, int(W / 2)-self.over_pix:W]
        x2lbot_img = x[:, :, int(H / 2)-self.over_pix:H, 0:int(W / 2)+self.over_pix]
        x2rbot_img = x[:, :, int(H / 2)-self.over_pix:H, int(W / 2)-self.over_pix:W]


        # stage2 encoder
        feat2_enc_ltop = self.unet_enconder_patch1(x2ltop_img)
        feat2_enc_rtop = self.unet_enconder_patch1(x2rtop_img)
        feat2_enc_lbot = self.unet_enconder_patch1(x2lbot_img)
        feat2_enc_rbot = self.unet_enconder_patch1(x2rbot_img)
        del x2ltop_img
        del x2rtop_img
        del x2lbot_img
        del x2rbot_img

        feat2_enc_l = [torch.cat((k, v), 2) for k, v in zip(feat2_enc_ltop, feat2_enc_lbot)]
        feat2_enc_r = [torch.cat((k, v), 2) for k, v in zip(feat2_enc_rtop, feat2_enc_rbot)]
        feat2_skip_l = feat2_enc_l[-1]

        W1 = int(W / 4 / 2 ** 5)
        over_pix_1 = int(self.over_pix / 2 ** 5)
        H3 = int(H/2/2**5)
        feat2_skip_l_1 = feat2_skip_l[:,:,0:H3,:]
        feat2_skip_l_2 = feat2_skip_l[:, :, H3+over_pix_1 * 2:, :]
        feat2_skip_l_1[:,:,H3-over_pix_1:,:] = (feat2_skip_l_1[:,:,H3-over_pix_1:,:] + feat2_skip_l[:,:,H3 + over_pix_1:H3 + over_pix_1 * 2,:])/2
        feat2_skip_l_2[:,:,0:over_pix_1,:] = (feat2_skip_l_2[:,:,0:over_pix_1,:] + feat2_skip_l[:,:,H3:H3 + over_pix_1,:])/2
        feat2_skip_l = torch.cat((feat2_skip_l_1, feat2_skip_l_2),2)
        # feat2_enc_l[4] = feat2_skip_l
        del feat2_skip_l_1
        del feat2_skip_l_2




        feat2_skip_r = feat2_enc_r[-1]
        feat2_skip_r_1 = feat2_skip_r[:, :, 0:H3, :]
        feat2_skip_r_2 = feat2_skip_r[:, :, H3+over_pix_1 * 2:, :]
        feat2_skip_r_1[:, :, H3-over_pix_1:, :] = (feat2_skip_r_1[:, :, H3-over_pix_1:, :] + feat2_skip_r[:, :, H3 + over_pix_1:H3 + over_pix_1 * 2, :]) / 2
        feat2_skip_r_2[:, :, 0:over_pix_1, :] = (feat2_skip_r_2[:, :, 0:over_pix_1, :] + feat2_skip_r[:, :, H3:H3 + over_pix_1, :]) / 2
        feat2_skip_r = torch.cat((feat2_skip_r_1, feat2_skip_r_2), 2)
        # feat2_enc_r[4] = feat2_skip_r
        del feat2_skip_r_1
        del feat2_skip_r_2

        # stage2 decoder
        feat2_dec_l = self.unet_deconder_patch1(feat2_enc_l)
        feat2_dec_r = self.unet_deconder_patch1(feat2_enc_r)
        del feat2_enc_l
        del feat2_enc_r
        H4 = int(H/2)
        feat2_dec_l_1 = feat2_dec_l[:, :, 0:H4, :]
        feat2_dec_l_2 = feat2_dec_l[:, :, H4 + self.over_pix*2:, :]
        feat2_dec_l_1[:, :, H4 - self.over_pix:, :] = (feat2_dec_l_1[:, :, H4 - self.over_pix:, :] + feat2_dec_l[:, :, H4 + self.over_pix:H4 + self.over_pix*2,
                                                                                 :]) / 2
        feat2_dec_l_2[:, :, 0:self.over_pix, :] = (feat2_dec_l_1[:, :, 0:self.over_pix, :] + feat2_dec_l[:, :, H4:H4 + self.over_pix, :]) / 2
        feat2_dec_l = torch.cat((feat2_dec_l_1, feat2_dec_l_2), 2)
        del feat2_dec_l_1
        del feat2_dec_l_2


        feat2_dec_r_1 = feat2_dec_r[:, :, 0:H4, :]
        feat2_dec_r_2 = feat2_dec_r[:, :, H4 + self.over_pix*2:, :]
        feat2_dec_r_1[:, :, H4 - self.over_pix:, :] = (feat2_dec_r_1[:, :, H4 - self.over_pix:, :] + feat2_dec_r[:, :, H4 + self.over_pix:H4 + self.over_pix*2,
                                                                                 :]) / 2
        feat2_dec_r_2[:, :, 0:self.over_pix, :] = (feat2_dec_r_1[:, :, 0:self.over_pix, :] + feat2_dec_r[:, :, H4:H4 + self.over_pix, :]) / 2
        feat2_dec_r = torch.cat((feat2_dec_r_1, feat2_dec_r_2), 2)
        del feat2_dec_r_1
        del feat2_dec_r_2
        x3r_img = torch.cat((x3r_img, feat2_dec_r), 1)
        x3l_img = torch.cat((x3l_img, feat2_dec_l), 1)
        del feat2_dec_l
        del feat2_dec_r


        # stage3 decoder
        feat3_enc_l = self.unet_enconder_patch2(x3l_img)
        feat3_enc_r = self.unet_enconder_patch2(x3r_img)
        del x3l_img
        del x3r_img
        feat3_enc_l[-1] = torch.cat((feat3_enc_l[-1], feat2_skip_l), 1)
        feat3_enc_r[-1] = torch.cat((feat3_enc_r[-1], feat2_skip_r), 1)
        del feat2_skip_l
        del feat2_skip_r
        feat3_enc = [torch.cat((k, v), 3) for k, v in zip(feat3_enc_l, feat3_enc_r)]
        del feat3_enc_l
        del feat3_enc_r
        feat3_skip = feat3_enc[-1]
        W3 = int(W/2/2**5)
        feat3_skip_1 = feat3_skip[:, :, :, 0:W3]
        feat3_skip_2 = feat3_skip[:, :, :, W3+ over_pix_1 * 2:]
        feat3_skip_1[:, :, :, W3- over_pix_1:] = (feat3_skip_1[:, :, :, W3- over_pix_1:] + feat3_skip[:, :, :, W3 + over_pix_1:W3 + over_pix_1 * 2]) / 2
        feat3_skip_2[:, :, :, 0:over_pix_1] = (feat3_skip_2[:, :, :, 0:over_pix_1] + feat3_skip[:, :, :, W3:W3 + over_pix_1]) / 2
        feat3_skip = torch.cat((feat3_skip_1, feat3_skip_2), 3)



        del feat3_skip_1
        del feat3_skip_2
        feat3_dec = self.unet_deconder_patch2(feat3_enc)
        W4 = int(W / 2)
        feat3_dec_1 = feat3_dec[:, :, :, 0:W4]
        feat3_dec_2 = feat3_dec[:, :, :, W4 + self.over_pix*2:]
        feat3_dec_1[:, :, :, W4 - self.over_pix:] = (feat3_dec_1[:, :, :, W4 - self.over_pix:] + feat3_dec[:, :, :, W4 + self.over_pix:W4 + self.over_pix*2]) / 2
        feat3_dec_2[:, :, :, 0:self.over_pix] = (feat3_dec_1[:, :, :, 0:self.over_pix] + feat3_dec[:, :, :, W4:W4 + self.over_pix]) / 2
        feat3_dec = torch.cat((feat3_dec_1, feat3_dec_2), 3)
        x_cros = torch.cat((x, feat3_dec), 1)
        del feat3_dec
        del feat3_dec_1
        del feat3_dec_2

        # stage4 encoder
        x_feat_enc = self.unet_enconder(x_cros)
        x_feat_enc[-1] = torch.cat((x_feat_enc[-1], feat3_skip), 1)
        del feat3_skip
        # stage4 decoder
        x_feat_dec = self.unet_deconder(x_feat_enc)
        del x_feat_enc

        x_adnet = self.adnet(x_cros)
        del x_cros
        xx = torch.cat((x_feat_dec, x_adnet), 1)
        del x_feat_dec
        xx = self.cov(xx)
        return xx


class CMBFSCNN_level4(nn.Module):
    def __init__(self, in_channels=8, out_channels=1, n_feats=16):
        super(CMBFSCNN_level4, self).__init__()
        self.adnet = BRDNet(n_feats=n_feats, in_channel = in_channels*2)
        self.unet_enconder_patch1 = Encoder(in_channels = in_channels)
        self.unet_deconder_patch1 = Decoder(n_feat = 512, out_channels = in_channels)
        self.unet_enconder_patch2 = Encoder(in_channels = 2 *in_channels)
        self.unet_deconder_patch2 = Decoder(n_feat=512*2, out_channels = in_channels)
        self.unet_enconder_patch3 = Encoder(in_channels=2 * in_channels)
        self.unet_deconder_patch3 = Decoder(n_feat=512 * 3, out_channels = in_channels)
        self.unet_enconder = Encoder(in_channels=2 * in_channels)
        self.unet_deconder = Decoder(n_feat=512 * 4, out_channels = out_channels)
        self.cov = nn.Sequential(
            nn.Conv2d(2, out_channels, kernel_size=3, padding=1, stride=1))

    def forward(self, x):
        H = x.size(2)
        W = x.size(3)

        #  Patches for Stage 3
        x3l_img = x[:, :, :, 0:int(W / 2)+32]
        x3r_img = x[:, :, :, int(W / 2)-32:W]

        #  Patches for Stage 2
        x2ltop_img = x[:, :, 0:int(H / 2)+32, 0:int(W / 2)+32]
        x2rtop_img = x[:, :, 0:int(H / 2)+32, int(W / 2)-32:W]
        x2lbot_img = x[:, :, int(H / 2)-32:H, 0:int(W / 2)+32]
        x2rbot_img = x[:, :, int(H / 2)-32:H, int(W / 2)-32:W]

        #  Patches for Stage 1
        x1top1_img = x[:, :, 0:int(H / 2)+32, 0:int(W / 4)+32]
        x1top2_img = x[:, :, 0:int(H / 2)+32, int(W / 4)-32:int(W / 2)+32]
        x1top3_img = x[:, :, 0:int(H / 2)+32, int(W / 2)-32:int(3 * W / 4)+32]
        x1top4_img = x[:, :, 0:int(H / 2)+32, int(3 * W / 4)-32:W]

        x1bot1_img = x[:, :, int(H / 2)-32:H, 0:int(W / 4)+32]
        x1bot2_img = x[:, :, int(H / 2)-32:H, int(W / 4)-32:int(W / 2)+32]
        x1bot3_img = x[:, :, int(H / 2)-32:H, int(W / 2)-32:int(3 * W / 4)+32]
        x1bot4_img = x[:, :, int(H / 2)-32:H, int(3 * W / 4)-32:W]

        # stage1 encoder
        feat_enc_top1 = self.unet_enconder_patch1(x1top1_img)
        feat_enc_top2 = self.unet_enconder_patch1(x1top2_img)
        feat_enc_top3 = self.unet_enconder_patch1(x1top3_img)
        feat_enc_top4 = self.unet_enconder_patch1(x1top4_img)
        feat_enc_bot1 = self.unet_enconder_patch1(x1bot1_img)
        feat_enc_bot2 = self.unet_enconder_patch1(x1bot2_img)
        feat_enc_bot3 = self.unet_enconder_patch1(x1bot3_img)
        feat_enc_bot4 = self.unet_enconder_patch1(x1bot4_img)

        ## Concat deep features
        feat_enc_ltop = [torch.cat((k, v), 3) for k, v in zip(feat_enc_top1, feat_enc_top2)]
        feat_enc_rtop = [torch.cat((k, v), 3) for k, v in zip(feat_enc_top3, feat_enc_top4)]
        feat_enc_lbot = [torch.cat((k, v), 3) for k, v in zip(feat_enc_bot1, feat_enc_bot2)]
        feat_enc_rbot = [torch.cat((k, v), 3) for k, v in zip(feat_enc_bot3, feat_enc_bot4)]

        # ------------------- The boundary of the merged data for output of enconder of unit 1 ---------------
        feat1_skip_ltop = feat_enc_ltop[4]
        feat1_skip_ltop_1 = feat1_skip_ltop[:, :, :,0:4]
        feat1_skip_ltop_2 = feat1_skip_ltop[:, :, :,6:]
        feat1_skip_ltop_1[:, :, :,3] = (feat1_skip_ltop_1[:, :, :,3] + feat1_skip_ltop[:, :, :,5]) / 2
        feat1_skip_ltop_2[:, :, :, 0] = (feat1_skip_ltop_2[:, :, :,0] + feat1_skip_ltop[:, :, :,4]) / 2
        feat1_skip_ltop = torch.cat((feat1_skip_ltop_1, feat1_skip_ltop_2), 3)
        feat1_skip_rtop = feat_enc_rtop[4]
        feat1_skip_rtop_1 = feat1_skip_rtop[:, :, :, 0:4]
        feat1_skip_rtop_2 = feat1_skip_rtop[:, :, :, 6:]
        feat1_skip_rtop_1[:, :, :, 3] = (feat1_skip_rtop_1[:, :, :, 3] + feat1_skip_rtop[:, :, :, 5]) / 2
        feat1_skip_rtop_2[:, :, :, 0] = (feat1_skip_rtop_2[:, :, :, 0] + feat1_skip_rtop[:, :, :, 4]) / 2
        feat1_skip_rtop = torch.cat((feat1_skip_rtop_1, feat1_skip_rtop_2), 3)

        feat1_skip_lbot = feat_enc_lbot[4]
        feat1_skip_lbot_1 = feat1_skip_lbot[:, :, :, 0:4]
        feat1_skip_lbot_2 = feat1_skip_lbot[:, :, :, 6:]
        feat1_skip_lbot_1[:, :, :, 3] = (feat1_skip_lbot_1[:, :, :, 3] + feat1_skip_lbot[:, :, :, 5]) / 2
        feat1_skip_lbot_2[:, :, :, 0] = (feat1_skip_lbot_2[:, :, :, 0] + feat1_skip_lbot[:, :, :, 4]) / 2
        feat1_skip_lbot = torch.cat((feat1_skip_lbot_1, feat1_skip_lbot_2), 3)

        feat1_skip_rbot = feat_enc_rbot[4]
        feat1_skip_rbot_1 = feat1_skip_rbot[:, :, :, 0:4]
        feat1_skip_rbot_2 = feat1_skip_rbot[:, :, :, 6:]
        feat1_skip_rbot_1[:, :, :, 3] = (feat1_skip_rbot_1[:, :, :, 3] + feat1_skip_rbot[:, :, :, 5]) / 2
        feat1_skip_rbot_2[:, :, :, 0] = (feat1_skip_rbot[:, :, :, 0] + feat1_skip_rbot[:, :, :, 4]) / 2
        feat1_skip_rbot = torch.cat((feat1_skip_rbot_1, feat1_skip_rbot_2), 3)

        # stage1 decoder
        feat_dec_ltop = self.unet_deconder_patch1(feat_enc_ltop)
        feat_dec_rtop = self.unet_deconder_patch1(feat_enc_rtop)
        feat_dec_lbot = self.unet_deconder_patch1(feat_enc_lbot)
        feat_dec_rbot = self.unet_deconder_patch1(feat_enc_rbot)

        # ------------------- The boundary of the merged data for output of deconder of unit 1 ---------------
        feat_dec_ltop_1 = feat_dec_ltop[:, :, :,0:128]
        feat_dec_ltop_2 = feat_dec_ltop[:, :, :,128 + 64:]
        feat_dec_ltop_1[:, :, :,128-32:] = (feat_dec_ltop_1[:, :, :,128 - 32:] + feat_dec_ltop[:, :, :,128 + 32:128 + 64]) / 2
        feat_dec_ltop_2[:, :, :,0:32] = (feat_dec_ltop_1[:, :, :,0:32] + feat_dec_ltop[:, :, :,128:128 + 32]) / 2
        feat_dec_ltop = torch.cat((feat_dec_ltop_1, feat_dec_ltop_2), 3)

        feat_dec_rtop_1 = feat_dec_rtop[:, :, :, 0:128]
        feat_dec_rtop_2 = feat_dec_rtop[:, :, :, 128 + 64:]
        feat_dec_rtop_1[:, :, :, 128 - 32:] = (feat_dec_rtop_1[:, :, :, 128 - 32:] + feat_dec_rtop[:, :, :,128 + 32:128 + 64]) / 2
        feat_dec_rtop_2[:, :, :, 0:32] = (feat_dec_rtop_1[:, :, :, 0:32] + feat_dec_rtop[:, :, :, 128:128 + 32]) / 2
        feat_dec_rtop = torch.cat((feat_dec_rtop_1, feat_dec_rtop_2), 3)

        feat_dec_lbot_1 = feat_dec_lbot[:, :, :, 0:128]
        feat_dec_lbot_2 = feat_dec_lbot[:, :, :, 128 + 64:]
        feat_dec_lbot_1[:, :, :, 128 - 32:] = (feat_dec_lbot_1[:, :, :, 128 - 32:] + feat_dec_lbot[:, :, :,128 + 32:128 + 64]) / 2
        feat_dec_lbot_2[:, :, :, 0:32] = (feat_dec_lbot_1[:, :, :, 0:32] + feat_dec_lbot[:, :, :, 128:128 + 32]) / 2
        feat_dec_lbot = torch.cat((feat_dec_lbot_1, feat_dec_lbot_2), 3)

        feat_dec_rbot_1 = feat_dec_rbot[:, :, :, 0:128]
        feat_dec_rbot_2 = feat_dec_rbot[:, :, :, 128 + 64:]
        feat_dec_rbot_1[:, :, :, 128 - 32:] = (feat_dec_rbot_1[:, :, :, 128 - 32:] + feat_dec_rbot[:, :, :,128 + 32:128 + 64]) / 2
        feat_dec_rbot_2[:, :, :, 0:32] = (feat_dec_rbot_1[:, :, :, 0:32] + feat_dec_rbot[:, :, :, 128:128 + 32]) / 2
        feat_dec_rbot = torch.cat((feat_dec_rbot_1, feat_dec_rbot_2), 3)

        # ---------------------------------------------------------------------------------------------------
        x2ltop_img = torch.cat((x2ltop_img, feat_dec_ltop), 1)
        x2rtop_img = torch.cat((x2rtop_img, feat_dec_rtop), 1)
        x2lbot_img = torch.cat((x2lbot_img, feat_dec_lbot), 1)
        x2rbot_img = torch.cat((x2rbot_img, feat_dec_rbot), 1)

        # stage2 encoder
        feat2_enc_ltop = self.unet_enconder_patch2(x2ltop_img)
        feat2_enc_rtop = self.unet_enconder_patch2(x2rtop_img)
        feat2_enc_lbot = self.unet_enconder_patch2(x2lbot_img)
        feat2_enc_rbot = self.unet_enconder_patch2(x2rbot_img)
        feat2_enc_ltop[4] = torch.cat((feat2_enc_ltop[4], feat1_skip_ltop), 1)
        feat2_enc_rtop[4] = torch.cat((feat2_enc_rtop[4], feat1_skip_rtop),1)
        feat2_enc_lbot[4] = torch.cat((feat2_enc_lbot[4], feat1_skip_lbot), 1)
        feat2_enc_rbot[4] = torch.cat((feat2_enc_rbot[4], feat1_skip_rbot), 1)
        feat2_enc_l = [torch.cat((k, v), 2) for k, v in zip(feat2_enc_ltop, feat2_enc_lbot)]
        feat2_enc_r = [torch.cat((k, v), 2) for k, v in zip(feat2_enc_rtop, feat2_enc_rbot)]
        feat2_skip_l = feat2_enc_l[4]
        feat2_skip_l_1 = feat2_skip_l[:,:,0:8,:]
        feat2_skip_l_2 = feat2_skip_l[:, :, 10:, :]
        feat2_skip_l_1[:,:,7,:] = (feat2_skip_l_1[:,:,7,:] + feat2_skip_l[:,:,9,:])/2
        feat2_skip_l_2[:,:,0,:] = (feat2_skip_l_2[:,:,0,:] + feat2_skip_l[:,:,8,:])/2
        feat2_skip_l = torch.cat((feat2_skip_l_1, feat2_skip_l_2),2)
        # feat2_enc_l[4] = feat2_skip_l

        feat2_skip_r = feat2_enc_r[4]
        feat2_skip_r_1 = feat2_skip_r[:, :, 0:8, :]
        feat2_skip_r_2 = feat2_skip_r[:, :, 10:, :]
        feat2_skip_r_1[:, :, 7, :] = (feat2_skip_r_1[:, :, 7, :] + feat2_skip_r[:, :, 9, :]) / 2
        feat2_skip_r_2[:, :, 0, :] = (feat2_skip_r_2[:, :, 0, :] + feat2_skip_r[:, :, 8, :]) / 2
        feat2_skip_r = torch.cat((feat2_skip_r_1, feat2_skip_r_2), 2)
        # feat2_enc_r[4] = feat2_skip_r

        # stage2 decoder
        feat2_dec_l = self.unet_deconder_patch2(feat2_enc_l)
        feat2_dec_r = self.unet_deconder_patch2(feat2_enc_r)
        feat2_dec_l_1 = feat2_dec_l[:, :, 0:256, :]
        feat2_dec_l_2 = feat2_dec_l[:, :, 256 + 64:, :]
        feat2_dec_l_1[:, :, 256 - 32:, :] = (feat2_dec_l_1[:, :, 256 - 32:, :] + feat2_dec_l[:, :, 256 + 32:256 + 64,
                                                                                 :]) / 2
        feat2_dec_l_2[:, :, 0:32, :] = (feat2_dec_l_1[:, :, 0:32, :] + feat2_dec_l[:, :, 256:256 + 32, :]) / 2
        feat2_dec_l = torch.cat((feat2_dec_l_1, feat2_dec_l_2), 2)

        feat2_dec_r_1 = feat2_dec_r[:, :, 0:256, :]
        feat2_dec_r_2 = feat2_dec_r[:, :, 256 + 64:, :]
        feat2_dec_r_1[:, :, 256 - 32:, :] = (feat2_dec_r_1[:, :, 256 - 32:, :] + feat2_dec_r[:, :, 256 + 32:256 + 64,
                                                                                 :]) / 2
        feat2_dec_r_2[:, :, 0:32, :] = (feat2_dec_r_1[:, :, 0:32, :] + feat2_dec_r[:, :, 256:256 + 32, :]) / 2
        feat2_dec_r = torch.cat((feat2_dec_r_1, feat2_dec_r_2), 2)
        x3r_img = torch.cat((x3r_img, feat2_dec_r), 1)
        x3l_img = torch.cat((x3l_img, feat2_dec_l), 1)

        # stage3 decoder
        feat3_enc_l = self.unet_enconder_patch3(x3l_img)
        feat3_enc_r = self.unet_enconder_patch3(x3r_img)
        feat3_enc_l[4] = torch.cat((feat3_enc_l[4], feat2_skip_l), 1)
        feat3_enc_r[4] = torch.cat((feat3_enc_r[4], feat2_skip_r), 1)
        feat3_enc = [torch.cat((k, v), 3) for k, v in zip(feat3_enc_l, feat3_enc_r)]
        feat3_skip = feat3_enc[4]
        feat3_skip_1 = feat3_skip[:, :, :, 0:8]
        feat3_skip_2 = feat3_skip[:, :, :, 10:]
        feat3_skip_1[:, :, :, 7] = (feat3_skip_1[:, :, :, 7] + feat3_skip[:, :, :, 9]) / 2
        feat3_skip_2[:, :, :, 0] = (feat3_skip_2[:, :, :, 0] + feat3_skip[:, :, :, 8]) / 2
        feat3_skip = torch.cat((feat3_skip_1, feat3_skip_2), 3)
        feat3_dec = self.unet_deconder_patch3(feat3_enc)
        feat3_dec_1 = feat3_dec[:, :, :, 0:256]
        feat3_dec_2 = feat3_dec[:, :, :, 256 + 64:]
        feat3_dec_1[:, :, :, 256 - 32:] = (feat3_dec_1[:, :, :, 256 - 32:] + feat3_dec[:, :, :, 256 + 32:256 + 64]) / 2
        feat3_dec_2[:, :, :, 0:32] = (feat3_dec_1[:, :, :, 0:32] + feat3_dec[:, :, :, 256:256 + 32]) / 2
        feat3_dec = torch.cat((feat3_dec_1, feat3_dec_2), 3)
        x_cros = torch.cat((x, feat3_dec), 1)
        # stage4 encoder
        x_feat_enc = self.unet_enconder(x_cros)
        x_feat_enc[4] = torch.cat((x_feat_enc[4], feat3_skip), 1)
        # stage4 decoder
        x_feat_dec = self.unet_deconder(x_feat_enc)

        x_adnet = self.adnet(x_cros)
        xx = torch.cat((x_feat_dec, x_adnet), 1)
        xx = self.cov(xx)
        return xx



class UNet(nn.Module):
    def __init__(self, in_channels=6, out_channels=1):
        super(UNet, self).__init__()
        # the shape of input: (B, 1, nside*4, nside*3) = (B, 1, 2048, 1536)
        self.down1 = UNetDown(in_channels, 16, normalize=False,kernel_size=4,stride=2,padding=1) # (N,64,L/2,H/2)
        self.down2 = UNetDown(16, 32, kernel_size=4,stride=2,padding=1)  # (N,128,L/2**2,H/2**2)
        self.down3 = UNetDown(32, 64,kernel_size=4,stride=2,padding=1)  # (N,256,L/2**3,H/2**3)
        self.down4 = UNetDown(64, 128, kernel_size=4,stride=2,padding=1) # (N,512,L/2**4,H/2**4)
        self.down5 = UNetDown(128, 256, kernel_size=4, stride=2, padding=1)  # (N,512,L/2**5,H/2**5)
        # self.down6 = UNetDown(256, 256, kernel_size=3, stride=1, padding=1)  # (N,512,L/2**5,H/2**5)


        # self.up3 = UNetUp(256, 256, kernel_size=3,stride=1,padding=1)  # (N,256*2,L/2**3,H/2**3)
        self.up4 = UNetUp(256, 128, kernel_size=4, stride=2, padding=1)  # (N,128*2,L/2**2,H/2**2)
        self.up5 = UNetUp(256, 64, kernel_size=4, stride=2, padding=1)  # (N,64*2,L/2,H/2)
        self.up6 = UNetUp(128, 32, kernel_size=4, stride=2, padding=1)  # (N,64*2,L/2,H/2)
        self.up7 = UNetUp(64, 16, kernel_size=4, stride=2, padding=1)  # (N,64*2,L/2,H/2)
        self.up8 = nn.Sequential(
            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),
            nn.InstanceNorm2d(16),
            nn.LeakyReLU(0.4, inplace=True),
        )
        self.final = nn.Sequential(
            nn.Conv2d(16, out_channels, kernel_size=3, padding=1, stride=1),
            nn.LeakyReLU(0.4, inplace=True),
        )

    def forward(self, x):
        # U-Net generator with skip connections from encoder to decoder
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        d5 = self.down5(d4)

        u4 = self.up4(d5, d4)
        u5 = self.up5(u4, d3)
        u6 = self.up6(u5, d2)
        u7 = self.up7(u6, d1)
        u8 = self.up8(u7)
        uf = self.final(u8)
        return uf




class UNetDown(nn.Module):
    def __init__(self, in_size, out_size, kernel_size=3,padding=1,stride=1,normalize=True, dropout=0.0):
        super(UNetDown, self).__init__()
        layers = [nn.Conv2d(in_size, out_size, kernel_size=kernel_size,stride=stride, padding = padding, bias=False)]
        if normalize:
            layers.append(nn.InstanceNorm2d(out_size))
        layers.append(nn.LeakyReLU(0.4, inplace=True))
        if dropout >0:
            layers.append(nn.Dropout(dropout))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)


class UNetUp(nn.Module):
    def __init__(self, in_size, out_size, dropout=0.0, kernel_size=3,padding=1,stride=1):
        super(UNetUp, self).__init__()
        layers = [
            nn.ConvTranspose2d(in_size, out_size, kernel_size=kernel_size,stride=stride, padding = padding, bias=False),
            nn.InstanceNorm2d(out_size),
            nn.LeakyReLU(0.4, inplace=True),
        ]
        if dropout >0:
            layers.append(nn.Dropout(dropout))
        self.model = nn.Sequential(*layers)
    def forward(self, x, skip_input):
        x = self.model(x)
        x = torch.cat((x, skip_input), 1)
        return x





class Encoder(nn.Module):
    def __init__(self, in_channels=6):
        super(Encoder, self).__init__()
        self.down1 = UNetDown(in_channels, 16, kernel_size=4, stride=2, padding=1)
        self.down2 = UNetDown(16, 32, kernel_size=4, stride=2, padding=1)
        self.down3 = UNetDown(32, 64, kernel_size=4, stride=2, padding=1)
        self.down4 = UNetDown(64, 128, kernel_size=4, stride=2, padding=1)
        self.down5 = UNetDown(128, 256, kernel_size=4, stride=2, padding=1)

    def forward(self, x):
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        d5 = self.down5(d4)
        return [d1, d2, d3, d4, d5]

class Decoder(nn.Module):
    def __init__(self, out_channels=6, n_feat=256):
        super(Decoder, self).__init__()
        self.up1 = UNetUp(n_feat, 128, kernel_size=4, stride=2, padding=1)
        self.up2 = UNetUp(256, 64, kernel_size=4, stride=2, padding=1)
        self.up3 = UNetUp(128, 32, kernel_size=4, stride=2, padding=1)
        self.up4 = UNetUp(64, 16, kernel_size=4, stride=2, padding=1)
        self.up5 = nn.Sequential(
            nn.ConvTranspose2d(32, out_channels, kernel_size=4, stride=2, padding=1),
            #nn.InstanceNorm2d(16),
            #nn.LeakyReLU(0.4, inplace=True),
        )

    def forward(self, x):
        d1, d2, d3, d4, d5 = x
        u1 = self.up1(d5, d4)
        u2 = self.up2(u1, d3)
        u3 = self.up3(u2, d2)
        u4 = self.up4(u3, d1)
        u5 = self.up5(u4)
        return u5



class UpNet(nn.Module):

    def __init__(self,n_feats,in_channel):
        super(UpNet, self).__init__()
        layers = [nn.Conv2d(in_channels=in_channel, out_channels=n_feats, kernel_size=3, stride=1, padding=1),
                # BatchRenorm2d(64),
                nn.InstanceNorm2d(n_feats),
                nn.LeakyReLU(0.4, inplace=True)]

        for i in range(5):
            layers.append(nn.Conv2d(n_feats, n_feats, 3, 1, 1))
            # layers.append(BatchRenorm2d(64))
            layers.append(nn.InstanceNorm2d(n_feats))
            layers.append(nn.LeakyReLU(0.4, inplace=True))

        layers.append(nn.Conv2d(n_feats, 1, 3, 1, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        out = self.net(x)
        return out


class DownNet(nn.Module):
    def __init__(self,n_feats,in_channel):
        super(DownNet, self).__init__()
        layers = [nn.Conv2d(in_channels=in_channel, out_channels=n_feats, kernel_size=3, stride=1, padding=1),
                #BatchRenorm2d(64),
                nn.InstanceNorm2d(n_feats),
                nn.LeakyReLU(0.4, inplace=True)]

        for i in range(2):
            layers.append(nn.Conv2d(n_feats, n_feats, 3, 1, padding=2, dilation=2))
            layers.append(nn.LeakyReLU(0.4, inplace=True))
        layers.append(nn.Conv2d(n_feats, n_feats, 3, 1, 1))
        #layers.append(BatchRenorm2d(64))
        layers.append(nn.InstanceNorm2d(n_feats))
        layers.append(nn.LeakyReLU(0.4, inplace=True))
        for i in range(2):
            layers.append(nn.Conv2d(n_feats, n_feats, 3, 1, padding=2, dilation=2))
            layers.append(nn.LeakyReLU(0.4, inplace=True))
        layers.append(nn.Conv2d(n_feats, n_feats, 3, 1, 1))
        # layers.append(BatchRenorm2d(64))
        layers.append(nn.InstanceNorm2d(n_feats))
        layers.append(nn.LeakyReLU(0.4, inplace=True))

        layers.append(nn.Conv2d(n_feats, 1, 3, 1, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        out = self.net(x)
        return out


class BRDNet(nn.Module):
    def __init__(self,n_feats, in_channel):
        super(BRDNet, self).__init__()
        self.upnet = UpNet(n_feats=n_feats,in_channel=in_channel)
        self.dwnet = DownNet(n_feats=n_feats,in_channel=in_channel)
        self.conv = nn.Conv2d(in_channel+2, 1, 3, 1, 1)
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        #import pdb;pdb.set_trace()
        out1 = self.upnet(x)
        out2 = self.dwnet(x)
        #out1 = x - out1
        #out2 = x - out2
        out = torch.cat((out1, out2, x), 1)
        out = self.conv(out)
        #out = x - out
        return out



def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)







================================================
FILE: cmbfscnn/get_power_sperctra.py
================================================
import camb
import numpy as np

# this parameters are get from Table 3 (TT+lowP) of Planck 2015 results -XIII
hubble = [67.31, 0.96]   # [best fitting values, std 1 sigma]
ombh2 = [0.02222, 0.00023]
omch2 = [0.1197, 0.0022]
re_optical_depth = [0.078, 0.019]
scalar_amp_1 = [2.1955e-9, 0.0791e-9]
scalar_spectral_index_1 = [0.9655, 0.0062]

def sim_power_spectra(sim_H0, sim_ombh2, sim_omch2, sim_tau, sim_As, sim_ns,
                      spectra_type='lensed_scalar'):
    '''
    spectra_type: 'lensed_scalar', 'unlensed_total'.
    given five Cosmological parameter ,use camb software package ot get power_sperctra.
    our target is to obtain 'cmb_specs' that is necessary in simulating cmb with pysm.
    obtaining power spectra is fist step for getting 'cmb_specs'
    '''
    #Set up a new set of parameters for CAMB
    #pars = camb.CAMBparams()
    pars = camb.model.CAMBparams()
    #This function sets up CosmoMC-like settings, with one massive neutrino and \
    #helium set using BBN consistency
    pars.set_cosmology(H0=sim_H0, ombh2=sim_ombh2, omch2=sim_omch2, tau=sim_tau)
    pars.InitPower.set_params(As=sim_As, ns=sim_ns)
    pars.set_for_lmax(3500, lens_potential_accuracy=1);
    #calculate results for these parameters
    results = camb.get_results(pars)
    cl_phi = results.get_lens_potential_cls(lmax=3550)
    #print('phi power', cl_phi.shape)
    #get dictionary of CAMB power spectra
    powers =results.get_cmb_power_spectra(pars)
    # unlensedtotCls = powers['unlensed_total']
    unlensedtotCls = powers[spectra_type]
    #Python CL arrays are all zero based (starting at L=0), Note L=0,1 entries
    #will be zero by default.
    #The different CL are always in the order TT, EE, BB, TE (with BB=0 for
    #unlensed scalar results).
    CMB_outputscale = 7.42835025e12
    unlensedtotCls = unlensedtotCls*CMB_outputscale
    l = np.arange(unlensedtotCls.shape[0])[2:]
    unlensedtotCls = unlensedtotCls[2:, :]
    index = unlensedtotCls.shape[0]
    cl_phi = cl_phi[2:,:]  # phi phi spectrum
    cl_phi = cl_phi[:index,:]
    #print('phi power', cl_phi)
    unlensedtotCls_ = np.c_[l, unlensedtotCls,cl_phi]
    #print('=========', unlensedtotCls_[:].shape)
    return unlensedtotCls_

def sim_UniformParam(param, times_l = 5, times_r = 5):
    range_param = [param[0]-param[1]*times_l, param[0]+param[1]*times_r]
    sim_param = np.random.rand() * (range_param[1] - range_param[0]) + range_param[0]
    return sim_param

def sim_NormalParams(param, param_label):
    sim_param = np.random.randn() * param[1] + param[0]
#    print '%s:'%param_label, '%s'%sim
    return sim_param

# this function is randomizing cosmological parameters
# then using random or fixed  cosmological parameters to  get power pectra
# if random is 'Normal', paramater 'times' is useless
def get_spectra(random='Normal', times=5, spectra_type='lensed_scalar'):
    if random == 'Normal':
        sim_H0 = sim_NormalParams(hubble, 'hubble')  # realize random for cosmological parameter
        sim_ombh2 = sim_NormalParams(ombh2, 'ombh2')
        sim_omch2 = sim_NormalParams(omch2, 'omch2')
#        sim_tau = sim_NormalParams(re_optical_depth, 're_optical_depth')
        sim_tau = 0.078
        sim_As = sim_NormalParams(scalar_amp_1, 'scalar_amp_1')
        sim_ns = sim_NormalParams(scalar_spectral_index_1, 'scalar_spectral_index_1')
    elif random == 'Uniform':
        sim_H0 = sim_UniformParam(hubble, times_l=times, times_r=times)
        sim_ombh2 = sim_UniformParam(ombh2, times_l=times, times_r=times)
        sim_omch2 = sim_UniformParam(omch2, times_l=times, times_r=times)
#        times_tau_l = (0.078-0.003)/0.019
#        sim_tau = sim_UniformParam(re_optical_depth, times_l=times_tau_l, times_r=times)
        sim_tau = 0.078
        sim_As = sim_UniformParam(scalar_amp_1, times_l=times, times_r=times)
        sim_ns = sim_UniformParam(scalar_spectral_index_1, times_l=times, times_r=times)
    elif random == 'fixed':
        sim_H0 = hubble[0]
        sim_ombh2 = ombh2[0]
        sim_omch2 = omch2[0]
        sim_tau = re_optical_depth[0]
        sim_As = scalar_amp_1[0]
        sim_ns = scalar_spectral_index_1[0]
    sim_params = np.c_[sim_H0,sim_ombh2,sim_omch2,sim_tau,sim_As,sim_ns]
    unlensedtotCls = sim_power_spectra(sim_H0,sim_ombh2,sim_omch2,sim_tau,sim_As,sim_ns, spectra_type=spectra_type)
    return unlensedtotCls, sim_params   # the shape of unlensedtotCls is [N_l, 5]
# [:,0] is ell, index from 1 to 5 represent TT, EE, BB, TE

def ReadClFromPycamb(random=None, times=None, runCAMB = False, spectra_type='lensed_scalar'):
    if runCAMB:
        cls, sim_params = get_spectra(random=random, times=times, spectra_type=spectra_type)
        
        # cls is power spectra wiht shape[N_l,5]. [:,0] is ell, index from 1 to 5 represent TT, EE, BB, TE
        data = Spectra(Cls = cls.transpose(), isCl = False, \
        Name = 'CMB Spectra', Checkl = False) #
        # .transpose() can change shape Cls, make the shape (N_l, 5) of Cls become (5,N_l)
        # Spectra function is checking ell, which didn't seem to help much
    # data.Cls = np.concatenate([data.Cls, np.zeros([3,len(data.Cls[0])])]) ## shape (8,N_l)
    # data.Cls = np.vstack((data.Cls,cl_phi))
    return data, sim_params  #

# this function is checking ell, which didn't seem to help much
class Spectra:
    def __init__(self, Cls = None, isCl = True, Name = '', Checkl = True):
        self.Name = Name
        self.Cls = Cls # cls is power spectra wiht shape[N_l,5]. [:,0] is ell, index from 1 to 5 represent TT, EE, BB, TE
        self.isCl = isCl
        self.isDl = not self.isCl
        if Checkl:
            self.lcheck()
    def lcheck(self, lmax = None):
        if type(self.Cls) != type(None):
            lmin = int(min(self.Cls[0])) # calculate minimum of ell
            if lmin > 0:
                tmp = np.zeros((np.shape(self.Cls)[0] - 1 , lmin))
                tmp1 = np.array([range(lmin)])
                tmp = np.concatenate((tmp1, tmp))
                self.Cls = np.concatenate((tmp, self.Cls), axis = 1)
            if lmax == None:
                lmax = max(self.Cls[0]) # calculate maximum of ell
            lmax = int(lmax)
            self.Cls = self.Cls.transpose()[:lmax + 1].transpose()
            # .transpose() can change shape Cls, make the shape (N_l, 5) of Cls become (5,N_l)
    def Cl2Dl(self):
        if self.isDl:
            raise ValueError('Spectra have already been multiplied by factor l(l+1)/2/pi')
        data = self.Cls.transpose()
        for i in range(len(data)):
            data[i][1:] = data[i][1:] * data[i][0] * (data[i][0] + 1.) / 2. / np.pi
        self.Cls = data.transpose()
        self.isDl = True
        self.isCl = False
    def Dl2Cl(self):
        if self.isCl:
            raise ValueError('Spectra have already been divided by factor l(l+1)/2/pi')
        data = self.Cls.transpose()
        for i in range(len(data)):
            if data[i][0] == 0.:
                continue
            data[i][1:] = data[i][1:] / data[i][0] / (data[i][0] + 1.) * 2. * np.pi
        self.Cls = data.transpose()
        self.isDl = False
        self.isCl = True

# this function return the cmb_specs in synfast model of PYSM
def ParametersSampling(random='Normal', times=5, spectra_type='lensed_scalar'):
#        Components.ParametersSampling(self)#original
#        data = ReadClFromCAMB('CMB_ML', params = self.paramsample, runCAMB = True)#original
    data, sim_params = ReadClFromPycamb(random=random, times=times, runCAMB = True, spectra_type=spectra_type) # added
    return data.Cls  # power specs, shape(8,N_l),
# [:,0] is ell, index from 1 to 5 represent TT, EE, BB, TE, index from 5 to 8 are



================================================
FILE: cmbfscnn/main.py
================================================

# -*- coding: utf-8 -*-


import sys
from cmbfscnn.utils import *
from cmbfscnn.CMBFS import CMBFSCNN
try:
    config_dir = sys.argv[1]
except:
    print("Please enter the configuration file")
    sys.exit(1)

config = load_pkl(config_dir)
cmbfcnn = CMBFSCNN(config)
cmbfcnn.run_CMBFSCNN()


================================================
FILE: cmbfscnn/namaster.py
================================================

# -*- coding: utf-8 -*-

import pymaster as nmt
import numpy as np



class Calculate_power_spectrum(object):
    def __init__(self, map1, map2, mask, nside=512, aposize=None, nlb=10, Bl=None,Dl=True):
        '''
        :param map1:  field 1, spin-1 (1-D array with shape (nside**2*12)) or spin-2 (2-D array). spin-0 field represents T map, and spin-2 field is Q+iU or Q-iU
        :param map2: field 1, spin-1 or spin-2
        :param mask: 1-D array with shape (nside**2*12,), the mask file used to the CMB map.
        :param aposize: float or None, apodization scale in degrees.
        :param nlb: int, the bin size (\delta_\ell) of multipoles, it can be set to ~ 1/fsky
        :param Dl: if True return Dl, if False return Cl
        '''
        self.map1 = map1
        self.map2 = map2
        self.mask = mask
        self.nside = nside
        self.aposize = aposize
        self.nlb = nlb
        self.Bl = Bl
        self.Dl = Dl
    
        
    def cl2dl(self, Cl, ell_start=2, ell_in=None, get_ell=True):
        '''
        calculate Dl from Cl
        ell_start: 0 or 2, which should depend on Dl
        ell_in: the ell of Cl (as the input of this function)
        '''
        if ell_start==0:
            lmax_cl = len(Cl) - 1
        elif ell_start==2:
            lmax_cl = len(Cl) + 1
        
        ell = np.arange(lmax_cl + 1)
        if ell_in is not None:
            if ell_start==2:
                ell[2:] = ell_in
        
        factor = ell * (ell + 1.) / 2. / np.pi
        if ell_start==0:
            Dl = np.zeros_like(Cl)
            Dl[2:] = Cl[2:] * factor[2:]
            ell_2 = ell
        elif ell_start==2:
            Dl = Cl * factor[2:]
            ell_2 = ell[2:]
        if get_ell:
            return ell_2, Dl
        else:
            return Dl
    

    def compute_master(self, f_a, f_b, wsp, clb=None):
        # Compute the power spectrum (a la anafast) of the masked fields
        # Note that we only use n_iter=0 here to speed up the computation,
        # but the default value of 3 is recommended in general.
        cl_coupled = nmt.compute_coupled_cell(f_a, f_b)
        # Decouple power spectrum into bandpowers inverting the coupling matrix
        cl_decoupled = wsp.decouple_cell(cl_coupled, cl_bias=clb)
        return cl_decoupled
    
    
    def get_power_spectra_for_spin1(self):
        '''
        Calculate Cl * ell*(ell+1)/2/np.pi for spin-1 including TT, QQ, and UU.
        '''
        if self.aposize is not None:    # apodize mask on a scale of self.aposize  deg 
            mask = nmt.mask_apodization(self.mask, aposize=self.aposize, apotype="Smooth")
        else:
            mask = self.mask
        # Create contaminated fields
        f_1 = nmt.NmtField(mask, [self.map1], templates=None, beam=self.Bl)
        f_2 = nmt.NmtField(mask, [self.map2], templates=None, beam=self.Bl)
        # Create binning scheme. We will use 20 multipoles per bandpower. nlb=\delta_ell ~ 1/fsky 
        b = nmt.NmtBin.from_nside_linear(self.nside, nlb=self.nlb, is_Dell=False) 
        
        # We then generate an NmtWorkspace object that we use to compute and store
        # the mode coupling matrix. Note that this matrix depends only on the masks
        # of the two fields to correlate, but not on the maps themselves (in this
        # case both maps are the same.
        w = nmt.NmtWorkspace()
        w.compute_coupling_matrix(f_1, f_2, b)
        cl_master = self.compute_master(f_1, f_2, w) # get power spectra Cl 
        ell = b.get_effective_ells()  # get ell
        if self.Dl:
            ell, dl_master = self.cl2dl(cl_master[0], ell_start=2, ell_in=ell) # get Dl
        else:
            ell = ell
            dl_master = cl_master[0]
        return ell, dl_master
    
    def get_power_spectra_for_spin2(self):
        '''
        Calculate Cl * ell*(ell+1)/2/np.pi for spin-2 including EE and BB.
        '''
        if self.aposize is not None:    # apodize mask on a scale of self.aposize  deg 
            mask = nmt.mask_apodization(self.mask, aposize=self.aposize, apotype="Smooth")
        else:
            mask = self.mask
        # Create contaminated fields
        f_1 = nmt.NmtField(mask, self.map1, templates=None, beam=self.Bl)
        f_2 = nmt.NmtField(mask, self.map2, templates=None, beam=self.Bl)
        if self.Dl:
            b = nmt.NmtBin.from_nside_linear(self.nside, nlb=self.nlb, is_Dell=True) 
        else:
            b = nmt.NmtBin.from_nside_linear(self.nside, nlb=self.nlb, is_Dell=False)
        w = nmt.NmtWorkspace()
        w.compute_coupling_matrix(f_1, f_2, b)
        Dl_master = self.compute_master(f_1, f_2, w)
        ell = b.get_effective_ells()
        return ell, Dl_master   # the shape of fl_22 is (4, cl): (EE,cl), (EB, cl), (BE,cl), (BB)
        



































================================================
FILE: cmbfscnn/plottor.py
================================================

# -*- coding: utf-8 -*-

import healpy as hp
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from mpl_toolkits.axes_grid1 import make_axes_locatable



def plot_sphere_map(denoise_map, target_map, title=[], range=[], save_dir = '',N_sample= 0):
    residual_map = target_map - denoise_map
    fig = plt.figure(1, figsize=(18, 6))
    cmap = plt.get_cmap(plt.cm.jet)
    hp.mollview(target_map[N_sample,:], fig=fig.number, cmap=cmap, sub=(1, 3, 1),
                unit=r'$\mathrm{\mu K}$',min=-1*range[0], max=range[0],title=title[0])
    hp.mollview(denoise_map[N_sample,:], fig=fig.number, cmap=cmap, sub=(1, 3, 2),
                unit=r'$\mathrm{\mu K}$', min=-1*range[1], max=range[1],title=title[1])
    hp.mollview(residual_map[N_sample,:], fig=fig.number, cmap=cmap, sub=(1, 3, 3),
                unit=r'$\mathrm{\mu K}$',min=-1*range[2], max=range[2], title=title[2])
    plt.subplots_adjust(top=0.97, bottom=0.06, left=0.1, right=0.97, hspace=0.01, wspace=0)
    # if not save_dir == None:
    plt.savefig(save_dir+'.png')
    # plt.show()


def plot_image(denoise_map, target_map, title, N_sample= 0,save_dir = '',range=[]):
    denoise_map, target_map = denoise_map[N_sample,:], target_map[N_sample,:]

    fig = plt.figure(figsize=(24, 24))
    gs0 = gridspec.GridSpec(2, 2, figure=fig)
    gs01 = gs0[0].subgridspec(1, 1)
    gs02 = gs0[1].subgridspec(1, 1)
    gs03 = gs0[2].subgridspec(1, 1)
    ax1 = fig.add_subplot(gs01[0])
    ax2 = fig.add_subplot(gs02[0],sharex=ax1,sharey=ax1)
    ax3 = fig.add_subplot(gs03[0],sharex=ax1,sharey=ax1)

    im1 = ax1.imshow(target_map, cmap=plt.cm.jet, vmin=-1*range[0], vmax=range[0])
    divider = make_axes_locatable(ax1)
    cax = divider.append_axes("right", size="5%", pad=0.1)
    cb = fig.colorbar(im1,cax=cax)
    cb.ax.tick_params(which='major',length=12, direction='in', width=3, labelsize=30)
    ax1.set_title(title[0], fontsize=30)


    im2 = ax2.imshow(denoise_map,  cmap=plt.cm.jet, vmin=-1*range[1], vmax=range[1])
    divider = make_axes_locatable(ax2)
    cax2 = divider.append_axes("right", size="5%", pad=0.1)
    cb2 = fig.colorbar(im2,cax=cax2)
    cb2.ax.tick_params(which='major',  length=12, direction='in', width=3, labelsize=30)
    ax2.set_title(title[1], fontsize=30)

    im3 = ax3.imshow(denoise_map-target_map, cmap=plt.cm.jet, vmin=-1*range[2], vmax=range[2])
    divider = make_axes_locatable(ax3)
    cax3 = divider.append_axes("right", size="5%", pad=0.1)
    cb = fig.colorbar(im3,cax=cax3)
    cb.ax.tick_params(which='major',  length=12, direction='in', width=3, labelsize=30)
    ax3.set_title(title[2], fontsize=30)

    fig.tight_layout()
    plt.subplots_adjust(top=0.98, bottom=0.06, left=0.05, right=0.94, hspace=0.04, wspace=0.4)
    plt.savefig(save_dir+'.png')
    # plt.show()
    return


def plot_EEBB_PS(ell, out_EE, tar_EE, out_BB, tar_BB, out_denoise_EE, true_EE, out_denoise_BB, true_BB):
    def make_plot(axs):
        box = dict(facecolor='yellow', pad=5, alpha=0.2)

        # Fixing random state for reproducibility

        ax1 = axs[0, 0]
        ax1.plot(ell, tar_EE, label="Simulated noisy EE", c='k')
        ax1.plot(ell, out_EE, label="Recovered noisy EE", c='r')
        ax1.set_ylabel('$D_{\ell}^{EE}$', fontsize=10)
        ax1.set_xlabel('$\ell$', fontsize=10)
        ax1.set_xlim(0, 1500)
        ax1.legend(loc='upper left', fontsize=7)

        ax3 = axs[1, 0]
        ax3.plot(ell, tar_BB, label="Simulated noisy BB", c='k')
        ax3.plot(ell, out_BB, label="Recovered noisy BB", c='r')
        ax3.set_ylabel('$D_{\ell}^{BB}$', fontsize=10)
        ax3.set_xlabel('$\ell$', fontsize=10)
        ax3.set_xlim(0, 1500)
        ax3.legend(loc='upper left', fontsize=7)

        ax2 = axs[0, 1]
        ax2.plot(ell, true_EE, label="True EE", c='k')
        ax2.plot(ell, out_denoise_EE, label="Recovered EE", c='r')
        ax2.set_ylabel('$D_{\ell}^{EE}$', fontsize=10)
        ax2.set_xlabel('$\ell$', fontsize=10)
        ax2.set_xlim(0, 1500)
        ax2.legend(loc='upper left', fontsize=7)

        ax4 = axs[1, 1]
        ax4.plot(ell, true_BB, label="True BB", c='k')
        ax4.plot(ell, out_denoise_BB, label="Recovered BB", c='r')
        ax4.set_ylabel('$D_{\ell}^{BB}$', fontsize=10)
        ax4.set_xlabel('$\ell$', fontsize=10)
        ax4.set_xlim(0, 1500)
        ax4.legend(loc='upper left', fontsize=7)

    # Plot 1:
    fig, axs = plt.subplots(2, 2)
    fig.subplots_adjust(left=0.2, wspace=0.6, hspace=0.6)
    make_plot(axs)

    # just align the last column of Axes:
    fig.align_ylabels(axs[:, 1])
    plt.savefig('power.png')
    # plt.show()



def plot_QQUU_PS(ell, out_QQ, tar_QQ, out_UU, tar_UU, out_denoise_QQ, true_QQ, out_denoise_UU, true_UU):
    def make_plot(axs):
        box = dict(facecolor='yellow', pad=5, alpha=0.2)

        # Fixing random state for reproducibility

        ax1 = axs[0, 0]
        ax1.plot(ell, tar_QQ, label="Simulated noisy QQ", c='k')
        ax1.plot(ell, out_QQ, label="Recovered noisy QQ", c='r')
        ax1.set_ylabel('$D_{\ell}^{QQ}$', fontsize=10)
        ax1.set_xlabel('$\ell$', fontsize=10)
        ax1.set_xlim(0, 1500)
        ax1.legend(loc='upper left', fontsize=7)

        ax3 = axs[1, 0]
        ax3.plot(ell, tar_UU, label="Simulated noisy UU", c='k')
        ax3.plot(ell, out_UU, label="Recovered noisy UU", c='r')
        ax3.set_ylabel('$D_{\ell}^{UU}$', fontsize=10)
        ax3.set_xlabel('$\ell$', fontsize=10)
        ax3.set_xlim(0, 1500)
        ax3.legend(loc='upper left', fontsize=7)

        ax2 = axs[0, 1]
        ax2.plot(ell, true_QQ, label="True QQ", c='k')
        ax2.plot(ell, out_denoise_QQ, label="Recovered QQ", c='r')
        ax2.set_ylabel('$D_{\ell}^{QQ}$', fontsize=10)
        ax2.set_xlabel('$\ell$', fontsize=10)
        ax2.set_xlim(0, 1500)
        ax2.legend(loc='upper left', fontsize=7)

        ax4 = axs[1, 1]
        ax4.plot(ell, true_UU, label="True UU", c='k')
        ax4.plot(ell, out_denoise_UU, label="Recovered UU", c='r')
        ax4.set_ylabel('$D_{\ell}^{BB}$', fontsize=10)
        ax4.set_xlabel('$\ell$', fontsize=10)
        ax4.set_xlim(0, 1500)
        ax4.legend(loc='upper left', fontsize=7)

    # Plot 1:
    fig, axs = plt.subplots(2, 2)
    fig.subplots_adjust(left=0.2, wspace=0.6, hspace=0.6)
    make_plot(axs)

    # just align the last column of Axes:
    fig.align_ylabels(axs[:, 1])
    plt.savefig('power.png')
    # plt.show()


================================================
FILE: cmbfscnn/Results.py
================================================

# -*- coding: utf-8 -*-
import numpy as np
from . import plottor as pt

from . import CMBFS_mode as Cm



class Plot_results(Cm.Calculate_power_spectra):
    def __init__(self,  result_dir = 'DATA_results/', is_half_split_map=True):
        super(Plot_results, self).__init__()
        self.result_dir = result_dir
        self.is_half_split_map = is_half_split_map
        self.save_PS_dir
        self._creat_ps_file

    def plot_predicted_sphere_map(self):
        if self.is_half_split_map:
            pre_cmbQ = np.load(getattr(self,'output_Q_dir_1'))
            target_cmbQ = np.load(getattr(self,'target_Q_dir_1'))
            pre_cmbU = np.load(getattr(self, 'output_U_dir_1'))
            target_cmbU = np.load(getattr(self, 'target_U_dir_1'))
        else:
            pre_cmbQ = np.load(getattr(self, 'output_Q_dir'))
            target_cmbQ = np.load(getattr(self, 'target_Q_dir'))
            pre_cmbU = np.load(getattr(self, 'output_U_dir'))
            target_cmbU = np.load(getattr(self, 'target_U_dir'))
        pt.plot_sphere_map(pre_cmbQ, target_cmbQ, title=['Simulated CMB Q map', 'Recovered CMB Q map', 'Residual'],
                        range=[10, 10, 0.2],save_dir='recover_CMB_Q_map',N_sample=0)
        pt.plot_sphere_map(pre_cmbU, target_cmbU, title=['Simulated CMB U map', 'Recovered CMB U map', 'Residual'],
                        range=[10, 10, 0.2], save_dir='recover_CMB_U_map',N_sample=0)

    def plot_predicted_flat_map(self):
        if self.is_half_split_map:
            pre_cmbQ = np.load(getattr(self, 'output_Qmap_dir') + 'predicted_CMB_Q'  + '_map_half_1.npy')
            target_cmbQ = np.load(getattr(self, 'output_Qmap_dir') + 'target_CMB_Q'  + '_map_half_1.npy')
            pre_cmbU = np.load(getattr(self, 'output_Umap_dir') + 'predicted_CMB_U' + '_map_half_1.npy')
            target_cmbU = np.load(getattr(self, 'output_Umap_dir') + 'target_CMB_U' + '_map_half_1.npy')
        else:
            pre_cmbQ = np.load(getattr(self, 'output_Qmap_dir') + 'predicted_CMB_Q' + '_map.npy')
            target_cmbQ = np.load(getattr(self, 'output_Qmap_dir') + 'target_CMB_Q' + '_map.npy')
            pre_cmbU = np.load(getattr(self, 'output_Umap_dir') + 'predicted_CMB_U' + '_map.npy')
            target_cmbU = np.load(getattr(self, 'output_Umap_dir') + 'target_CMB_U' + '_map.npy')
        title1,title2 = ['Simulated CMB Q map', 'Recovered CMB Q map', 'Residual'], ['Simulated CMB U map', 'Recovered CMB U map', 'Residual']
        pt.plot_image(pre_cmbQ, target_cmbQ, title1, N_sample=0, save_dir='recovered_CMB_flat_Qmap', range=[10,10,0.2])
        pt.plot_image(pre_cmbU, target_cmbU, title1, N_sample=0, save_dir='recovered_CMB_flat_Umap', range=[10, 10, 0.2])

    def plot_recovered_CMB_QU_PS(self,nlb=5):
        pre_cmbQ_ps = np.load(getattr(self, 'save_output_Q_dir_1').format(nlb))
        pre_cmbU_ps = np.load(getattr(self, 'save_output_U_dir_1').format(nlb))
        tar_cmbQ_ps = np.load(getattr(self, 'save_target_Q_dir_1').format(nlb))
        tar_cmbU_ps = np.load(getattr(self, 'save_target_U_dir_1').format(nlb))
        true_cmb_Q_ps = np.load(getattr(self, 'true_output_Q_dir').format(nlb))
        true_cmb_U_ps = np.load(getattr(self, 'true_output_U_dir').format(nlb))
        pre_denoise_Q_ps = np.load(getattr(self, 'save_output_cros_Q_dir').format(nlb))
        pre_denoise_U_ps = np.load(getattr(self, 'save_output_cros_U_dir').format(nlb))
        print('+++++++++++', pre_cmbQ_ps.shape)
        pt.plot_QQUU_PS(ell=pre_cmbQ_ps[0,0,:], out_QQ=pre_cmbQ_ps[0,1,:], tar_QQ=tar_cmbQ_ps[0,1,:], out_UU=pre_cmbU_ps[0,1,:],
                     tar_UU=tar_cmbU_ps[0,1,:], out_denoise_QQ=pre_denoise_Q_ps[0,1,:], true_QQ=true_cmb_Q_ps[0,1,:],
                     out_denoise_UU=pre_denoise_U_ps[0,1,:], true_UU=true_cmb_U_ps[0,1,:])

    def plot_recovered_CMB_EB_PS(self,nlb=5):
        pre_cmbEB_ps = np.load(getattr(self, 'save_output_EB_dir').format(nlb))
        tar_cmbEB_ps = np.load(getattr(self, 'save_target_EB_dir').format(nlb))
        true_cmbEB_ps = np.load(getattr(self, 'save_true_EB_dir').format(nlb))
        pre_denoise_cmbEB_ps = np.load(getattr(self, 'save_output_cros_EB_dir').format(nlb))
        pt.plot_EEBB_PS(ell=pre_cmbEB_ps[0,0,:], out_EE=pre_cmbEB_ps[0,1,:], tar_EE=tar_cmbEB_ps[0,1,:], out_BB=pre_cmbEB_ps[0,2,:], tar_BB=tar_cmbEB_ps[0,2,:],
                     out_denoise_EE=pre_denoise_cmbEB_ps[0,1,:], true_EE=true_cmbEB_ps[0,1,:], out_denoise_BB=pre_denoise_cmbEB_ps[0,2,:], true_BB=true_cmbEB_ps[0,2,:])













================================================
FILE: cmbfscnn/simulate_sky_map.py
================================================
# -*- coding: utf-8 -*-

import os
import sys
import numpy as np
import healpy as hp
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)
import pysm
from pysm.nominal import models
from pysm.common import convert_units

from . import get_power_sperctra as ps

def c2_mode(nside):
    return [{
        'model' : 'taylens',
        'nside' : nside,
        'cmb_seed' : 1111,
        'delens': False,
        'output_unlens': False
        }]


def c2_unlens_mode(nside):
    return [{
        'model' : 'taylens',
        'nside' : nside,
        'cmb_seed' : 1111,
        'delens': False,
        'output_unlens':True
        }]


def get_specificRandn(n, mu, sigma, range_min, range_max):
    randn = np.random.randn(n) * sigma + mu
    choose_1 = np.where(randn>=range_min)
    randn = randn[choose_1]
    choose_2 = np.where(randn<=range_max)
    randn = randn[choose_2]
    return randn


class Get_data(object):
    def __init__(self, Nside,  config_random = {}, freqs=None,  using_beam = False,
                  beam = None, out_unit = None):
        self.Nside = Nside
        self.freqs = freqs
        self.using_beam = using_beam
        self.beam = beam
        self.out_unit = out_unit # the unit of output, 'K_CMB', 'Jysr', 'uK_RJ';
        # Default unit of signals is 'uK_RJ'; Default unit of noise may be 'K_CMB'
        self.config_random = config_random

    def data(self):
        # random can be 'fixed', fix cosmological paramaters

        cmb_specs = ps.ParametersSampling(random=self.config_random['Random_types_of_cosmological_parameters'], spectra_type='unlensed_scalar')
        s1 = models("s1", self.Nside)
        d1 = models("d1", self.Nside)
        a2 = models("a2", self.Nside)
        c2 = c2_mode(self.Nside)
        c2[0]['cmb_specs'] = cmb_specs
        c1_seed = self.config_random['cmb_seed']
        c2[0]['cmb_seed'] = c1_seed

        c2_unlens = c2_unlens_mode(self.Nside)
        cmb_spe = cmb_specs.copy()
        c2_unlens[0]['cmb_specs'] = cmb_spe
        c2_unlens[0]['cmb_seed'] = c1_seed

        # random of syn parameters
        # np.random.seed(int(seed+100**1))
        s1_seed = self.config_random['syn_seed']
        np.random.seed(int(s1_seed))
        if 'syn_spectralindex_random' not in self.config_random:
            s1_index_std = 0
        else:
            s1_index_std, syn_index_ran_class = self.config_random['syn_spectralindex_random']
        if s1_index_std == 0:
            print('Note! syn_spectralindex are not random')
        else:
            if syn_index_ran_class == 'one' :
                s1[0]['spectral_index'] = s1[0]['spectral_index'] + \
                                          get_specificRandn(50, 0, s1_index_std, -2 * s1_index_std, 2 * s1_index_std)[
                                              0] * (s1[0]['spectral_index'])
            elif syn_index_ran_class == 'multi' :
                pixel_n = len(s1[0]['spectral_index'])
                s1[0]['spectral_index'] = s1[0]['spectral_index'] + \
                                          get_specificRandn(pixel_n * 2, 0, s1_index_std,
                                                                                      -2 * s1_index_std,
                                                                                      2 * s1_index_std)[:pixel_n] * (
                                                      s1[0]['spectral_index'])
            else:
                print('Note! syn_spectralindex config error')

        if 'syn_amplitude_random' not in self.config_random:
            s1_A_std = 0
        else:
            s1_A_std, syn_A_ran_class = self.config_random['syn_amplitude_random']
        if s1_A_std == 0:
            print('Note! syn_amplitude are not random')
        else:
            if syn_A_ran_class == 'one':
                s1[0]['A_I'] = s1[0]['A_I'] + get_specificRandn(50, 0, s1_A_std, -2 * s1_A_std, 2 * s1_A_std)[0] * \
                               s1[0]['A_I']
                s1[0]['A_Q'] = s1[0]['A_Q'] + get_specificRandn(50, 0, s1_A_std, -2 * s1_A_std, 2 * s1_A_std)[0] * \
                               s1[0]['A_Q']
                s1[0]['A_U'] = s1[0]['A_U'] + get_specificRandn(50, 0, s1_A_std, -2 * s1_A_std, 2 * s1_A_std)[0] * \
                               s1[0]['A_U']
            elif syn_A_ran_class == 'multi':
                pixel_n = len(s1[0]['A_I'])
                s1[0]['A_I'] = s1[0]['A_I'] + get_specificRandn(pixel_n*2, 0, s1_A_std, -2 * s1_A_std, 2 * s1_A_std)[
                                              :pixel_n] * s1[0]['A_I']
                s1[0]['A_Q'] = s1[0]['A_Q'] + get_specificRandn(pixel_n*2, 0, s1_A_std, -2 * s1_A_std, 2 * s1_A_std)[
                                              :pixel_n] * s1[0]['A_Q']
                s1[0]['A_U'] = s1[0]['A_U'] + get_specificRandn(pixel_n*2, 0, s1_A_std, -2 * s1_A_std, 2 * s1_A_std)[
                                              :pixel_n] * s1[0]['A_U']
            else:
                print('Note! syn_amplitude_random config error')

        # random of dust parameters
        # np.random.seed(int(seed + 100 ** 2))
        d1_seed = self.config_random['dust_seed']
        np.random.seed(int(d1_seed))
        if 'dust_amplitude_random' not in self.config_random:
            d1_A_std = 0
        else:
            d1_A_std, dust_A_ran_class = self.config_random['dust_amplitude_random']
        if d1_A_std == 0 :
            print('Note! dust_amplitude are not random')
        else:
            if dust_A_ran_class == 'one':
                d1[0]['A_I'] = d1[0]['A_I'] + get_specificRandn(50, 0, d1_A_std, -2 * d1_A_std, 2 * d1_A_std)[0] * \
                               d1[0]['A_I']
                d1[0]['A_Q'] = d1[0]['A_Q'] + get_specificRandn(50, 0, d1_A_std, -2 * d1_A_std, 2 * d1_A_std)[0] * \
                               d1[0]['A_Q']
                d1[0]['A_U'] = d1[0]['A_U'] + get_specificRandn(50, 0, d1_A_std, -2 * d1_A_std, 2 * d1_A_std)[0] * \
                               d1[0]['A_U']
            elif dust_A_ran_class == 'multi':
                pixel_n = len(d1[0]['A_I'])
                d1[0]['A_I'] = d1[0]['A_I'] + get_specificRandn(pixel_n*2, 0, d1_A_std, -2 * d1_A_std, 2 * d1_A_std)[:pixel_n] * \
                               d1[0]['A_I']
                d1[0]['A_Q'] = d1[0]['A_Q'] + get_specificRandn(pixel_n*2, 0, d1_A_std, -2 * d1_A_std, 2 * d1_A_std)[:pixel_n] * \
                               d1[0]['A_Q']
                d1[0]['A_U'] = d1[0]['A_U'] + get_specificRandn(pixel_n*2, 0, d1_A_std, -2 * d1_A_std, 2 * d1_A_std)[:pixel_n] * \
                               d1[0]['A_U']
            else:
                print('Note! dust_amplitude config error')

        if 'dust_spectralindex_random' not in self.config_random:
            d1_index_std = 0
        else:
            d1_index_std, dust_index_ran_class = self.config_random['dust_spectralindex_random']
        if d1_index_std == 0:
            print('Note! dust_spectralindex are not random')
        else:
            if dust_index_ran_class == 'one' :
                mss = d1[0]['spectral_index']
                d1[0]['spectral_index'] = mss + \
                                          get_specificRandn(50, 0, d1_index_std, -2 * d1_index_std, 2 * d1_index_std)[
                                              0] * (mss - 2.)
            elif dust_index_ran_class == 'multi' :
                pixel_n = len(s1[0]['spectral_index'])
                mss = d1[0]['spectral_index']
                d1[0]['spectral_index'] = mss + \
                                          get_specificRandn(pixel_n*2, 0, d1_index_std, -2 * d1_index_std, 2 * d1_index_std)[
                                              :pixel_n] * (mss - 2.)
            else:
                print('Note! syn_spectralindex config error')

        if 'dust_temp_random' not in self.config_random:
            d1_temp_std = 0
        else:
            d1_temp_std, dust_temp_ran_class = self.config_random['dust_temp_random']
        if d1_temp_std == 0:
            print('Note! dust_temp are not random')
        else:
            if dust_temp_ran_class == 'one':
                d1[0]['temp'] = d1[0]['temp'] + get_specificRandn(50, 0, d1_temp_std, -2 * d1_temp_std, 2 * d1_temp_std)[0] * \
                               d1[0]['temp']
            elif dust_temp_ran_class == 'multi':
                pixel_n = len(d1[0]['temp'])
                d1[0]['temp'] = d1[0]['temp'] + get_specificRandn(pixel_n*2, 0, d1_temp_std, -2 * d1_temp_std, 2 * d1_temp_std)[
                                              :pixel_n] * \
                               d1[0]['temp']
            else:
                print('Note! syn_temp config error')

        a1_seed = self.config_random['ame_seed']
        np.random.seed(int(a1_seed))
        if 'ame_amplitude_random' not in self.config_random:
            a1_A_std = 0
        else:
            a1_A_std, ame_A_ran_class = self.config_random['ame_amplitude_random']
        if a1_A_std == 0:
            print('Note! ame_amplitude are not random')
        else:
            if ame_A_ran_class == 'one':
                a2[0]['A_I'] = a2[0]['A_I'] + get_specificRandn(50, 0, a1_A_std, -2 * a1_A_std, 2 * a1_A_std)[0] * \
                               a2[0]['A_I']
            elif ame_A_ran_class == 'multi':
                pixel_n = len(a2[0]['A_I'])
                a2[0]['A_I'] = a2[0]['A_I'] + get_specificRandn(pixel_n*2, 0, a1_A_std, -2 * a1_A_std, 2 * a1_A_std)[:pixel_n] * \
                               a2[0]['A_I']
            else:
                print('Note! ame_A config error')

        sky_config = {
            'synchrotron': s1,
            'dust': d1,
            'cmb': c2,
            'ame': a2,
            # 'freefree': f1
        }
        sky = pysm.Sky(sky_config)  #

        total = sky.signal()(self.freqs)
        total = total.astype(np.float32)

        cmb = sky.cmb(self.freqs)
        cmb = cmb.astype(np.float32)
        if not self.out_unit == None:
            Uc_signal = np.array(convert_units("uK_RJ", self.out_unit, self.freqs))
            if not len(self.freqs)>1:  # one frequence
                cmb = cmb * Uc_signal[:, None, None]
                total = total * Uc_signal[:, None, None]
            else:
                cmb = cmb * Uc_signal[:, None, None]
                total = total * Uc_signal[:, None, None]

        cmb = self.data_proce_beam(cmb,using_beam_1=self.using_beam, beam_1=self.beam)
        total = self.data_proce_beam(total, using_beam_1=self.using_beam, beam_1=self.beam)
        return cmb, total

    def data_proce_beam(self, map_da,using_beam_1=False, beam_1=None):
        if using_beam_1:
            beam = beam_1
            map_n = np.array(
                [hp.smoothing(m, fwhm=np.pi / 180. * b / 60., verbose=False) for (m, b) in zip(map_da, beam)])
        else:
            map_n = map_da
            # dd = map_new
        return map_n

    def noiser(self, Sens, is_half_split_map = True):
        """Calculate white noise maps for given sensitivities.  Returns noise, and noise maps at the given nside in (T, Q, U). Input
        sensitivities are expected to be in uK_CMB amin for the rest of
        PySM.

        :param is_half_split_map: If it is an half-split map, the noise level will increase by sqrt(2) times

        """
        # solid angle per pixel in amin2
        npix = hp.nside2npix(self.Nside)
        # solid angle per pixel in amin2, Note!!!!!
        pix_amin2 = 4. * np.pi / float(hp.nside2npix(self.Nside)) * (180. * 60. / np.pi) ** 2
        """sigma_pix_I/P is std of noise per pixel. It is an array of length
        equal to the number of input maps."""
        if is_half_split_map:
            sigma_pix_I = np.sqrt(Sens ** 2 / pix_amin2)*np.sqrt(2)
        else:
            sigma_pix_I = np.sqrt(Sens ** 2 / pix_amin2)
        noise = np.random.randn(len(Sens), 3,npix)
        noise *= sigma_pix_I[:, None,None]
        if not self.out_unit ==None:
            Uc_noise = np.array(convert_units("uK_CMB", self.out_unit, self.freqs))
            noise = noise * Uc_noise[:, None, None]
        return noise.astype(np.float32)









================================================
FILE: cmbfscnn/spherical.py
================================================
# -*- coding: utf-8 -*-

import numpy as np
import healpy as hp


class Cut(object):
    '''
    cut a Healpix map to 12 parts, or to 12*subblocks_nums parts
    '''

    def __init__(self, maps_in, subblocks_nums=1, nest=False):
        '''
        :param maps_in: the input healpix maps (one map with the shape of (12*nside**2,) or multiple maps with the shape of (N, 12*nside**2))
        :param nest: bool, if False map_in is assumed in RING scheme, otherwise map_in is NESTED
        :param subblocks_nums: int, the number after dividing the square nside*nside into small squares,
                               subblocks_nums=1^2, 2^2, 4^2, 8^2, 16^2, ..., default 1
        '''
        self.maps_in = maps_in
        self.nest = nest
        self.subblocks_nums = subblocks_nums

    @property
    def _multi_map(self):
        if len(self.maps_in.shape) == 1:
            return False
        elif len(self.maps_in.shape) == 2:
            self.multi_map_n = self.maps_in.shape[0]
            return True

    @property
    def nside(self):
        if self._multi_map:
            nsd = int(np.sqrt(self.maps_in[0].shape[0] / self.subblocks_nums / 12))
        else:
            nsd = int(np.sqrt(self.maps_in.shape[0] / self.subblocks_nums / 12))
        return nsd

    def _expand_array(self, original_array):
        '''
        to be used in nestedArray2nestedMap, expand the given small array into a large array

        :param original_array: with the shape of (2**n, 2**n), where n=1, 2, 4, 6, 8, 10, ...
        '''
        add_value = original_array.shape[0] ** 2
        array_0 = original_array
        array_1 = array_0 + add_value
        array_2 = array_0 + add_value * 2
        array_3 = array_0 + add_value * 3
        array_3_1 = np.c_[array_3, array_1]
        array_2_0 = np.c_[array_2, array_0]
        array = np.r_[array_3_1, array_2_0]
        return array

    def _ordinal_array(self):
        '''
        obtain an array containing the ordinal number, the shape is (nside, nside)
        '''
        circle_num = (int(np.log2(self.nside ** 2)) - 2) // 2  # use //
        ordinal_array = np.array([[3., 1.], [2., 0.]])
        for i in range(circle_num):
            ordinal_array = self._expand_array(ordinal_array)
        return ordinal_array, circle_num

    def nestedArray2nestedMap(self, map_cut):
        '''
        reorder the cut map into NESTED ordering to show the same style using
        plt.imshow() as that using Healpix

        :param map_cut: the cut map, the shape of map_cut is (nside**2,)

        return the reorded data, the shape is (nside, nside)
        '''
        array_fill, circle_num = self._ordinal_array()
        # for i in range(2 ** (circle_num + 1)):
        #     for j in range(2 ** (circle_num + 1)):
        #         array_fill[i][j] = map_cut[int(array_fill[i][j])]
        array_fill = map_cut[array_fill.reshape(1, -1).astype(int)].reshape(2 ** (circle_num + 1),
                                                                            2 ** (circle_num + 1))

        # array_fill should be transposed to keep the figure looking like that in HEALPix

        return array_fill

    def nestedMap2nestedArray(self, map_block):
        '''
        Restore the cut map(1/12 of full sky map) into an array which is in NESTED ordering

        need transpose if the map is transposed in nestedArray2nestedMap function
        '''
        map_cut = np.ones(self.nside ** 2)
        array_fill, circle_num = self._ordinal_array()
        for i in range(2 ** (circle_num + 1)):
            for j in range(2 ** (circle_num + 1)):
                map_cut[int(array_fill[i][j])] = map_block[i][j]
        return map_cut

    def _block(self, Map, block_n):
        '''
        return one block of one of the original maps
        '''
        if not self.nest:
            # reorder the map from RING ordering to NESTED
            map_NEST = hp.reorder(Map, r2n=True)

        map_part = map_NEST[block_n * self.nside ** 2: (block_n + 1) * self.nside ** 2]
        map_part = self.nestedArray2nestedMap(map_part)
        return map_part

    def block(self, block_n):
        if self._multi_map:
            map_part = []
            for i in range(self.multi_map_n):
                map_part.append(self._block(self.maps_in[i], block_n))
        else:
            map_part = self._block(self.maps_in, block_n)
        return map_part

    def _block_all(self, Map):
        '''
        return all blocks (12 blocks) of one of the original maps
        '''
        map_parts = []
        for blk in range(12 * self.subblocks_nums):
            map_parts.append(self._block(Map, blk))
        return map_parts

    def block_all(self):
        if self._multi_map:
            map_parts = []
            for i in range(self.multi_map_n):
                map_parts.append(self._block_all(self.maps_in[i]))
        else:
            map_parts = self._block_all(self.maps_in)
        return map_parts


class Block2Full(Cut):
    '''
    stitch a cut map (1/12 of full sky map) to a full sky map with other parts is zeros
    '''

    def __init__(self, maps_block, block_n, subblocks_nums=1, base_map=None):
        '''
        :param maps_block: the cut map in NESTED ording (one map in 2D array with the shape of (nside, nside)
        or multiple maps in 3D array with the shape of (N, nside, nside) or multiple maps in a list with each element has the shape of (nside,nside))
        :param block_n: int, the number of cut map, 0, 1, 2, ..., 11
        :param subblocks_nums: int, the number after dividing the square nside*nside into small squares,
                               subblocks_nums=1^2, 2^2, 4^2, 8^2, 16^2, ..., default 1
        '''
        self.maps_block = maps_block
        self.block_n = block_n
        self.base_map = base_map
        self.subblocks_nums = subblocks_nums

    @property
    def _multi_map(self):
        # list -> array
        if isinstance(self.maps_block, list):
            self.maps_block = np.array(self.maps_block)

        if len(self.maps_block.shape) == 2:
            return False
        elif len(self.maps_block.shape) == 3:
            self.multi_map_n = self.maps_block.shape[0]
            return True

    @property
    def nside(self):
        if self._multi_map:
            nsd = self.maps_block.shape[1]
        else:
            nsd = self.maps_block.shape[0]
        return nsd

    def _full(self, map_block):
        '''
        return a full sphere map
        '''
        map_block_array = self.nestedMap2nestedArray(map_block)
        if self.base_map is None:
            map_NEST = np.zeros(12 * self.subblocks_nums * self.nside ** 2)
        else:
            map_NEST = hp.reorder(self.base_map, r2n=True)
        map_NEST[self.block_n * self.nside ** 2: (self.block_n + 1) * self.nside ** 2] = map_block_array
        map_RING = hp.reorder(map_NEST, n2r=True)
        return map_RING

    def full(self):
        if self._multi_map:
            map_full = []
            for i in range(self.multi_map_n):
                map_full.append(self._full(self.maps_block[i]))
        else:
            map_full = self._full(self.maps_block)
        return map_full


# %%
def sphere2piecePlane(sphere_map, nside=256):
    '''
    cut full map to 12 blocks, then piecing them together into a plane
    this is only for the case of subblocks_nums=1
    '''
    blocks = Cut(sphere_map).block_all()

    piece_map = np.zeros((nside * 4, nside * 3))
    # part 1
    piece_map[nside * 3:, :nside] = blocks[1]  # block 1
    piece_map[nside * 3:, nside:nside * 2] = blocks[5]
    piece_map[nside * 3:, nside * 2:] = blocks[8]
    # part 2
    piece_map[nside * 2:nside * 3, :nside] = blocks[0]
    piece_map[nside * 2:nside * 3, nside:nside * 2] = blocks[4]
    piece_map[nside * 2:nside * 3, nside * 2:] = blocks[11]
    # part 3
    piece_map[nside:nside * 2, :nside] = blocks[3]
    piece_map[nside:nside * 2, nside:nside * 2] = blocks[7]
    piece_map[nside:nside * 2, nside * 2:] = blocks[10]
    # part 4
    piece_map[:nside, :nside] = blocks[2]
    piece_map[:nside, nside:nside * 2] = blocks[6]
    piece_map[:nside, nside * 2:] = blocks[9]
    return piece_map







def sphere2piecePlane_mult(sphere_map, nside=256):
    '''
    cut full map to 12 blocks, then piecing them together into a plane
    this is only for the case of subblocks_nums=1
    '''
    if len(sphere_map.shape) == 1:
        multi_map = False
    elif len(sphere_map.shape) == 2:
        multi_map = True
        multi_map_n = sphere_map.shape[0]

    blocks = Cut(sphere_map).block_all()

    if multi_map:
        piece_map = np.zeros((multi_map_n, nside * 4, nside * 3))
        for i in range(multi_map_n):
            # part 1
            piece_map[i, nside * 3:, :nside] = blocks[i][1]  # block 1
            piece_map[i, nside * 3:, nside:nside * 2] = blocks[i][5]
            piece_map[i, nside * 3:, nside * 2:] = blocks[i][8]
            # part 2
            piece_map[i, nside * 2:nside * 3, :nside] = blocks[i][0]
            piece_map[i, nside * 2:nside * 3, nside:nside * 2] = blocks[i][4]
            piece_map[i, nside * 2:nside * 3, nside * 2:] = blocks[i][11]
            # part 3
            piece_map[i, nside:nside * 2, :nside] = blocks[i][3]
            piece_map[i, nside:nside * 2, nside:nside * 2] = blocks[i][7]
            piece_map[i, nside:nside * 2, nside * 2:] = blocks[i][10]
            # part 4
            piece_map[i, :nside, :nside] = blocks[i][2]
            piece_map[i, :nside, nside:nside * 2] = blocks[i][6]
            piece_map[i, :nside, nside * 2:] = blocks[i][9]
    else:
        piece_map = np.zeros((nside * 4, nside * 3))
        # part 1
        piece_map[nside * 3:, :nside] = blocks[1]  # block 1
        piece_map[nside * 3:, nside:nside * 2] = blocks[5]
        piece_map[nside * 3:, nside * 2:] = blocks[8]
        # part 2
        piece_map[nside * 2:nside * 3, :nside] = blocks[0]
        piece_map[nside * 2:nside * 3, nside:nside * 2] = blocks[4]
        piece_map[nside * 2:nside * 3, nside * 2:] = blocks[11]
        # part 3
        piece_map[nside:nside * 2, :nside] = blocks[3]
        piece_map[nside:nside * 2, nside:nside * 2] = blocks[7]
        piece_map[nside:nside * 2, nside * 2:] = blocks[10]
        # part 4
        piece_map[:nside, :nside] = blocks[2]
        piece_map[:nside, nside:nside * 2] = blocks[6]
        piece_map[:nside, nside * 2:] = blocks[9]
    return piece_map

# sphere2piecePlane_squa



def sphere2piecePlane_squa(sphere_map, nside=512):
    '''
    cut full map to 12 blocks, then piecing them together into a plane
    this is only for the case of subblocks_nums=1
    '''
    blocks = Cut(sphere_map).block_all()

    piece_map = np.zeros((nside * 5, nside * 5))
    piece_map[4 * nside:5 * nside, 0:nside] = blocks[4]
    piece_map[4 * nside:5 * nside, nside:2 * nside] = blocks[8]
    piece_map[3 * nside:4 * nside, 0:nside] = blocks[0]

    piece_map[3 * nside:4 * nside, nside:2 * nside] = blocks[5]
    piece_map[3 * nside:4 * nside, 2 * nside:3 * nside] = blocks[9]
    piece_map[2 * nside:3 * nside, nside:2 * nside] = blocks[1]
    piece_map[2 * nside:3 * nside, 2 * nside:3 * nside] = blocks[6]
    piece_map[2 * nside:3 * nside, 3 * nside:4 * nside] = blocks[10]
    piece_map[nside:2 * nside, 2 * nside:3 * nside] = blocks[2]
    piece_map[nside:2 * nside, 3 * nside:4 * nside] = blocks[7]
    piece_map[nside:2 * nside, 4 * nside:5 * nside] = blocks[11]
    piece_map[0:nside, 3 * nside:4 * nside] = blocks[3]
    piece_map[0:nside, 4 * nside:5 * nside] = blocks[4]
    return piece_map

def sphere2piecePlane_squa_mult(sphere_map, nside=512):
    if len(sphere_map.shape) == 1:
        multi_map = False
    elif len(sphere_map.shape) == 2:
        multi_map = True
        multi_map_n = sphere_map.shape[0]

    if multi_map:
        piece_map = np.zeros((multi_map_n, nside * 5, nside * 5))
        for i in range(multi_map_n):
            sphere_map_i = sphere_map[i]
            piece_map[i,:] = sphere2piecePlane_squa(sphere_map_i, nside)

        return piece_map

    else:
        return sphere2piecePlane_squa(sphere_map, nside)


def sphere2piecePlane_squa_pad(sphere_map, nside=512, padding_size = 128):
    '''
    cut full map to 12 blocks, then piecing them together into a plane
    this is only for the case of subblocks_nums=1
    '''
    mult = 2
    pad = padding_size
    blocks = Cut(sphere_map).block_all()

    piece_map = np.zeros((nside * 5, nside * 5))
    piece_map[4 * nside:5 * nside, 0:nside] = blocks[4]
    piece_map[4 * nside:5 * nside, nside:2 * nside] = blocks[8]
    piece_map[3 * nside:4 * nside, 0:nside] = blocks[0]
    #
    piece_map[2 * nside:3 * nside, nside - 32 * mult:nside] = np.rot90(blocks[0][0:32 * mult, :], k=3, axes=(
        0, 1))  # np.rot90(blocks[0], k=3, axes=(0, 1))
    piece_map[3 * nside - 32 * mult:3 * nside, 0:nside] = np.rot90(blocks[1][:, 0:32 * mult], k=1, axes=(
        0, 1))  # np.transpose(blocks[1][:, 0:32*3])  #np.rot90(blocks[1][:, 0:32], k=1, axes=(0, 1))
    piece_map[1 * nside:2 * nside, (2 * nside) - 32 * mult:2 * nside] = np.rot90(blocks[1][0:32 * mult, :], k=3,
                                                                                 axes=(0, 1))
    piece_map[2 * nside - 32 * mult:2 * nside, nside:nside * 2] = np.rot90(blocks[2][:, 0:32 * mult], k=1,
                                                                           axes=(0, 1))
    piece_map[0 * nside:1 * nside, (3 * nside) - 32 * mult:3 * nside] = np.rot90(blocks[2][0:32 * mult, :], k=3,
                                                                                 axes=(0, 1))
    piece_map[1 * nside - 32 * mult:1 * nside, 2 * nside:nside * 3] = np.rot90(blocks[3][:, 0:32 * mult], k=1,
                                                                               axes=(0, 1))

    piece_map[4 * nside:5 * nside, 2 * nside:nside * 2 + 32 * mult] = np.rot90(
        blocks[9][nside - 32 * mult:nside, :], k=3, axes=(0, 1))
    piece_map[4 * nside:4 * nside + 32 * mult, 2 * nside:nside * 3] = np.rot90(
        blocks[8][:, nside - 32 * mult:nside], k=1,
        axes=(0, 1))
    piece_map[3 * nside:4 * nside, 3 * nside:nside * 3 + 32 * mult] = np.rot90(
        blocks[10][nside - 32 * mult:nside, :], k=3,
        axes=(0, 1))
    piece_map[3 * nside:3 * nside + 32 * mult, 3 * nside:nside * 4] = np.rot90(
        blocks[9][:, nside - 32 * mult:nside], k=1,
        axes=(0, 1))
    piece_map[2 * nside:3 * nside, 4 * nside:nside * 4 + 32 * mult] = np.rot90(
        blocks[11][nside - 32 * mult:nside, :], k=3,
        axes=(0, 1))
    piece_map[2 * nside:2 * nside + 32 * mult, 4 * nside:nside * 5] = np.rot90(
        blocks[10][:, nside - 32 * mult:nside], k=1,
        axes=(0, 1))

    piece_map[3 * nside:4 * nside, nside:2 * nside] = blocks[5]
    piece_map[3 * nside:4 * nside, 2 * nside:3 * nside] = blocks[9]
    piece_map[2 * nside:3 * nside, nside:2 * nside] = blocks[1]
    piece_map[2 * nside:3 * nside, 2 * nside:3 * nside] = blocks[6]
    piece_map[2 * nside:3 * nside, 3 * nside:4 * nside] = blocks[10]
    piece_map[nside:2 * nside, 2 * nside:3 * nside] = blocks[2]
    piece_map[nside:2 * nside, 3 * nside:4 * nside] = blocks[7]
    piece_map[nside:2 * nside, 4 * nside:5 * nside] = blocks[11]
    piece_map[0:nside, 3 * nside:4 * nside] = blocks[3]
    piece_map[0:nside, 4 * nside:5 * nside] = blocks[4]

    piece_map = np.pad(piece_map, pad_width=pad, mode='constant')
    piece_map[pad + 5 * nside:pad + 5 * nside + 32 * mult, pad + 1 * nside:pad + nside * 2] = np.rot90(
        blocks[11][:, nside - 32 * mult:nside], k=1,
        axes=(0, 1))
    piece_map[pad + 1 * nside:pad + 2 * nside, pad + 5 * nside:pad + 5 * nside + 32 * mult] = np.rot90(
        blocks[8][nside - 32 * mult:nside, :], k=3,
        axes=(0, 1))
    piece_map[pad - 32 * mult:pad, pad + 3 * nside:pad + nside * 4] = np.rot90(
        blocks[0][:, 0: 32 * mult], k=1,
        axes=(0, 1))
    piece_map[pad + 3 * nside:pad + 4 * nside, pad - 32 * mult:pad] = np.rot90(
        blocks[3][0: 32 * mult, :], k=3,
        axes=(0, 1))

    piece_map[pad + 4 * nside:pad + 5 * nside, pad - 32 * mult:pad] = blocks[3][:, nside - 32 * mult:nside]
    piece_map[pad + 5 * nside:pad + 5 * nside + 32 * mult, pad + 0 * nside:pad + 1 * nside] = blocks[11][
                                                                                              0: 32 * mult, :]
    piece_map[pad - 32 * mult:pad, pad + 4 * nside:pad + 5 * nside] = blocks[0][nside - 32 * mult:nside, :]
    piece_map[pad + 0 * nside:pad + 1 * nside, pad + 5 * nside:pad + 5 * nside + 32 * mult] = blocks[8][:,
                                                                                              0: 32 * mult]

    return piece_map




def sphere2piecePlane_squa_pad_mult(sphere_map, nside=512,padding_size=128):
    '''
    cut full map to 12 blocks, then piecing them together into a plane
    this is only for the case of subblocks_nums=1
    '''
    if len(sphere_map.shape) == 1:
        multi_map = False
    elif len(sphere_map.shape) == 2:
        multi_map = True
        multi_map_n = sphere_map.shape[0]

    if multi_map:
        piece_map = np.zeros((multi_map_n, nside * 5 + 2*padding_size, nside * 5+2*padding_size))
        for i in range(multi_map_n):
            sphere_map_i = sphere_map[i]
            piece_map[i,:] = sphere2piecePlane_squa_pad(sphere_map_i, nside, padding_size)

        return piece_map

    else:
        return sphere2piecePlane_squa_pad(sphere_map, nside, padding_size)








def sphere2piecePlane_n(sphere_map, nside=256, subblocks_nums=1):
    '''
    cut full map to 12 blocks, then piecing them together into a plane
    this is only for the case of subblocks_nums=1
    '''
    blocks = Cut(sphere_map, subblocks_nums=subblocks_nums).block_all()

    return blocks


def piecePlane2blocks(piece_map, nside=256):
    '''
    :param Map: plane map whose shape is (nside*4, nside*3)
    this is only for the case of subblocks_nums=1
    '''
    blocks = {}
    # part 1
    blocks['block_1'] = piece_map[nside * 3:, :nside]  # block 1
    blocks['block_5'] = piece_map[nside * 3:, nside:nside * 2]
    blocks['block_8'] = piece_map[nside * 3:, nside * 2:]
    # part 2
    blocks['block_0'] = piece_map[nside * 2:nside * 3, :nside]
    blocks['block_4'] = piece_map[nside * 2:nside * 3, nside:nside * 2]
    blocks['block_11'] = piece_map[nside * 2:nside * 3, nside * 2:]
    # part 3
    blocks['block_3'] = piece_map[nside:nside * 2, :nside]
    blocks['block_7'] = piece_map[nside:nside * 2, nside:nside * 2]
    blocks['block_10'] = piece_map[nside:nside * 2, nside * 2:]
    # part 4
    blocks['block_2'] = piece_map[:nside, :nside]
    blocks['block_6'] = piece_map[:nside, nside:nside * 2]
    blocks['block_9'] = piece_map[:nside, nside * 2:]
    return blocks


def piecePlane2blocks_squa(piece_map, nside=256):
    '''
    :param Map: plane map whose shape is (nside*5, nside*5)
    this is only for the case of subblocks_nums=1
    '''
    blocks = {}
    blocks['block_1'] = piece_map[2 * nside:3 * nside, nside:2 * nside]  # block 1
    blocks['block_5'] = piece_map[3 * nside:4 * nside, nside:2 * nside]
    blocks['block_8'] = piece_map[4 * nside:5 * nside, nside:2 * nside]

    blocks['block_0'] = piece_map[3 * nside:4 * nside, 0:nside]
    blocks['block_4'] = piece_map[4 * nside:5 * nside, 0:nside]
    blocks['block_11'] = piece_map[nside:2 * nside, 4 * nside:5 * nside]

    blocks['block_3'] = piece_map[0:nside, 3 * nside:4 * nside]
    blocks['block_7'] = piece_map[nside:2 * nside, 3 * nside:4 * nside]
    blocks['block_10'] = piece_map[2 * nside:3 * nside, 3 * nside:4 * nside]

    blocks['block_2'] = piece_map[nside:2 * nside, 2 * nside:3 * nside]
    blocks['block_6'] = piece_map[2 * nside:3 * nside, 2 * nside:3 * nside]
    blocks['block_9'] = piece_map[3 * nside:4 * nside, 2 * nside:3 * nside]
    return blocks



def piecePlane2blocks_squa_pad(piece_map, nside=256,padding_size=128):
    '''
    :param Map: plane map whose shape is (nside*5, nside*5)
    this is only for the case of subblocks_nums=1
    '''
    blocks = {}
    pad = padding_size
    piece_map = piece_map[pad:-pad,pad:-pad]
    blocks['block_1'] = piece_map[2 * nside:3 * nside, nside:2 * nside]  # block 1
    blocks['block_5'] = piece_map[3 * nside:4 * nside, nside:2 * nside]
    blocks['block_8'] = piece_map[4 * nside:5 * nside, nside:2 * nside]

    blocks['block_0'] = piece_map[3 * nside:4 * nside, 0:nside]
    blocks['block_4'] = piece_map[4 * nside:5 * nside, 0:nside]
    blocks['block_11'] = piece_map[nside:2 * nside, 4 * nside:5 * nside]

    blocks['block_3'] = piece_map[0:nside, 3 * nside:4 * nside]
    blocks['block_7'] = piece_map[nside:2 * nside, 3 * nside:4 * nside]
    blocks['block_10'] = piece_map[2 * nside:3 * nside, 3 * nside:4 * nside]

    blocks['block_2'] = piece_map[nside:2 * nside, 2 * nside:3 * nside]
    blocks['block_6'] = piece_map[2 * nside:3 * nside, 2 * nside:3 * nside]
    blocks['block_9'] = piece_map[3 * nside:4 * nside, 2 * nside:3 * nside]
    return blocks





# def piecePlanes2blocks_mult_(piece_maps, nside=256):
#     '''this is only for the case of subblocks_nums=1'''
#     if len(piece_maps.shape) == 2:
#         multi_map = False
#     elif len(piece_maps.shape) == 3:  # shape: [freq, pix, pix]
#         multi_map = True
#         multi_map_n = piece_maps.shape[0]
#
#     if multi_map:
#         base_map = np.zeros((multi_map_n, 12 * nside ** 2))
#         for i in range(multi_map_n):
#             base_map[i, :] = piecePlane2blocks(piece_maps[i, :], nside=nside)
#         return base_map
#     else:
#         return piecePlane2blocks(piece_maps, nside=nside)


def piecePlanes2blocks_mult(piece_maps, nside=256, block_n = None):


    # if len(piece_maps.shape) == 2:
    #     return piecePlane2blocks(piece_maps, nside=nside)
    if len(piece_maps.shape) == 2:
        blocks = {}
        # part 1
        blocks['block_1'] = piece_maps[nside * 3:, :nside]  # block 1
        blocks['block_5'] = piece_maps[nside * 3:, nside:nside * 2]
        blocks['block_8'] = piece_maps[nside * 3:, nside * 2:]
        # part 2
        blocks['block_0'] = piece_maps[nside * 2:nside * 3, :nside]
        blocks['block_4'] = piece_maps[nside * 2:nside * 3, nside:nside * 2]
        blocks['block_11'] = piece_maps[nside * 2:nside * 3, nside * 2:]
        # part 3
        blocks['block_3'] = piece_maps[nside:nside * 2, :nside]
        blocks['block_7'] = piece_maps[nside:nside * 2, nside:nside * 2]
        blocks['block_10'] = piece_maps[nside:nside * 2, nside * 2:]
        # part 4
        blocks['block_2'] = piece_maps[:nside, :nside]
        blocks['block_6'] = piece_maps[:nside, nside:nside * 2]
        blocks['block_9'] = piece_maps[:nside, nside * 2:]
    elif len(piece_maps.shape) == 3:
        blocks = {}
        # part 1
        blocks['block_1'] = piece_maps[:, nside * 3:, :nside]  # block 1
        blocks['block_5'] = piece_maps[:, nside * 3:, nside:nside * 2]
        blocks['block_8'] = piece_maps[:,nside * 3:, nside * 2:]
        # part 2
        blocks['block_0'] = piece_maps[:, nside * 2:nside * 3, :nside]
        blocks['block_4'] = piece_maps[:, nside * 2:nside * 3, nside:nside * 2]
        blocks['block_11'] = piece_maps[:, nside * 2:nside * 3, nside * 2:]
        # part 3
        blocks['block_3'] = piece_maps[:, nside:nside * 2, :nside]
        blocks['block_7'] = piece_maps[:, nside:nside * 2, nside:nside * 2]
        blocks['block_10'] = piece_maps[:, nside:nside * 2, nside * 2:]
        # part 4
        blocks['block_2'] = piece_maps[:, :nside, :nside]
        blocks['block_6'] = piece_maps[:, :nside, nside:nside * 2]
        blocks['block_9'] = piece_maps[:, :nside, nside * 2:]
    if block_n is None:
        return blocks
    else:
        return blocks[block_n].swapaxes(-1,-2)  # Note!



def piecePlanes2blocks(piece_maps, nside=256):
    '''this is only for the case of subblocks_nums=1'''
    if len(piece_maps.shape) == 2:
        return piecePlane2blocks(piece_maps, nside=nside)
    elif len(piece_maps.shape) == 3:
        blocks = {}
        # part 1
        blocks['block_1'] = piece_maps[:, nside * 3:, :nside]  # block 1
        blocks['block_5'] = piece_maps[:, nside * 3:, nside:nside * 2]
        blocks['block_8'] = piece_maps[:, nside * 3:, nside * 2:]
        # part 2
        blocks['block_0'] = piece_maps[:, nside * 2:nside * 3, :nside]
        blocks['block_4'] = piece_maps[:, nside * 2:nside * 3, nside:nside * 2]
        blocks['block_11'] = piece_maps[:, nside * 2:nside * 3, nside * 2:]
        # part 3
        blocks['block_3'] = piece_maps[:, nside:nside * 2, :nside]
        blocks['block_7'] = piece_maps[:, nside:nside * 2, nside:nside * 2]
        blocks['block_10'] = piece_maps[:, nside:nside * 2, nside * 2:]
        # part 4
        blocks['block_2'] = piece_maps[:, :nside, :nside]
        blocks['block_6'] = piece_maps[:, :nside, nside:nside * 2]
        blocks['block_9'] = piece_maps[:, :nside, nside * 2:]
        return blocks


def piecePlane2sphere(piece_map, nside=256):
    '''
    :param Map: plane map whose shape is (nside*4, nside*3)
    this is only for the case of subblocks_nums=1
    '''
    blocks = piecePlane2blocks(piece_map, nside=nside)
    base_map = np.zeros(12 * nside ** 2)
    for i in range(12):
        full_map = Block2Full(blocks['block_%s' % i], i, base_map=base_map).full()
        base_map = full_map
    return full_map

def piecePlane_squa2sphere(piece_map, nside=512):
    '''
    :param Map: plane map whose shape is (nside*5, nside*5)
    this is only for the case of subblocks_nums=1
    '''
    blocks = piecePlane2blocks_squa(piece_map, nside=nside)
    base_map = np.zeros(12 * nside ** 2)
    for i in range(12):
        full_map = Block2Full(blocks['block_%s' % i], i, base_map=base_map).full()
        base_map = full_map
    return full_map

def piecePlane_squa2sphere_mult(piece_maps, nside=512):
    '''
    :param Map: plane map whose shape is (nside*5, nside*5)
    this is only for the case of subblocks_nums=1
    '''
    if len(piece_maps.shape) == 2:
        multi_map = False
    elif len(piece_maps.shape) == 3:  # shape: [freq, pix, pix]
        multi_map = True
        multi_map_n = piece_maps.shape[0]
    if multi_map:
        base_map = np.zeros((multi_map_n, 12 * nside ** 2))
        for i in range(multi_map_n):
            base_map_i = np.zeros(12 * nside ** 2)
            blocks = piecePlane2blocks_squa(piece_maps[i, :], nside=nside)
            for ii in range(12):
                full_map = Block2Full(blocks['block_%s' % ii], ii, base_map=base_map_i).full()
                base_map_i = full_map
            base_map[i, :] = base_map_i
        return base_map

    else:
        return piecePlane_squa2sphere(piece_maps, nside=nside)



def piecePlane_squa2sphere_pad(piece_map, nside=512, padding_size=128):
    '''
    :param Map: plane map whose shape is (nside*4, nside*3)
    this is only for the case of subblocks_nums=1
    '''
    blocks = piecePlane2blocks_squa_pad(piece_map, nside=nside,padding_size=padding_size)
    base_map = np.zeros(12 * nside ** 2)
    for i in range(12):
        full_map = Block2Full(blocks['block_%s' % i], i, base_map=base_map).full()
        base_map = full_map
    return full_map


def piecePlane_squa2sphere_pad_mult(piece_maps, nside=512, padding_size=128):
    '''
    :param Map: plane map whose shape is (nside*4, nside*3)
    this is only for the case of subblocks_nums=1
    '''
    if len(piece_maps.shape) == 2:
        multi_map = False
    elif len(piece_maps.shape) == 3:  # shape: [freq, pix, pix]
        multi_map = True
        multi_map_n = piece_maps.shape[0]
    if multi_map:
        base_map = np.zeros((multi_map_n, 12 * nside ** 2))
        for i in range(multi_map_n):
            base_map_i = np.zeros(12 * nside ** 2)
            blocks = piecePlane2blocks_squa_pad(piece_maps[i, :], nside=nside, padding_size=padding_size)
            for ii in range(12):
                full_map = Block2Full(blocks['block_%s' % ii], ii, base_map=base_map_i).full()
                base_map_i = full_map
            base_map[i, :] = base_map_i
        return base_map
    else:
        return piecePlane_squa2sphere_pad(piece_maps, nside=nside, padding_size=padding_size)


def piecePlane2sphere_mult(piece_maps, nside=256):
    '''
    :param Map: plane map whose shape is (nside*4, nside*3)
    this is only for the case of subblocks_nums=1
    '''
    if len(piece_maps.shape) == 2:
        multi_map = False
    elif len(piece_maps.shape) == 3:  # shape: [freq, pix, pix]
        multi_map = True
        multi_map_n = piece_maps.shape[0]
    if multi_map:
        base_map = np.zeros((multi_map_n, 12 * nside ** 2))

        for i in range(multi_map_n):
            base_map_i = np.zeros(12 * nside ** 2)
            blocks = piecePlane2blocks(piece_maps[i, :], nside=nside)
            for ii in range(12):
                full_map = Block2Full(blocks['block_%s' % ii], ii, base_map=base_map_i).full()
                base_map_i = full_map
            base_map[i, :] = base_map_i
        return base_map
    else:
        blocks = piecePlane2blocks(piece_maps, nside=nside)
        base_map = np.zeros(12 * nside ** 2)
        for i in range(12):
            full_map = Block2Full(blocks['block_%s' % i], i, base_map=base_map).full()
            base_map = full_map
        return full_map

def blockPlane2sphere_mult(block_maps, nside=256, block_n='block_0'):
    if len(block_maps.shape) == 2:
        multi_map = False
    elif len(block_maps.shape) == 3:  # shape: [freq, pix, pix]
        multi_map = True
        multi_map_n = block_maps.shape[0]
    if multi_map:
        base_map = np.zeros((multi_map_n, 12 * nside ** 2))
        for i in range(multi_map_n):
            full_map_i = Block2Full(block_maps[i].swapaxes(-1,-2), block_n=int(block_n[-1]), base_map=None).full() # Note! .T
            base_map[i, :] = full_map_i
        return base_map
    else:
        return Block2Full(block_maps.swapaxes(-1,-2), block_n=int(block_n[-1]), base_map=None).full() # Note! .T


def add_frame(map_cut, sides=(512, 512)):
    ''' add a frame to one block '''
    # here use map_cut.copy() is right, if use map_frame=map_cut,
    # otherwise, the input map_cut will change also!!!, why???
    side_H, side_W = sides
    map_frame = map_cut.copy()
    for i in range(side_H):
        for j in range(side_W):
            for pix in range(10):
                if i == 0:
                    map_frame[i + pix, j] = 1e6
                elif i == side_H - 1:
                    map_frame[i - pix, j] = 1e6
                elif j == 0:
                    map_frame[i, j + pix] = 1e6
                elif j == side_W - 1:
                    map_frame[i, j - pix] = 1e6
    return map_frame









================================================
FILE: cmbfscnn/utils.py
================================================

import pickle
import numpy as np
import healpy as hp
from . import namaster as na
from. import spherical as sp
from tqdm import tqdm


def save_pkl(obj, pkl_name):
    with open(pkl_name + '.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_pkl(pkl_name):
    with open(pkl_name + '.pkl', 'rb') as f:
        return pickle.load(f)

def MAD(output_data, target):
    MAD = np.mean(abs(output_data - target))
    return MAD

# def write_yaml(data, dir=''):
#     file = open(dir, 'w', encoding='utf-8')  # w:
#     # yaml.dump(data, file,allow_unicode=True)
#     yaml.safe_dump(data, file, allow_unicode=True)
#     file.close()
#
# def read_yaml(dir = ''):
#     f = open(dir, 'r',encoding='utf-8')
#     conf_str = f.read()
#     f.close()
#     return yaml.load(conf_str, Loader=yaml.FullLoader)

def get_mask(partial_map):
    mask = np.where(partial_map==0,0,1)
    return mask

def get_Bl(fwhm,nside=512):
    lmax = 3*nside - 1
    gauss_beam = hp.gauss_beam(fwhm * np.pi / 10800., lmax=lmax)
    return gauss_beam


def get_full_map(Q_map, block_n=0):
    if not block_n==None:
        Q_map = sp.Block2Full(Q_map, block_n, base_map=None).full()
    return Q_map

def get_mask(partial_map):
    mask = np.where(abs(partial_map) > 1e-6, 1, 0)
    return mask

def get_mask_for_block(block = 'block_0', nside=512):
    partial_map = np.ones(12*nside**2)
    map_new = sp.sphere2piecePlane_mult(sphere_map=partial_map, nside=nside)
    map_new_block = sp.piecePlanes2blocks_mult(piece_maps=map_new, nside=nside,
                                                           block_n=block)
    map_new = sp.blockPlane2sphere_mult(map_new_block, nside=nside, block_n=block)
    mask = np.where(abs(map_new) > 1e-6, 1, 0)
    return mask

class Get_power(object):
    def __init__(self, data_Q1_dir=None, data_Q2_dir = None,  data_U1_dir=None, data_U2_dir = None, block_n=0, nside=512,mask=None,
                 beam_file=None,nlb=None,Dl=None,aposize=None):
        self.data_Q1_dir = data_Q1_dir
        self.data_Q2_dir = data_Q2_dir
        self.data_U1_dir = data_U1_dir
        self.data_U2_dir = data_U2_dir
        self.block_n = block_n
        self.nside = nside
        if data_Q2_dir==None:
            self.data_Q2_dir =self.data_Q1_dir
        if data_U2_dir==None:
            self.data_U2_dir =self.data_U1_dir
        self.mask = mask
        self.beam_file = beam_file
        self.nlb = nlb
        self.Dl = Dl
        self.aposize = aposize


    def cal_Cl(self,map1, map2):
        denoise_power_spin2 = na.Calculate_power_spectrum(map1, map2, self.mask, aposize=self.aposize,
                                                              Bl=self.beam_file, nside=self.nside, nlb=self.nlb,
                                                              Dl=self.Dl)
        if map1.ndim==2:
            ell, powe_de = denoise_power_spin2.get_power_spectra_for_spin2()
            EE, BB = powe_de[0, :], powe_de[3, :]
            return ell, EE, BB
        elif map1.ndim==1:
            ell, powe = denoise_power_spin2.get_power_spectra_for_spin1()
            return ell, powe
        else:
            print('ERROR: shape of maps have error')

    def map_QU(self, Q_map, U_map=None):
        if type(U_map) == np.ndarray:
            if not self.block_n == None:
                Q_map = get_full_map(Q_map)
                U_map = get_full_map(U_map)
            else:
                Q_map = sp.piecePlane2sphere(Q_map, nside=self.nside)
                U_map = sp.piecePlane2sphere(U_map, nside=self.nside)
            full_map = np.zeros((2, len(Q_map)))
            full_map[0, :] = Q_map
            full_map[1, :] = U_map
            return full_map
        else:
            if not self.block_n == None:
                Q_map = get_full_map(Q_map)
            else:
                Q_map = sp.piecePlane2sphere(Q_map, nside=self.nside)
            return Q_map

    def get_N_power(self, N_sample, EB_power = False):
        if EB_power:
            Q1 = np.load(self.data_Q1_dir)
            Q2 = np.load(self.data_Q2_dir)
            U1 = np.load(self.data_U1_dir)
            U2 = np.load(self.data_U2_dir)
        else:
            Q1 = np.load(self.data_Q1_dir)
            Q2 = np.load(self.data_Q2_dir)

        if self.mask is None:
            self.mask = get_mask(Q1[0,:])
        pbar = tqdm(total=len(N_sample))
        for ite, nn in enumerate(N_sample):


            if EB_power:
                full_map1 = self.map_QU(Q1[nn,:], U1[nn,:])
                full_map2 = self.map_QU(Q2[nn,:], U2[nn,:])
                ell, EE, BB = self.cal_Cl(full_map1, full_map2)
                spectra = np.vstack((ell, EE, BB))
                if ite == 0:
                    spectra_N = spectra[None, :]
                else:
                    spectra_N = np.vstack((spectra_N, spectra[None, :]))
            else:
                Q1_i = self.map_QU(Q1[nn,:])
                Q2_i = self.map_QU(Q2[nn,:])
                ell, pow = self.cal_Cl(Q1_i, Q2_i)
                spectra = np.vstack((ell, pow))
                if ite == 0:
                    spectra_N = spectra[None, :]
                else:
                    spectra_N = np.vstack((spectra_N, spectra[None, :]))
            pbar.set_description('Calculating power spectra')
            pbar.update(1)

        return spectra_N

    def get_N_power_from_spheremap(self, N_sample, EB_power = False):
        if EB_power:
            Q1 = np.load(self.data_Q1_dir)
            Q2 = np.load(self.data_Q2_dir)
            U1 = np.load(self.data_U1_dir)
            U2 = np.load(self.data_U2_dir)
        else:
            Q1 = np.load(self.data_Q1_dir)
            Q2 = np.load(self.data_Q2_dir)

        if self.mask is None:
            self.mask = get_mask(Q1[0,:])
        pbar = tqdm(total=len(N_sample))
        for ite, nn in enumerate(N_sample):

            if EB_power:

                full_map1 = np.zeros((2, 12*self.nside**2))
                full_map1[0, :] = Q1[nn,:]
                full_map1[1, :] = U1[nn,:]
                full_map2 = np.zeros((2, 12*self.nside**2))
                full_map2[0, :] = Q2[nn, :]
                full_map2[1, :] = U2[nn, :]


                ell, EE, BB = self.cal_Cl(full_map1, full_map2)
                spectra = np.vstack((ell, EE, BB))
                if ite == 0:
                    spectra_N = spectra[None, :]
                else:
                    spectra_N = np.vstack((spectra_N, spectra[None, :]))
            else:
                Q1_i = Q1[nn,:]
                Q2_i = Q2[nn,:]
                ell, pow = self.cal_Cl(Q1_i, Q2_i)
                spectra = np.vstack((ell, pow))
                if ite == 0:
                    spectra_N = spectra[None, :]
                else:
                    spectra_N = np.vstack((spectra_N, spectra[None, :]))
            pbar.set_description('Calculating power spectra')
            pbar.update(1)

        return spectra_N




================================================
FILE: examples/__init__.py
================================================

# -*- coding: utf-8 -*-






================================================
FILE: examples/main.py
================================================

# -*- coding: utf-8 -*-


import sys
from cmbfscnn.utils import *
from cmbfscnn.CMBFS import CMBFSCNN
try:
    config_dir = sys.argv[1]
except:
    print("Please enter the configuration file")
    sys.exit(1)

config = load_pkl(config_dir)
cmbfcnn = CMBFSCNN(config)
cmbfcnn.run_CMBFSCNN()


================================================
FILE: examples/Tutorial.py
================================================
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2024/7/9
# @Author  : Ye-Peng Yan
# @File    : Tutorial.py

import numpy as np
from cmbfscnn.utils import *
from cmbfscnn.CMBFS import CMBFSCNN
import matplotlib.pyplot as plt
import os


# -------------------------------------------------------------------------------------------------------
# Set basic configuration
# -------------------------------------------------------------------------------------------------------

# Firstly, simulate the data of the sky signal.
# Set random configuration files for foreground and cosmological parameters
# example: 'syn_spectralindex_random':(0.05, 'one') means that random spectral index of synchrotron radiation is required.
# The random size of the spectral index is 5%. The random pattern is "one", indicating random pixel dependently.
# If the random pattern is 'multi',  meaning random pixel independently. Please refer to Table 2 in our paper for details.
data_config_random_one_05 = {'syn_spectralindex_random':(0.05, 'one'), 'syn_amplitude_random':(0.1, 'one'),
                           'dust_spectralindex_random':(0.05, 'one'), 'dust_amplitude_random':(0.1, 'one'),
                           'dust_temp_random':(0.05, 'multi'),
                               'ame_amplitude_random':(0.1, 'one'), 'Random_types_of_cosmological_parameters':'Uniform'}

## Set instrument information, including frequencies, beams, and white noise level. Please refer to Table 1 in our paper for details.
# the performance of CMB-S4 experiment
freqs_CMB_S4 = np.array([30,40,85,95,145,155,220,270]) # frequencies
Beams_CMB_S4 = np.array([72.8,72.8,25.5,22.7,25.5,22.7,13.0,13.0]) # FWHM
Sens_CMB_S4 = np.array([3.53,4.46,0.88,0.78,1.23,1.34,3.48,5.97]) # white noise level
output_beam_CMB_S4 = 13.0
output_freq_CMB_S4 =  220

# the performance of LiteBIRD experiment
freqs_LiteBIRD = np.array([50,78,100,119,140,166,195,235,280,337])
Beams_LiteBIRD = np.array([56.8,36.9,30.2,26.3,23.7,28.9,28.0,24.7,22.5,20.9])
Sens_LiteBIRD = np.array([32.78,18.59,12.93,9.79,9.55,5.81,7.12,15.16,17.98,24.99])
output_beam_LiteBIRD =  28.9
output_freq_LiteBIRD =  166

nside = 512
save_data_dir = 'DATA/'
save_result_dir = 'DATA_results/'


# The number of samples for simulating sky maps
N_sky_maps = [1000, 300, 300] # The sample sizes of sky map for the training set, validation set，and test set are 1000, 300, and 300, respectively.
N_noise_maps = [300, 300, 300] # The sample sizes of noise map for the training set, validation set，and test set are 1000, 300, and 300, respectively.
is_half_split_map = True  # Do you use 'half-split maps' for testing? Our paper uses the 'half-split maps'.
is_fullsky = False # Do you use a full-sky map for testing？As a tutorial, we use partial-sky ('block_0') for testing
# Training with a full-sky map requires a significant amount of GPU memory (>24GB), and it is recommended to use multiple GPUs for training.

# For CMB experiments with high white noise levels, such as LiteBIRD, we set the expected output of CNN to use CMB+ILC noise. Thus, we need calculate ILC noise using ILC method.
# Please refer to Section 3.2 in our paper for details.
# Readers can use the following six lines of code to calculate ILC noise.
using_ilc_cmbmap = False # Is ILC noise used as the expected noise output.

is_polarization_data = True # Is polarization data used for testing? If false, the simulated data includes temperature and polarization.
block_n = 'block_0' # The sky is divided into 12 blocks, and block 0 is selected for testing. Only valid for parameter is_fullsky = True
dataset_type = ['traing_set', 'validation_set', 'testing_set'] # The dataset includes training, validation, and testing sets



# -------------------------------------------------------------------------------------------------------
# Generate configuration file
# -------------------------------------------------------------------------------------------------------

# Then, you need to define a parameter file
# The configuration file includes Data parameters, ILC parameters, Training CNN parameters

Data_parameters = {
    'data_config_random': data_config_random_one_05,
    'freqs': freqs_CMB_S4,
    "output_freq": output_freq_CMB_S4,
    "beams": Beams_CMB_S4,
    "output_beam": output_beam_CMB_S4,
    "sens": Sens_CMB_S4,
    "nside": nside,
    "save_data_dir": save_data_dir,
    "save_result_dir": save_result_dir,
    "is_half_split_map": is_half_split_map,
    "is_polarization_data": is_polarization_data,
    'is_fullsky': is_fullsky,
    "block_n": block_n,
    'padding': True,
    "dataset_type": dataset_type,
    "N_sky_maps": N_sky_maps,
    "N_noise_maps": N_noise_maps,
    "N_threads_preprocessing": 1,
    "component": 'QU'
}

if is_fullsky:
    ilc_mask = np.ones(12*nside**2)
else:
    ilc_mask = get_mask_for_block(block_n,nside)

ILC_parameters = {
    "using_ilc_cmbmap": using_ilc_cmbmap,
    "ilc_mask": ilc_mask,
    "ILC_N_threads": 1
}

Training_CNN_parameters = {
    "iteration": 3e4,
    "batch_size": 12,
    "learning_rate": 0.01,
    "device_ids": [0],
    "CNN_model": 'CMBFSCNN_level3'
}

conf_paras = {**Data_parameters, **ILC_parameters, **Training_CNN_parameters}
if not os.path.exists(save_data_dir):
    os.makedirs(save_data_dir)
save_pkl(conf_paras, save_data_dir+'config')


# If there is a configuration file, you can directly read the configuration file
# conf_paras = load_pkl(save_data_dir+'config')

cmbfcnn = CMBFSCNN(conf_paras)

# -------------------------------------------------------------------------------------------------------
# Simulating data and data preprocessing
# -------------------------------------------------------------------------------------------------------
cmbfcnn.data_simulation()
if using_ilc_cmbmap:
    cmbfcnn.cal_ilc_noise()
cmbfcnn.data_preprocessing()

# plot map
cmb = np.load(save_data_dir+'noiseless/cmb/cmb0.npy')
total = np.load(save_data_dir+'noiseless/total/total0.npy')
print(cmb.shape, total.shape)
hp.mollview(cmb[6,1,:], title="CMB Q map",cmap = plt.get_cmap(plt.cm.jet))
hp.mollview(total[6,1,:], title="total Q map",cmap = plt.get_cmap(plt.cm.jet))
plt.show()

# plot
total_block = np.load(save_data_dir+'observed_flat_block_map/training_set/total/total0.npy')
total = np.load(save_data_dir+'observed_flat_map/training_set/total/total0.npy')
plt.imshow(total_block[6,0,:], cmap=plt.cm.jet,vmin=-10,vmax=10)
plt.imshow(total[0,0,:], cmap=plt.cm.jet,vmin=-10,vmax=10)
plt.show()


# -------------------------------------------------------------------------------------------------------
# Training CNN models
# -------------------------------------------------------------------------------------------------------
cmbfcnn.training_cnn()

# -------------------------------------------------------------------------------------------------------
# Prediction of CNN models
# -------------------------------------------------------------------------------------------------------
# get predicted map
cmbfcnn.get_predicted_maps()
# Calculate power spectra
cmbfcnn.calculate_power_spectra()

# -------------------------------------------------------------------------------------------------------
# Plot results
# -------------------------------------------------------------------------------------------------------

cmbfcnn._plot_results()







================================================
FILE: pysm/__init__.py
================================================
from .pysm import Sky, Instrument
from .components import Dust, Synchrotron, AME, Freefree, CMB
from .common import read_map, convert_units

def get_template_dir():
    import os.path
    return os.path.join(os.path.abspath(os.path.dirname(__file__)), 'template')



================================================
FILE: pysm/common.py
================================================
"""
.. module:: common
   :platform: Unix
   :synopsis: convenience functions used in other modules.

.. moduleauthor: Ben Thorne <ben.thorne@physics.ox.ac.uk>
"""

from __future__ import print_function
import healpy as hp
import numpy as np
import scipy.constants as constants
import scipy.integrate
import sys

def FloatOrArray(model):
    """Decorator to modify models to allow computation across an array of
    frequencies, and for a single float.

    :param model: model function which we will decorate.
    :type model: function
    :return: wrapped function -- function

    """
    def decorator(nu, **kwargs):
        """Evaluate if nu is a float."""
        try:
            nu_float = float(nu)
            nu_float = np.array(nu)
            return model(nu_float, **kwargs) 
        except TypeError:
            try:
                """If not float, check if nu is one dimensional"""
                nu_1darray = np.asarray(nu)
                if not (nu_1darray.ndim == 1):
                    print("Frequencies must be float or convertable to 1d array.")
                    sys.exit(1)
                    """If it is 1d array evaluate model function over all its elements."""
                return np.array([model(x, **kwargs) for x in nu_1darray])
            except ValueError:
                """Fail if not convertable to 1d array"""
                print("Frequencies must be either float or convertable to array.")
                raise
                sys.exit(1)
    return decorator

def write_map(fname, output_map, nside=None, pixel_indices=None):
    """Convenience function wrapping healpy's write_map and handling of partial sky

    :param fname: path to fits file.
    :type fname: str.
    :param output_map: map or maps to be written to disk
    :type fname: np.ndarray.
    :param nside: nside of the pixel indices, necessary just if pixel_indices is defined
    :type nside: int.
    :param pixel_indices: pixels in RING ordering where the output map is defined
    :type field: array of ints.
    """

    if pixel_indices is None:
        full_map = output_map
    else:
        assert not nside is None, "nside is required if you provide pixel_indices"
        full_map = build_full_map(pixel_indices, output_map, nside)

    hp.write_map(fname, full_map, overwrite=True)

def read_map(fname, nside, field = (0), pixel_indices=None, mpi_comm=None, verbose = False):
    """Convenience function wrapping healpy's read_map and upgrade /
    downgrade in one function.
    
    :param fname: path to fits file.  
    :type fname: str.   
    :param nside: nside to which we up or down grade.  
    :type nside: int.  
    :param field: fields of fits file from which to read.  
    :type field: tuple of ints.  
    :param pixel_indices: read only a subset of pixels in RING ordering
    :type field: array of ints.
    :param mpi_comm: Read on rank 0 and broadcast over MPI
    :type field: mpi4py MPI Communicator.
    :param verbose: run in verbose mode.  
    :type verbose: bool.
    :returns: numpy.ndarray -- the maps that have been read. 
    """
    if (mpi_comm is not None and mpi_comm.rank==0) or (mpi_comm is None):
        output_map = hp.ud_grade(hp.read_map(fname, field = field, verbose = verbose), nside_out = nside)
    elif mpi_comm is not None and mpi_comm.rank>0:
        npix = hp.nside2npix(nside)
        try:
            ncomp = len(field)
        except TypeError: # field is int
            ncomp = 1
        shape = npix if ncomp == 1 else (len(field), npix)
        output_map = np.empty(shape, dtype=np.float64)

    if mpi_comm is not None:
        mpi_comm.Bcast(output_map, root=0)

    if pixel_indices is None:
        return output_map
    else:
        try: # multiple components
            return [each[pixel_indices] for each in output_map]
        except IndexError: # single component
            return output_map[pixel_indices]

def loadtxt(fname, mpi_comm=None, **kwargs):
    """MPI-aware loadtxt function
    reads text file on rank 0 with np.loadtxt and broadcasts over MPI

    :param fname: path to fits file.
    :type fname: str.
    :param mpi_comm: Read on rank 0 and broadcast over MPI
    :type field: mpi4py MPI Communicator.
    :returns: numpy.ndarray -- the data read
    """

    if (mpi_comm is not None and mpi_comm.rank==0) or (mpi_comm is None):
        data = np.loadtxt(fname, **kwargs)
    elif mpi_comm is not None and mpi_comm.rank>0:
        data = None

    if mpi_comm is not None:
        data = mpi_comm.bcast(data, root=0)

    return data

def read_key(Class, keyword, dictionary):
    """Gives the input class an attribute with the name of the keyword and
    value of the corresponding dictionary item.

    :param Class: Class instance for which we are defining attributes.
    :type Class: object.
    :param keyword: keyword of the dictionary element to set to Class.__class__.__name__
    :type keyworkd: str.
    :param dictionary: dictionary from which we are taking the value corresponding to keyword.
    :type dictionary: dict.
    :returns: does not return, but modifies inputs.
    """
    try:
        setattr(Class, '_%s__%s'%(Class.__class__.__name__, keyword), dictionary[keyword])
    except KeyError: 
        print("%s not set."%keyword)
    return

def convert_units(unit1, unit2, nu):
    """Function to do unit conversions between Rayleigh-Jeans units, CMB
    units, and flux units.

    :param unit1: unit from which we are converting.
    :type unit1: str.
    :param unit2: unit to which we are converting.
    :type unit2: str.
    :param nu: frequency at which to calculate unit conversion.
    :type nu: float, np.ndarray.
    :returns: unit conversion coefficient - float, numpy.ndarray.
    """
    if "K_CMB" in unit1:
        #first deal with the unit conversion
        if "Jysr" in unit2:
            conversion_factor = K_CMB2Jysr(nu) 
        elif "K_RJ" in unit2:
            conversion_factor = K_CMB2Jysr(nu) / K_RJ2Jysr(nu)
        elif "K_CMB" in unit2:
            conversion_factor = np.ones_like(nu)
        else:
            print("Incorrect format or unit.")

    elif "K_RJ" in unit1:
        if "Jysr" in unit2:
            conversion_factor = K_RJ2Jysr(nu)
        elif "K_CMB" in unit2:
            conversion_factor = K_RJ2Jysr(nu) / K_CMB2Jysr(nu)
        elif "K_RJ" in unit2:
            conversion_factor = np.ones_like(nu)
        else: 
            print("Incorrect format or unit.")

    elif "Jysr" in unit1:
        if "Jysr" in unit2:
            conversion_factor = np.ones_like(nu)
        elif "K_RJ" in unit2:
            conversion_factor = 1. / K_RJ2Jysr(nu)
        elif "K_CMB" in unit2:
            conversion_factor = 1. / K_CMB2Jysr(nu)
        else:
            print("Incorrect format or unit.")

    # Now deal with the magnitude
    if "n" in unit1[0]:
        prefac = 1.e-9
    elif "u" in unit1[0]:
        prefac = 1.e-6
    elif "m" in unit1[0]:
        prefac = 1.e-3
    elif "k" in unit1[0]:
        prefac = 1.e3
    elif "M" in unit1[0]:
        prefac = 1.e6
    elif "G" in unit1[0]:
        prefac = 1.e9
    elif "K" in unit1[0]:
        prefac = 1.
    elif "J" in unit1[0]:
        prefac = 1.
    else:
        print("Invalid format for unit1 in convert_units")
        sys.exit(1)

    if "n" in unit2[0]:
        postfac = 1.e9
    elif "u" in unit2[0]:
        postfac = 1.e6
    elif "m" in unit2[0]:
        postfac = 1.e3
    elif "k" in unit2[0]:
        postfac = 1.e-3
    elif "M" in unit2[0]:
        postfac = 1.e-6
    elif "G" in unit2[0]:
        postfac = 1.e-9
    elif "K" in unit2[0]:
        postfac = 1.
    elif "J" in unit2[0]:
        postfac = 1.
    else:
        print("Invalid format for unit2 in convert_units")
        sys.exit(1)

    return np.array(conversion_factor * prefac * postfac)

@FloatOrArray
def K_CMB2Jysr(nu): 
    """Kelvin_CMB to Janskies per steradian. Nu is in GHz.

    :param nu: frequency in GHz at which to calculate unit conversion.
    :type nu: float, numpy.ndarray
    :return: unit conversion coefficient - float.
    
    """
    return dB(nu, 2.7255) * 1.e26

@FloatOrArray
def K_RJ2Jysr(nu):
    """Kelvin_RJ to Janskies per steradian. Nu is in GHz.

    :param nu: frequency in GHz at which to calculate unit conversion.
    :type nu: float, numpy.ndarray.                                                                                                                      
    :return: unit conversion coefficient - float. 
    
    """
    return  2. * (nu * 1.e9 / constants.c) ** 2 * constants.k * 1.e26

def B(nu, T):
    """Planck function. 

    :param nu: frequency in GHz at which to evaluate planck function.
    :type nu: float.
    :param T: temperature of black body. 
    :type T: float.
    :return: float -- black body brightness.

    """
    x = constants.h * nu * 1.e9 / constants.k / T
    return 2. * constants.h * (nu * 1.e9) ** 3 / constants.c ** 2 / np.expm1(x)

def dB(nu, T):
    """Differential planck function. 
    
    :param nu: frequency in GHz at which to evaluate differential planck function.
    :type nu: float.
    :param T: temperature of black body. 
    :type T: float.
    :return: float -- differential black body function. 

    """
    x = constants.h * nu * 1.e9 / constants.k / T
    return B(nu, T) / T * x * np.exp(x) / np.expm1(x)

def bandpass_convert_units(unit, channel):
    """Function to calculate the unit conversion factor after bandpass integration from 
    Jysr to either RJ, CMB or MJysr. 

    We integrate the signal in units of MJysr:

    [I_MJysr] = int I_Mjysr(nu) * weights * dnu

    In order to convert to K_CMB we define A_CMB:

    [T_CMB] = A_CMB [I_MJysr] 

    If we observe the CMB then:

    [T_CMB] = A_CMB * int dB(nu, 2.7255) * T_CMB * weights * dnu

    So: A_CMB = 1. / int dB(nu, 2.7255) * weights * dnu. 

    In a similar way for Rayleigh-Jeans units:

    A_RJ = 1. / int 2 * k * nu ** 2 / c ** 2 * weights * dnu

    :param unit1: unit from which to convert (K_RJ, K_CMB, Jysr) with SI prefix (n, u, m, k, G).
    :type unit1: str.
    :param unit2: unit to which to convert (K_RJ, K_CMB, Jysr) with SI prefix (n, u, m, k, G).
    :type unit2: str.
    :param channel: tuple containing bandpass frequencies and weights: (frequencies, weights).
    :type channel: tuple.
    :param nu_c: central frequency used to calculate unit conversions. If not set the mean frequency of the bandpass is used. 
    :type nu_c: float.
    :return: unit conversion factor -- float

    """

    (frequencies, weights) = channel

    # normalise the weights and check that they integrate to 1.
    weights /= scipy.integrate.simps(weights, frequencies)
    
    #First do conversion of RJ, CMB, MJy
    if "CMB" in unit:
        prefac = 1. / scipy.integrate.simps(weights * K_CMB2Jysr(frequencies), frequencies)
    elif "RJ" in unit:
        prefac = 1. / scipy.integrate.simps(weights * K_RJ2Jysr(frequencies), frequencies)
    elif "Jysr" in unit:
        prefac = 1.

    #next sort out magnitude
    if "n" in unit[0]:
        postfac = 1.e9
    elif "u" in unit[0]:
        postfac = 1.e6
    elif "m" in unit[0]:
        postfac = 1.e3
    elif "k" in unit[0]:
        postfac = 1.e-3
    elif "M" in unit[0]:
        postfac = 1.e-6
    elif "G" in unit[0]:
        postfac = 1.e-9
    elif "K" in unit[0]:
        postfac = 1.
    elif "J" in unit[0]:
        postfac = 1.
    else:
        print("Invalid format for unit in bandpass_convert_units")
        sys.exit(1)
                    
    return prefac * postfac


def invert_safe(m):
    """Function to safely invert almost positive definite matrix.

    :param m: matrix to invert.
    :type m: numpy.ndarray.
    :return: inverted matrix -- numpy.ndarray

    """
    mb = m.copy()
    w_ok = False
    while not w_ok :
        w, v = np.linalg.eigh(mb)
        wmin = np.min(w)
        if wmin > 0:
            w_ok = True
        else:
            mb += np.diag(2. * np.max([1E-14, -wmin]) * np.ones(len(mb)))
    winv = 1. / w
    return np.dot(v, np.dot(np.diag(winv), np.transpose(v)))
        
def check_lengths(*args):
    """Function to check that all lengths of the input lists or arrays are equal.
    Returns True if lengths are equal, False if they are not.

    :param args: arrays or lists to check the length of. 
    :type args: list, numpy.ndarray
    :return: True if lengths are equal, else False -- bool

    """
    return (len(set([len(x) for x in args])) <= 1)

def tophat_bandpass(nu_c, delta_nu, samples = 50):
    """Calculate a tophat bandpass for a given central frequency and width. 
    This will return a tuple containing (frequencies, weights). 

    :param nu_c: central frequency of bandpass.
    :type nu_c: float.
    :param delta_nu: width of bandpass.
    :type delta_nu: float.
    :param samples: number samples in bandpass; the more samples the more accurate the result.
    :type samples: int.
    :return: tuple - (frequencies, weights)

    """
    freqs = np.linspace(nu_c - delta_nu / 2., nu_c + delta_nu / 2., samples)
    weights = np.ones_like(freqs) / (freqs.size * delta_nu / samples)
    print(np.sum(weights * delta_nu))
    return (freqs, weights)

def plot_maps(ins_conf, plot_odir):
    """Function to plot output maps for a given instrument configuration.

    :param ins_conf: configuration file used to initialise the :class:`pysm.pysm.Instrument` used to generate the maps.
    :type: dict
    :param plot_odir: output directory to write maps to.
    :type plot_odir: string
    """
    #get path to maps and define output plot path.
    odir = os.path.abspath(plot_odir)
    opath = os.path.join(odir, ins_conf.output_prefix+"_maps.pdf")

    #get number of plots / 3
    if ins_conf.Use_Bandpass:
        N = len(ins_conf.Channel_Names)
    else:
        N = len(ins_conf.Frequencies)

    #initialise figure
    fig = plt.figure(1, figsize = (8, N*3))
    def add_map(maps, i):
        hp.mollview(maps[0], sub = (N, 3, i+1), fig = 1)
        hp.mollview(maps[1], sub = (N, 3, i+1), fig = 1)
        hp.mollview(maps[2], sub = (N, 3, i+1), fig = 1)
        return

    #do the plotting.
    if not ins_conf.Use_Bandpass:
        for i, f in enumerate(ins_conf.Frequencies):
            m = hp.read_map(ins_conf.file_path(f = f, extra_info = "total"), field = (0, 1, 2))
            add_map(m, i)
            
    elif self.Use_Bandpass:
        for i in enumerate(ins_conf.Channel_Names):
            m = hp.read_map(ins_conf.file_path(channel_name = c, extra_info = "total"), field = (0, 1, 2))
            add_map(m, i)

    fig.savefig(opath, bbox_inches = 'tight', background = 'white')

    return fig

def build_full_map(pixel_indices, pixel_values, nside):
    output_shape = list(pixel_values.shape)
    output_shape[-1] = hp.nside2npix(nside)
    full_map = hp.UNSEEN * np.ones(output_shape, dtype=np.float64)
    full_map[..., pixel_indices] = pixel_values
    return full_map



================================================
FILE: pysm/components.py
================================================
"""
.. module:: components
   :platform: Unix
   :synopsis: module containing definitions of component objects in pysm.

.. moduleauthor: Ben Thorne <ben.thorne@physics.ox.ac.uk>
"""

from __future__ import absolute_import, print_function
import numpy as np
import healpy as hp
import os, sys, time
import scipy.constants as constants
from scipy.interpolate import interp1d, RectBivariateSpline
from scipy.special import factorial, comb
from .common import read_key, convert_units, FloatOrArray, invert_safe, B, read_map
from .nominal import template

class Synchrotron(object):
    """Class defining attributes and scaling laws of the synchrotron
    component, instantiated with a configuration dictionary containing
    the required parameters of the synchrotron models. The key
    item pairs are then assigned as attributes.

    The current possible attributes are:

    - `Model` : SED used, power law or curved power law.
    - `A_I` : intensity template used -- numpy.ndarray or float.
    - `A_Q` : Q template used -- numpy.ndarray or float.
    - `A_U` : U template used -- numpy.ndarray or float.
    - `Nu_0_I` : reference frequency of I template -- float.
    - `Nu_0_P` : reference frequency of Q and U template -- float.
    - `Spectral_Index` : spectral index used in power law and curved power law -- numpy.ndarray or float.
    - `Spectral_Curvature` -- numpy.ndarray or float.
    - `Nu_Curve` -- pivot frequency of curvature.

    """
    def __init__(self, config):
        for k in config.keys():
            read_key(self, k, config)
        return

    @property
    def Model(self):
        try:
            return self.__model
        except AttributeError:
            print("Synchrotron attribute 'Model' not set.")
            sys.exit(1)

    @property
    def A_I(self):
        try:
            return self.__A_I
        except AttributeError:
            print("Synchrotron attribute 'A_I' not set.")
            sys.exit(1)

    @property
    def A_Q(self):
        try:
            return self.__A_Q
        except AttributeError:
            print("Synchrotron attribute 'A_Q' not set.")
            sys.exit(1)

    @property
    def A_U(self):
        try:
            return self.__A_U
        except AttributeError:
            print("%s attribute 'A_U' not set.")
            sys.exit(1)

    @property
    def Nu_0_I(self):
        try:
            return self.__nu_0_I
        except AttributeError:
            print("Synchrotron attribute 'Nu_0_I' not set.")
            sys.exit(1)

    @property
    def Nu_0_P(self):
        try:
            return self.__nu_0_P
        except AttributeError:
            print("Synchrotron attribute 'Nu_0_P' not set.")
            sys.exit(1)

    @property
    def Spectral_Index(self):
        try:
            return self.__spectral_index
        except AttributeError:
            print("Synchrotron attribute 'Spectral_Index' not set.")
            sys.exit(1)

    @property
    def Spectral_Curvature(self):
        try:
            return self.__spectral_curvature
        except AttributeError:
            print("Synchrotron attribute 'Spectral_Curvature' not set.")
            sys.exit(1)

    @property
    def Nu_Curve(self):
        try:
            return self.__nu_curve
        except AttributeError:
            print("Synchrotron attribute 'Nu_Curve' not set.")
            sys.exit(1)

    def signal(self):
        """Function to return the selected SED.

        :return: function -- selected model SED.

        """
        return getattr(self, self.Model)()

    def power_law(self):
        """Returns synchrotron (T, Q, U) maps as a function of observation
        freuency, nu.

        This is the simplest model, using only a power law spectral
        dependence.  The map of the spectral index may be a constant
        or spatially varing.

        :return: power law model -- function

        """
        @FloatOrArray
        def model(nu, **kwargs):
            """Power law scaling model.

            :param nu: frequency at which to calculate the map.
            :type nu: float.
            :return: power law scaled maps, shape (3, Npix) -- numpy.ndarray shape

            """
            scaling_I = power_law(nu, self.Nu_0_I, self.Spectral_Index)
            scaling_P = power_law(nu, self.Nu_0_P, self.Spectral_Index)
            return np.array([self.A_I * scaling_I, self.A_Q * scaling_P, self.A_U * scaling_P])
        return model

    def curved_power_law(self):
        """Returns synchrotron (T, Q, U) maps as a function of observation
        frequency, nu.

        This model allows for curvature of the power law SED. The
        spectral curvature be a constant, or a map.

        :return: power law model -- function

        """
        @FloatOrArray
        def model(nu, **kwargs):
            """Power law scaling model.
            :param nu: frequency at which to calculate the map.
            :type nu: float.
            :return: power law scaled maps, shape (3, Npix) -- numpy.ndarray shape
            """
            curvature_term = np.log(power_law(nu, self.Nu_Curve, self.Spectral_Curvature))
            scaling_I = power_law(nu, self.Nu_0_I, self.Spectral_Index + curvature_term)
            scaling_P = power_law(nu, self.Nu_0_P, self.Spectral_Index + curvature_term)
            return np.array([self.A_I * scaling_I, self.A_Q * scaling_P, self.A_U * scaling_P])
        return model

class Dust(object):
    """Class defining attributes and scaling laws of the dust
    component, instantiated with a configuration dictionary containing
    the required parameters of the synchrotron models. The key
    item pairs are then assigned as attributes.

    The current possible attributes are:

    - `Model` : SED used, modified black body, Hensley and Draine 2017.
    - `A_I` : intensity template used -- numpy.ndarray, float.
    - `A_Q` : Q template used -- numpy.ndarray, float.
    - `A_U` : U template used -- numpy.ndarray, float.
    - `Nu_0_I` : reference frequency of I template -- float.
    - `Nu_0_P` : reference frequency of Q and U template -- float.
    - `Spectral_Index` : spectral index used in power law and curved power law -- numpy.ndarray, float.
    - `Temp` : temperature template used in the modified black body scaling -- numpy.ndarray, float.
    - `Draw_Uval` : boolean, whether or not to draw a random realisation of Uval using Planck temperature and dust data.
    - `Draw_Uval_Seed` : seed for random realisations of the dust temperature and spectral index used to compute Uval if Draw_Uval = True.
    - `Uval` : logarithm of the radiation field strength. Required by Henlsey Draine 2017 if draw_Uval=False.
    - `F_fe` : mass fraction of silicon grains with iron inclusions relative to total silicon grains.
    - `Fcar` : mass fraction of carbonaceous grains relative to silicate grains. Required by Hensley and Draine model.
    - `Add_Decorrelation` : add stochastic frequency decorrelation to the SED -- bool.
    - `Corr_Len` : correlation length to use in decorrelation model -- float.

    """
    def __init__(self, config, mpi_comm=None):
        for k in config.keys():
            read_key(self, k, config)
        self.mpi_comm = mpi_comm

    @property
    def Model(self):
        try:
            return self.__model
        except AttributeError:
            print("Dust attribute 'Model' not set.")
            sys.exit(1)

    @property
    def A_I(self):
        try:
            return self.__A_I
        except AttributeError:
            print("Dust attribute 'A_I' not set.")
            sys.exit(1)

    @property
    def A_Q(self):
        try:
            return self.__A_Q
        except AttributeError:
            print("Dust attribute 'A_Q' not set.")
            sys.exit(1)

    @property
    def A_U(self):
        try:
            return self.__A_U
        except AttributeError:
            print("Dust attribute 'A_U' not set.")
            sys.exit(1)

    @property
    def Nu_0_I(self):
        try:
            return self.__nu_0_I
        except AttributeError:
            print("Dust attribute 'Nu_0_I' not set.")
            sys.exit(1)

    @property
    def Nu_0_P(self):
        try:
            return self.__nu_0_P
        except AttributeError:
            print("Dust attribute 'Nu_0_P' not set.")
            sys.exit(1)

    @property
    def Spectral_Index(self):
        try:
            return self.__spectral_index
        except AttributeError:
            print("Dust attribute 'Spectral_Index' not set.")
            sys.exit(1)

    @property
    def Temp(self):
        try:
            return self.__temp
        except AttributeError:
            print("Dust attribute 'Temp' not set.")
            sys.exit(1)

    @property
    def Uval(self):
        try:
            return self.__uval
        except AttributeError:
            print("Dust attribute 'Uval' not set.")
            sys.exit(1)

    @Uval.setter
    def Uval(self, value):
        self.__uval = value

    @property
    def Fcar(self):
        try:
            return self.__fcar
        except AttributeError:
            print("Dust attribute 'Fcar' not set.")
            sys.exit(1)

    @property
    def F_fe(self):
        try:
            return self.__f_fe
        except AttributeError:
            print("Dust attribute 'F_fe' not set.")
            sys.exit(1)

    @property
    def Corr_Len(self):
        try:
            return self.__corr_len
        except AttributeError:
            print("Dust attribute 'Corr_Len' not set.")
            sys.exit(1)

    @property
    def Draw_Uval(self):
        try:
            return self.__draw_uval
        except AttributeError:
            print("Dust attribute 'Draw_Uval' not set.")
            sys.exit(1)

    @property
    def Draw_Uval_Seed(self):
        try:
            return self.__draw_uval_seed
        except AttributeError:
            print("Dust attribute 'Draw_Uval_Seed' not set.")
            sys.exit(1)

    @property
    def Add_Decorrelation(self):
        try:
            return self.__add_decorrelation
        except AttributeError:
            print("Dust attribute 'Add_Decorrelation' not set.")
            sys.exit(1)

    @property
    def pixel_indices(self):
        try:
            return self.__pixel_indices
        except AttributeError:
            print("Dust attribute 'pixel_indices' not set.")

    @property
    def nside(self):
        try:
            return self.__nside
        except AttributeError:
            print("Dust attribute 'nside' not set.")
            sys.exit(1)

    def signal(self, **kwargs):
        """Function to return the selected SED.

        :return: function -- selected scaling model.
        """
        return getattr(self, self.Model)(**kwargs)

    def modified_black_body(self, mpi_comm=None):
        """Returns dust (T, Q, U) maps as a function of frequency, nu.
        This is the simplest model, assuming a modified black body SED
        which is the same in temperature and polarisation.
        Note that the spectral index map is expected to be the index
        beta_d such that:
        I_nu = (nu/nu_0)^beta_d B(nu, T)/B(nu_0, T),
        in flux units. Therefore beta_d ~ 1.54.

        :return: function -- model (T, Q, U) maps.

        """
        @Add_Decorrelation(self)
        @FloatOrArray
        def model(nu, **kwargs):
            """Black body model

            :param nu: frequency at which to evaluate model.
            :type nu: float.
            :return: modified black body scaling of maps, shape (3, Npix).

            """
            scaling_I = power_law(nu, self.Nu_0_I, self.Spectral_Index - 2) * black_body(nu, self.Nu_0_I, self.Temp)
            scaling_P = power_law(nu, self.Nu_0_P, self.Spectral_Index - 2) * black_body(nu, self.Nu_0_P, self.Temp)
            expected_length = hp.nside2npix(self.nside) if self.pixel_indices is None else len(self.pixel_indices)
            assert len(scaling_I) == expected_length, "{} scaling different from expected {}".format(len(scaling_I), expected_length)
            return np.array([self.A_I * scaling_I, self.A_Q * scaling_P, self.A_U * scaling_P])
        return model

    @staticmethod
    def draw_uval(seed, nside, mpi_comm=None):
        #Use Planck MBB temperature data to draw realisations of the temperature and spectral
        #index from normal distribution with mean equal to the maximum likelihood commander value,
        # and standard deviation equal to the commander std.
        T_mean = read_map(template("COM_CompMap_dust-commander_0256_R2.00.fits"), 256, field = 3, mpi_comm=mpi_comm, verbose = False)
        T_std = read_map(template("COM_CompMap_dust-commander_0256_R2.00.fits"), 256, field = 5, mpi_comm=mpi_comm, verbose = False)
        beta_mean = read_map(template("COM_CompMap_dust-commander_0256_R2.00.fits"), 256, field = 6, mpi_comm=mpi_comm, verbose = False)
        beta_std = read_map(template("COM_CompMap_dust-commander_0256_R2.00.fits"), 256, field = 8, mpi_comm=mpi_comm, verbose = False)

        #draw the realisations
        np.random.seed(seed)
        T = T_mean + np.random.randn(len(T_mean)) * T_std
        beta = beta_mean + np.random.randn(len(beta_mean)) * beta_std

        #use modified stefan boltzmann law to relate radiation field strength to temperature and
        #spectral index. Since the interpolated data is only valid for -3 < uval <5 we clip
        #the generated values (the generated values are no where near these limits, but it is good
        #to note this for the future). We then udgrade the uval map to whatever nside is being
        #considered.Since nside is not a parameter Sky knows about we have to get it from
        #A_I, which is not ideal.
        uval_map = hp.ud_grade(np.clip((4. + beta) * np.log10(T / np.mean(T)), -3., 5.), nside_out = nside)
        return uval_map

    @staticmethod
    def read_hd_data(mpi_comm=None):
        # Read in precomputed dust emission properties in infrared as a function of U
        # the radiation field strength for a given grain composition and grain size distribution.
        if (mpi_comm is not None and mpi_comm.rank==0) or (mpi_comm is None):
            data = dict()
            #data_sil contains the emission properties for silicon grains with no iron inclusions.
            data["sil"] = np.genfromtxt(template("sil_fe00_2.0.dat"))
            #data_silfe containts the emission properties for sillicon grains with 5% iron inclusions.
            data["silfe"] = np.genfromtxt(template("sil_fe05_2.0.dat"))
            #data_car contains the emission properties of carbonaceous grains.
            data["car"] = np.genfromtxt(template("car_1.0.dat"))
        elif mpi_comm is not None and mpi_comm.rank>0:
            data = None

        if mpi_comm is not None:
            data = mpi_comm.bcast(data, root=0)

        #get the wavelength and the set of field strengths over which these values were calculated.
        wav = data["sil"][:, 0]
        uvec = np.arange(-3., 5.01, 0.1)
        return data["sil"], data["silfe"], data["car"], wav, uvec

    def hensley_draine_2017(self, *args, **kwargs):
        """Returns dust (T, Q, U) maps as a function of observing frequenvy in GHz, nu. Uses the Hensley and Draine 2017 model.

        This is based on a microphysical model of dust grains, taking into account the strength of the local radiation field, U,
        the grain compositions (carbonaceous, and silicate with varying degrees of iron abundance) and solving for the
        full temperature distribution with grain size.

        *Model Parameters*

        - log U (uval): Radiation field intensity parameter, sets grain temperatures. Must be between -3 and 5. U is the radiation field energy density relative to the MMP83 radiation field. So uval = -0.5 corresponds to a radiation field 10^-0.5 times as intense as the standard interstellar radiation field.
        - fcar: Mass fraction of carbonaceous grains relative to silicate grains
        - f_fe: Fraction of silicate grains with iron inclusions relative to silicate grains.

        Model is calibrated such that fcar = 1 and f_fe = 0 reproduce the Planck
        FIR dust SED. fcar = f_fe >> 1 will also do so but with different
        frequency-dependence of the polarized dust emission. In general,
        fcar =~ 1 + fsilfe is expected, meaning that: 1-f_fe + f_fe = f_car.
        So  in the current implementation f_car should stay ~1.

        :return: function - model (T, Q, U) maps.

        """
        data_sil, data_silfe, data_car, wav, uvec = self.read_hd_data(mpi_comm=self.mpi_comm)

        #interpolate the pre-computed solutions for the emissivity as a function of grain composition F_fe, Fcar, and
        #field strenth U, to get emissivity as a function of (U, wavelength).
        sil_i = RectBivariateSpline(uvec, wav, (data_sil[:, 3 : 84] * (wav[:, np.newaxis] * 1.e-6 / constants.c) * 1.e23).T) # to Jy/sr/H
        car_i = RectBivariateSpline(uvec, wav, (data_car[:, 3 : 84] * (wav[:, np.newaxis] * 1.e-6 / constants.c) * 1.e23).T) # to Jy/sr/H
        silfe_i = RectBivariateSpline(uvec, wav, (data_silfe[:, 3 : 84] * (wav[:, np.newaxis] * 1.e-6 / constants.c) * 1.e23).T) # to Jy/sr/H

        sil_p = RectBivariateSpline(uvec, wav, (data_sil[:, 84 : 165] * (wav[:, np.newaxis] * 1.e-6 / constants.c) * 1.e23).T) # to Jy/sr/H
        car_p = RectBivariateSpline(uvec, wav, (data_car[:, 84 : 165] * (wav[:, np.newaxis] * 1.e-6 / constants.c) * 1.e23).T) # to Jy/sr/H
        silfe_p = RectBivariateSpline(uvec, wav, (data_silfe[:, 84 : 165] * (wav[:, np.newaxis] * 1.e-6 / constants.c) * 1.e23).T) # to Jy/sr/H

        #now draw the random realisation of uval if draw_uval = true
        if self.Draw_Uval:
            self.Uval = self.draw_uval(self.Draw_Uval_Seed, self.nside, mpi_comm=self.mpi_comm)
        elif not self.Draw_Uval:
            pass
        else:
            print("Hensley_Draine_2017 model selected, but draw_uval not set. Set 'draw_uval' to True or False.")

        @FloatOrArray
        def model(nu, **kwargs):
            """Model of Hensley and Draine 2017.

            :param nu: frequency in GHz at which to evaluate the model.
            :type nu: float.
            :return: maps produced using Hensley and Draine 2017 SED.

            """

            if ('use_bandpass' in kwargs) and (kwargs['use_bandpass']):
                return np.zeros((3, len(self.A_I)))

            #Interpolation is done in wavelength and PySMvuses nu in GHz so we must convert from fequency
            #in GHz to wavelength in microns for both the evaluation frequencies and reference frequencies.
            nu_to_lambda = lambda x: 1.e-3 * constants.c / x #Note this is in SI units.

            #Define lambda functions for the evaluation of the intensity and polarisation models.
            #Note that the HD model intepolates in units of Jysr, so we convert to uK_RJ to match the
            #other scalings.
            eval_HD17_I = lambda nu, nu_0: convert_units("Jysr", "uK_RJ", nu) / convert_units("Jysr", "uK_RJ", nu_0) *(
                (1. - self.F_fe) * sil_i.ev(self.Uval, nu_to_lambda(nu))
                + self.Fcar * car_i.ev(self.Uval, nu_to_lambda(nu))
                + self.F_fe * silfe_i.ev(self.Uval, nu_to_lambda(nu)) ) / (
                    (1. - self.F_fe) * sil_i.ev(self.Uval, nu_to_lambda(nu_0))
                    + self.Fcar * car_i.ev(self.Uval, nu_to_lambda(nu_0))
                    + self.F_fe * silfe_i.ev(self.Uval, nu_to_lambda(nu_0))
                )
            eval_HD17_P = lambda nu, nu_0: convert_units("Jysr", "uK_RJ", nu) / convert_units("Jysr", "uK_RJ", nu_0) *(
                (1. - self.F_fe) * sil_p.ev(self.Uval, nu_to_lambda(nu))
                + self.Fcar * car_p.ev(self.Uval, nu_to_lambda(nu))
                + self.F_fe * silfe_p.ev(self.Uval, nu_to_lambda(nu)) ) / (
                    (1. - self.F_fe) * sil_p.ev(self.Uval, nu_to_lambda(nu_0))
                    + self.Fcar * car_p.ev(self.Uval, nu_to_lambda(nu_0))
                    + self.F_fe * silfe_p.ev(self.Uval, nu_to_lambda(nu_0))
                )

            """The interpolation above is only valid for nu > 10GHz. Therefore for frequencies below this
            we implement a fudge and use the Rayleigh Jeans formula. The dust signal at this point should
            be negligible in any case.
            nu_break is the lowest frequency in the interpolation files given for the HD17 model.
            """
            nu_break = 10.
            if (nu <= nu_break):
                #calculate the RJ scaling factor for frequencies below nu_break. At these frequencies
                #dust is largely irrelevant, and so we just use a constant spectral index of 1.54.
                RJ_factor = (nu / nu_break) ** 1.54

                #calculate the HD17  model at the break frequency.
                scaling_I = RJ_factor * eval_HD17_I(nu_break, self.Nu_0_I)
                scaling_P = RJ_factor * eval_HD17_P(nu_break, self.Nu_0_P)

            else:

                #calculate the intensity scaling from reference frequency
                #self.Nu_0_I to frequency nu.
                scaling_I = eval_HD17_I(nu, self.Nu_0_I)
                scaling_P = eval_HD17_P(nu, self.Nu_0_P)

            try:
                scaling_I = hp.ud_grade(scaling_I, nside_out = self.nside)
                scaling_P = hp.ud_grade(scaling_P, nside_out = self.nside)
                if not self.pixel_indices is None:
                    scaling_I = scaling_I[self.pixel_indices]
                    scaling_P = scaling_P[self.pixel_indices]
            except IndexError:
                pass
            return np.array([scaling_I * self.A_I, scaling_P * self.A_Q, scaling_P * self.A_U])
        return model

class AME(object):
    """Class defining attributes and scaling laws of the synchrotron
    component, instantiated with a configuration dictionary containing
    the required parameters of the synchrotron models. The key
    item pairs are then assigned as attributes.

    The current possible attributes are:

    - `Model` : SED used, power law or curved power law.
    - `A_I` : intensity template used -- numpy.ndarray or float.
    - `Nu_0_I` : reference frequency of I template -- float.
    - `Nu_0_P` : reference frequency of Q and U template -- float.
    - `Emissivity` : numerically computed emissivity used to scale AME. In the nominal models this is produced using the SpDust2 code (Ali-Haimoud 2008) -- numpy.ndarray
    - `Nu_Peak_0` : parameter required by SpDust2 -- float
    - `Nu_Peak` : parameter required by SpDust2 -- float, numpy.ndarray
    - `Pol_Frac` : polarisation fraction used in polarised AME model.
    - `Angle_Q` : Q template from which to calculate polarisation angle for AME.
    - `Angle_U` : U template from which to calculate polarisation angle for AME.

    """

    def __init__(self, config):
        for k in config.keys():
            read_key(self, k, config)
        return

    @property
    def A_I(self):
        try:
            return self.__A_I
        except AttributeError:
            print("AME attribute 'A_I' not set.")
            sys.ext(1)

    @property
    def Emissivity(self):
        try:
            return self.__emissivity
        except AttributeError:
            print("AME attribute 'Emissivity' not set.")
            sys.exit(1)

    @property
    def Model(self):
        try:
            return self.__model
        except AttributeError:
            print("AME attribute 'Model' not set.")
            sys.exit(1)

    @property
    def Nu_0_I(self):
        try:
            return self.__nu_0_I
        except AttributeError:
            print("AME attribute 'Nu_0_I' not set.")
            sys.exit(1)

    @property
    def Nu_Peak(self):
        try:
            return self.__nu_peak
        except AttributeError:
            print("AME attribute 'Nu_Peak' not set.")
            sys.exit(1)

    @property
    def Nu_Peak_0(self):
        try:
            return self.__nu_peak_0
        except AttributeError:
            print("AME attribute 'Nu_Peak_0' not set.")
            sys.exit(1)

    @property
    def Angle_Q(self):
        try:
            return self.__angle_q
        except AttributeError:
            print("AME attribute 'Angle_Q' not set.")
            sys.exit(1)

    @property
    def Angle_U(self):
        try:
            return self.__angle_u
        except AttributeError:
            print("AME attribute 'Angle_U' not set.")
            sys.exit(1)

    @property
    def Pol_Frac(self):
        try:
            return self.__pol_frac
        except AttributeError:
            print("AME attribute 'Pol_Frac' not set.")
            sys.exit(1)

    def signal(self):
        """Function to return the selected SED.

        :return: function -- selected model SED.

        """
        return getattr(self, self.Model)()

    def spdust_scaling(self, nu):
        """Returns AME SED at frequency in GHz, nu.
        Implementation of the SpDust2 code of (Ali-Haimoud et al 2012), evaluated for a
        Cold Neutral Medium.

        :param nu: frequency at which to calculate SED.
        :type nu: float.
        :return: spdust SED - float.

        """
        J = interp1d(self.Emissivity[0], self.Emissivity[1], bounds_error = False, fill_value = 0)
        arg1 = nu * self.Nu_Peak_0 / self.Nu_Peak
        arg2 = self.Nu_0_I * self.Nu_Peak_0 / self.Nu_Peak
        scaling = ((self.Nu_0_I / nu) ** 2) * (J(arg1) / J(arg2))
        return scaling

    def spdust(self):
        """Returns AME (T, Q, U) maps as a function of observing frequency, nu.

        :return: function -- AME spdust2 scaling as a function of frequency.
        """
        @FloatOrArray
        def model(nu, **kwargs):
            """Spdust2 unpolarised model.

            :param nu: frequency in GHz at which to calculate the AME maps using
            spdust2.
            :type nu: float.
            :return: AME maps at frequency nu, shape (3, Npix) -- numpy.ndarray.

            """
            return np.array([self.spdust_scaling(nu) * self.A_I, np.zeros_like(self.A_I), np.zeros_like(self.A_I)])
        return model

    def spdust_pol(self):
        """Returns AME (T,Q, U) maps a a function of observing frequency Polarisation
        version of :meth:`pysm.components.spdust` in which the Q and U templates
        are calculated using the polarisation angle from the input Q_Angle and
        U_Angle tepmlates, and the given Pol_Frac.
        Scaling is the same as spdust(self).

        :return: function -- polarised spdust2 model as a function of frequency.
        """
        @FloatOrArray
        def model(nu, **kwargs):
            """We use input Q and U from dust templates in order to make the
            polarisation angle consistent after down or up grading
            resolution. Downgrading polarisatoin angle templates gives
            a different result to downgrading Q and U maps then
            calculating polarisation angle.

            :param nu: frequency in GHz at which to evaluate the model.
            :type nu: float.
            :return: numpy.ndarray -- maps of polarised AME model, shape (3, Npix).

            """
            pol_angle = np.arctan2(self.Angle_U, self.Angle_Q)
            A_Q = self.A_I * self.Pol_Frac * np.cos(pol_angle)
            A_U = self.A_I * self.Pol_Frac * np.sin(pol_angle)
            return self.spdust_scaling(nu) * np.array([self.A_I, A_Q, A_U])
        return model

class Freefree(object):
    """Class defining attributes and scaling laws of the free-free
    component, instantiated with a configuration dictionary containing
    the required parameters of the free-free models. The key
    item pairs are then assigned as attributes.

    The current possible attributes are:

    - `Model` : SED used, for free-free only power law is available.
    - `A_I` : intensity template used -- numpy.ndarray or float.
    - `Nu_0_I` : reference frequency of I template -- float.
    - `Spectral_Index` : spectral index used in power law and curved power law -- numpy.ndarray or float.

    """
    def __init__(self, config):
        for k in config.keys():
            read_key(self, k, config)
        return

    @property
    def Model(self):
        try:
            return self.__model
        except AttributeError:
            print("Freefree attribute 'Model' not set.")
            sys.exit(1)

    @property
    def A_I(self):
        try:
            return self.__A_I
        except AttributeError:
            print("Freefree attribute 'A_I' not set.")
            sys.exit(1)

    @property
    def Nu_0_I(self):
        try:
            return self.__nu_0_I
        except AttributeError:
            print("Freefree attribute 'Nu_0_I' not set.")
            sys.exit(1)

    @property
    def Spectral_Index(self):
        try:
            return self.__spectral_index
        except AttributeError:
            print("Freefree attribute 'Spectral_Index' not set.")
            sys.exit(1)

    def signal(self):
        """Function to return the selected SED.

        :return: function -- selected scaling model.
        """
        return getattr(self, self.Model)()

    def power_law(self):
        """Returns synchrotron (T, Q, U) maps as a function of observation
        freuency, nu.

        This is the simplest model, using only a power law spectral
        dependence.  The map of the spectral index may be a constant
        or spatially varing.

        :return: function -- power law model.

        """
        @FloatOrArray
        def model(nu, **kwargs):
            """Power law scaling model.

            :param nu: frequency at which to calculate the map.
            :type nu: float.
            :return: numpy.ndarray -- power law scaled maps, shape (3, Npix).

            """
            scaling = power_law(nu, self.Nu_0_I, self.Spectral_Index)
            zeros = np.zeros_like(self.A_I)
            return np.array([self.A_I * scaling, zeros, zeros])
        return model

class CMB(object):
    """Class defining attributes and scaling laws of the synchrotron
    component, instantiated with a configuration dictionary containing
    the required parameters of the synchrotron models. The key
    item pairs are then assigned as attributes.

    The current possible attributes are:

    - `Model` : SED law, e.g. taylens.
    - `A_I` : intensity template used -- numpy.ndarray or float.
    - `A_Q` : Q template used -- numpy.ndarray or float.
    - `A_U` : U template used -- numpy.ndarray or float.
    - `cmb_specs` : input unlensed cls in CAMB format -- numpy.ndarray
    - `delensing_ells` : delensing fraction as a function of ell -- numpy.ndarray
    - `nside` : nside at which to generate CMB.
    - `cmb_seed` : random seed for CMB generation.
    - `cmb_specs_lensed` : input lensed cls in CAMB format` -- numpy.ndarray

    """
    def __init__(self, config):
        for k in config.keys():
            read_key(self, k, config)
        return

    @property
    def Model(self):
        try:
            return self.__model
        except AttributeError:
            print("CMB attribute 'Model' not set.")
            sys.exit(1)

    @property
    def CMB_Specs(self):
        try:
            return self.__cmb_specs
        except AttributeError:
            print("CMB attribute 'CMB_Specs' not set.")
            sys.exit(1)

    @property
    def CMB_Specs_Lensed(self):
        try:
            return self.__cmb_specs_lensed
        except AttributeError:
            print("CMB attribute 'CMB_Specs_Lensed' not set.")
            sys.exit(1)

    @property
    def Delens(self):
        try:
            return self.__delens
        except AttributeError:
            print("CMB attribute 'Delens' not set.")
            sys.exit(1)

    @property
    def Delensing_Ells(self):
        try:
            return self.__delensing_ells
        except AttributeError:
            print("CMB attribute 'Delensing_Ells' not set.")
            sys.exit(1)

    @property
    def Nside(self):
        try:
            return self.__nside
        except AttributeError:
            print("CMB attribute 'Nside' not set.")
            sys.exit(1)

    @property
    def CMB_Seed(self):
        try:
            return self.__cmb_seed
        except AttributeError:
            print("CMB attribute 'CMB_Seed' not set.")
            sys.exit(1)

    @property
    def A_I(self):
        try:
            return self.__A_I
        except AttributeError:
            print("CMB attribute 'A_I' not set.")

    @property
    def A_Q(self):
        try:
            return self.__A_Q
        except AttributeError:
            print("CMB attribute 'A_Q' not set.")

    @property
    def A_U(self):
        try:
            return self.__A_U
        except AttributeError:
            print("CMB attribute 'A_U' not set.")

    @property
    def pixel_indices(self):
        try:
            return self.__pixel_indices
        except AttributeError:
            return
            # print("CMB attribute 'pixel_indices' not set.")

    def signal(self):
        """Function to return the selected SED.

        :return: function -- selected model SED.

        """
        return getattr(self, self.Model)()

    def taylens(self):
        """Returns CMB (T, Q, U) maps as a function of observing frequency, nu.

        This code is extracted from the taylens code (reference).

        :return: function -- CMB maps.
        """
        synlmax = 8 * self.Nside #this used to be user-defined.
        data = self.CMB_Specs
        lmax_cl = len(data[0]) + 1
        l = np.arange(int(lmax_cl + 1))
        synlmax = min(synlmax, l[-1])

        #Reading input spectra in CAMB format. CAMB outputs l(l+1)/2pi hence the corrections.
        cl_tebp_arr=np.zeros([10, lmax_cl + 1])
        cl_tebp_arr[0, 2:] = 2 * np.pi * data[1] / (l[2:] * (l[2:] + 1))    #TT
        cl_tebp_arr[1, 2:] = 2 * np.pi * data[2] / (l[2:] * (l[2:] + 1))    #EE
        cl_tebp_arr[2, 2:] = 2 * np.pi * data[3] / (l[2:] * (l[2:] + 1))    #BB
        cl_tebp_arr[4, 2:] = 2 * np.pi * data[4] / (l[2:] * (l[2:] + 1))    #TE
        cl_tebp_arr[5, :] = np.zeros(lmax_cl + 1)                           #EB
        cl_tebp_arr[7, :] = np.zeros(lmax_cl + 1)                           #TB

        if self.Delens:
            cl_tebp_arr[3, 2:] = 2 * np.pi * data[5] * self.Delensing_Ells[1] / (l[2:] * (l[2:] + 1)) ** 2              #PP
            cl_tebp_arr[6,:] = np.zeros(lmax_cl + 1)                                                                    #BP
            cl_tebp_arr[8, 2:] = 2 * np.pi * data[7] * np.sqrt(self.Delensing_Ells[1]) / (l[2:] * (l[2:] + 1)) ** 1.5   #EP
            cl_tebp_arr[9, 2:] = 2 * np.pi * data[6] * np.sqrt(self.Delensing_Ells[1]) / (l[2:] * (l[2:] + 1)) ** 1.5   #TP
        else:
            cl_tebp_arr[3,2:] = 2 * np.pi * data[5] / (l[2:] * (l[2:] + 1)) ** 2        #PP
            cl_tebp_arr[6,:] =np.zeros(lmax_cl+1)                                       #BP
            cl_tebp_arr[8,2:] = 2 * np.pi * data[7] / (l[2:] * (l[2:] + 1)) ** 1.5      #EP
            cl_tebp_arr[9,2:] = 2 * np.pi * data[6] / (l[2:] * (l[2:] + 1)) ** 1.5      #TP

        # Coordinates of healpix pixel centers
        ipos = np.array(hp.pix2ang(self.Nside, np.arange(12 * (self.Nside ** 2))))

        # Simulate a CMB and lensing field
        cmb, aphi = simulate_tebp_correlated(cl_tebp_arr, self.Nside, synlmax, self.CMB_Seed)

        if cmb.ndim == 1:
            cmb = np.reshape(cmb, [1, cmb.size])

        # Compute the offset positions
        phi, phi_dtheta, phi_dphi = hp.alm2map_der1(aphi, self.Nside, lmax = synlmax)

        del aphi

        opos, rot = offset_pos(ipos, phi_dtheta, phi_dphi, pol=True, geodesic=False) #geodesic used to be used defined.
        del phi, phi_dtheta, phi_dphi

        # Interpolate maps one at a time
        maps  = []
        for comp in cmb:
            for m in taylor_interpol_iter(comp, opos, 3, verbose=False, lmax=None): #lmax here needs to be fixed. order of taylor expansion is fixed to 3.
                pass
            maps.append(m)
        del opos, cmb
        #save the map computed for future referemce.
        rm = apply_rotation(maps, rot)

        @FloatOrArray
        def model(nu, **kwargs):
            cmb_map = np.array(rm) * convert_units("uK_CMB", "uK_RJ", nu)
            if self.pixel_indices is None:
                return cmb_map
            else:
                return cmb_map[:, self.pixel_indices]
        return model

    def synfast(self):
        """Function for the calculation of lensed CMB maps directly from
        lensed Cls using healpix's synfast routine.
        """
        # get the spectra. These are in CAMB format, we discard the last
        # three corresponding to dd, dt, de, respectively.
        ell, tt, ee, bb, te, _, _, _ = self.CMB_Specs
        lmax_cl = len(ell) + 1
        ell = np.arange(lmax_cl + 1)

        # in CAMB format so we must divide by the scaling factor
        factor = ell * (ell + 1.) / 2. / np.pi

        cl_teb = np.zeros((6, lmax_cl + 1))
        cl_teb[0, 2:] = tt / factor[2:]
        cl_teb[1, 2:] = ee / factor[2:]
        cl_teb[2, 2:] = bb / factor[2:]
        cl_teb[3, 2:] = te / factor[2:]
        cl_teb[4, 2:] = 0.
        cl_teb[5, 2:] = 0.

        np.random.seed(self.CMB_Seed)
        T, Q, U = hp.synfast(cl_teb, self.Nside, pol=True, new=True, verbose=False)

        @FloatOrArray
        def model(nu, **kwargs):
            cmb_map = np.array([T, Q, U]) * convert_units("uK_CMB", "uK_RJ", nu)
            if self.pixel_indices is None:
                return cmb_map
            else:
                return cmb_map[:, self.pixel_indices]

        return model

    def pre_computed(self):
        """Returns a CMB (T, Q, U) maps as a function of observing frequency, nu.

        This function takes a pre-computed map of the CMB and scales
        it to some new frequency.

        """
        @FloatOrArray
        def model(nu, **kwargs):
            return np.array([self.A_I, self.A_Q, self.A_U]) * convert_units("uK_CMB", "uK_RJ", nu)
        return model

def power_law(nu, nu_0, b):
    """Calculate scaling factor for power-law SED.

    Returns a power law scaling by index b for a map at reference
    frequency nu_0 t0 be scale to frequency nu.

    :param nu: frequency being scaled to.
    :type nu: float.
    :param nu_0: reference frequency of power law.
    :type nu_0: float.
    :param b: spectral index by which to scale.
    :type b: float.

    """
    return (nu / nu_0) ** b

def black_body(nu, nu_0, T):
    """Calculate scaling factor for black body SED.

    Factor to scale a black body emitter of temperature T template
    from frequency nu_0 to frequency nu.

    :param nu: frequency being scaled to.
    :type nu: float.
    :param nu_0: reference frequency of power law.
    :type nu_0: float.
    :param T: temperature of black body function used to scale.
    :type T: float.
    :return: float -- black body at temperature T scaling from frequency nu_0 to nu.

    """
    return B(nu, T) / B(nu_0, T)

def get_decorrelation_matrices(freqs,freq_ref,corrlen) :
    """Function to compute the mean and covariance for the decorrelation

    :param freqs: frequencies at which to calculate covariance structure.
    :type freqs: numpy.array.
    :param freq_ref: reference frequency for constrained map.
    :type freq_ref: float.
    :corrlen: correlation length of imposed Gaussian decorrelation.
    :return: numpy.ndarray(len(freqs), len(freqs)), nump.ndarray(len(freqs)) -- the output covariance and mean.

    """
    if corrlen <= 0:
        rho_mean = np.ones([len(freqs), 1])
        rho_covar = np.zeros([len(freqs), len(freqs)])
    else:
        added_freq = False
        freqtot = np.array([f for f in freqs])
        if not (freq_ref in freqtot):
            freqtot = np.insert(freqtot, 0, freq_ref)
            added_freq = True
        indref = np.where(freqtot == freq_ref)[0][0]

        corrmatrix = np.exp(-0.5 * ((np.log(freqtot[:, None]) - np.log(freqtot[None, :])) / corrlen) ** 2)
        rho_inv = invert_safe(corrmatrix)
        rho_uu = np.delete(np.delete(rho_inv, indref, axis = 0), indref, axis = 1)
        rho_uu = invert_safe(rho_uu)
        rho_inv_cu = rho_inv[:, indref]
        rho_inv_cu=np.transpose(np.array([np.delete(rho_inv_cu, indref)]))
        rho_uu_w, rho_uu_v = np.linalg.eigh(rho_uu)

        rho_covar=np.dot(rho_uu_v, np.dot(np.diag(np.sqrt(np.maximum(rho_uu_w, np.zeros_like(rho_uu_w)))), np.transpose(rho_uu_v)))
        rho_mean=-np.dot(rho_uu, rho_inv_cu)

        if not added_freq:
            rho_covar_new=np.zeros([len(freqtot), len(freqtot)])
            rho_mean_new=np.ones([len(freqtot), 1])
            rho_covar_new[:indref, :indref] = rho_covar[:indref,:indref]
            rho_covar_new[indref + 1:, :indref] = rho_covar[indref:, :indref]
            rho_covar_new[:indref, indref + 1:] = rho_covar[:indref, indref:]
            rho_covar_new[indref + 1:, indref + 1:] = rho_covar[indref:, indref:]
            rho_covar = rho_covar_new
            rho_mean_new[:indref, :] = rho_mean[:indref, :]
            rho_mean_new[indref + 1:, :] = rho_mean[indref:, :]
            rho_mean = rho_mean_new

    return rho_covar, rho_mean

def Add_Decorrelation(Component):
    """Function to calculate a wrapper for some model(nu) function to add
    decorrelation.

    :param Component: instance of one of the classes in :mod:`pysm.component`
    :type Component: class
    :return: function - decorator used to add stochastic decorrelation to an emission model.

    Required parameters:

    - Component.Add_Decorrelation: bool - True = add decorrelation. Flase = do not.
    - Component.Corr_Len: float - correlation length defined in accompanying paper.

    Example use:
    .. code-block::

       class Synchrotron(object):

       def curved_power_law(self):
           @Add_Decorrelation(self)
           def model(nu):
               return np.array([T, Q, U])
           return model

    """
    if Component.Add_Decorrelation:
        def decorrelation(model):
            """This is the actual decorrelation decorator that will be implemented
            once the add_decorrelation function is evaluated.

            """
            def wrapper(nu, **kwargs):
                try:
                    N_freqs = len(nu)
                except TypeError: # nu is a single value
                    N_freqs = 1
                    nu = np.array([nu])
                rho_cov_I, rho_m_I = get_decorrelation_matrices(nu, Component.Nu_0_I, Component.Corr_Len)
                rho_cov_P, rho_m_P = get_decorrelation_matrices(nu, Component.Nu_0_P, Component.Corr_Len)
                extra_I = np.dot(rho_cov_I, np.random.randn(N_freqs))
                extra_P = np.dot(rho_cov_P, np.random.randn(N_freqs))
                decorr = np.zeros((N_freqs, 3))
                decorr[:, 0, None] = rho_m_I + extra_I[:, None]
                decorr[:, 1, None] = rho_m_P + extra_P[:, None]
                decorr[:, 2, None] = rho_m_P + extra_P[:, None]
                decorrelated = decorr[..., None] * model(nu, **kwargs)
                if N_freqs == 1:
                    return decorrelated[0]
                else:
                    return decorrelated
            return wrapper
        return decorrelation

    else:
        """If decorrelation not required do nothing with the decorator."""
        def decorrelation(model):
            def wrapper(nu, **kwargs):
                return model(nu, **kwargs)
            return wrapper
        return decorrelation

"""The following code is edited from the taylens code: Naess,
S. K. and Louis, T. 2013 'Lensing simulations by Taylor expansion -
not so inefficient after all' Journal of Cosmology and Astroparticle
Physics September 2013.  Available at:
https://github.com/amaurea/taylens

"""
def simulate_tebp_correlated(cl_tebp_arr, nside, lmax, seed):
        """This generates correlated T,E,B and Phi maps

        """
        np.random.seed(seed)
        alms=hp.synalm(cl_tebp_arr, lmax = lmax, new = True)
        aphi=alms[-1]
        acmb=alms[0 : -1]
        #Set to zero above map resolution to avoid aliasing
        beam_cut=np.ones(3 * nside)
        for ac in acmb:
                hp.almxfl(ac, beam_cut, inplace = True)
        cmb=np.array(hp.alm2map(acmb, nside, pol = True, verbose = False))
        return cmb, aphi

def taylor_interpol_iter(m, pos, order=3, verbose=False, lmax=None):
    """Given a healpix map m[npix], and a set of positions
    pos[{theta,phi},...], evaluate the values at those positions using
    harmonic Taylor interpolation to the given order (3 by
    default). Successively yields values for each cumulative order up
    to the specified one. If verbose is specified, it will print
    progress information to stderr.

    """
    nside = hp.npix2nside(m.size)
    if lmax is None:
        lmax = 3 * nside
    # Find the healpix pixel centers closest to pos,
    # and our deviation from these pixel centers.
    ipos = hp.ang2pix(nside, pos[0], pos[1])
    pos0 = np.array(hp.pix2ang(nside, ipos))
    dpos = pos[:2] - pos0
    # Take wrapping into account
    bad = dpos[1] > np.pi
    dpos[1, bad] = dpos[1, bad] - 2 * np.pi
    bad = dpos[1] <- np.pi
    dpos[1, bad] = dpos[1, bad] + 2 * np.pi

    # Since healpix' dphi actually returns dphi/sintheta, we choose
    # to expand in terms of dphi*sintheta instead.
    dpos[1] *= np.sin(pos0[0])
    del pos0

    # We will now Taylor expand our healpix field to
    # get approximations for the values at our chosen
    # locations. The structure of this section is
    # somewhat complicated by the fact that alm2map_der1 returns
    # two different derivatives at the same time.
    derivs = [[m]]
    res = m[ipos]
    yield res
    for o in range(1, order + 1):
            # Compute our derivatives
            derivs2 = [None for i in range(o+1)]
            used    = [False for i in range(o+1)]
            # Loop through previous level in steps of two (except last)
            if verbose: tprint("order %d" % o)
            for i in range(o):
                    # Each alm2map_der1 provides two derivatives, so avoid
                    # doing double work.
                    if i < o-1 and i % 2 == 1:
                            continue
                    a = hp.map2alm(derivs[i], use_weights = True, lmax = lmax, iter = 0)
                    derivs[i] = None
                    dtheta, dphi = hp.alm2map_der1(a, nside, lmax = lmax)[-2:]
                    derivs2[i : i + 2] = [dtheta, dphi]
                    del a, dtheta, dphi
                    # Use these to compute the next level
                    for j in range(i, min(i + 2, o + 1)):
                            if used[j]:
                                continue
                            N = comb(o, j) / factorial(o)
                            res += N * derivs2[j][ipos] * dpos[0]**(o-j) * dpos[1]**j
                            used[j] = True
                            # If we are at the last order, we don't need to waste memory
                            # storing the derivatives any more
                            if o == order: derivs2[j] = None
            derivs = derivs2
            yield res

"""The following functions are support routines for reading input
data and preparing it for being lensed. Most of them are only needed
to take care of tiny, curvature-related effects that can be safely
ignored.

"""
def readspec(fname):
    """Read a power spectrum with columns [l,comp1,comp2,....]  into a 2d
    array indexed by l. Entries with missing data are filled with
    0.

    """
    tmp = np.loadtxt(fname).T
    l, tmp = tmp[0], tmp[1:]
    res = np.zeros((len(tmp),np.max(l)+1))
    res[:,np.array(l,dtype=int)] = tmp
    return res

def offset_pos(ipos, dtheta, dphi, pol=False, geodesic=False):
    """Offsets positions ipos on the sphere by a unit length step along
    the gradient dtheta, dphi/sintheta, taking the curvature of the
    sphere into account. If pol is passed, also computes the cos and
    sin of the angle by which (Q,U) must be rotated to take into
    account the change in local coordinate system.

    If geodesic is passed, a quick and dirty, but quite accurate,
    approximation is used.

    Uses the memory of 2 maps (4 if pol) (plus that of the input
    maps).

    """
    opos = np.zeros(ipos.shape)
    if pol and not geodesic:
        orot = np.zeros(ipos.shape)
    else:
        orot = None
    if not geodesic:
            # Loop over chunks in order to conserve memory
            step = 0x10000
            for i in range(0, ipos.shape[1], step):
                    small_opos, small_orot = offset_pos_helper(ipos[:,i:i+step], dtheta[i:i+step], dphi[i:i+step], pol)
                    opos[:,i:i+step] = small_opos
                    if pol: orot[:, i : i + step] = small_orot
    else:
            opos[0] = ipos[0] + dtheta
            opos[1] = ipos[1] + dphi / np.sin(ipos[0])
            opos = fixang(opos)
    return opos, orot

def offset_pos_helper(ipos, dtheta, dphi, pol):
    grad = np.array((dtheta, dphi))
    dtheta, dphi = None, None
    d = np.sum(grad ** 2, 0) ** 0.5
    grad  /= d
    cosd, sind = np.cos(d), np.sin(d)
    cost, sint = np.cos(ipos[0]), np.sin(ipos[0])
    ocost  = cosd * cost - sind * sint * grad[0]
    osint  = (1 - ocost ** 2) ** 0.5
    ophi   = ipos[1] + np.arcsin(sind * grad[1] / osint)
    if not pol:
            return np.array([np.arccos(ocost), ophi]), None
    A      = grad[1] / (sind * cost / sint + grad[0] * cosd)
    nom1   = grad[0] + grad[1] * A
    denom  = 1 + A ** 2
    cosgam = 2 * nom1 ** 2 / denom - 1
    singam = 2 * nom1 * (grad[1] - grad[0] * A) / denom
    return np.array([np.arccos(ocost), ophi]), np.array([cosgam,singam])

def fixang(pos):
    """Handle pole wraparound."""
    a = np.array(pos)
    bad = np.where(a[0] < 0)
    a[0,bad] = -a[0, bad]
    a[1,bad] = a[1, bad]+np.pi
    bad = np.where(a[0] > np.pi)
    a[0,bad] = 2 * np.pi - a[0, bad]
    a[1,bad] = a[1, bad] + np.pi
    return a

def apply_rotation(m, rot):
    """Update Q,U components in polarized map by applying the rotation
    rot, represented as [cos2psi,sin2psi] per pixel. Rot is one of the
    outputs from offset_pos.

    """
    if len(m) < 3:
        return m
    if rot is None:
        return m
    m = np.asarray(m)
    res = m.copy()
    res[1] = rot[0] * m[1] - rot[1] * m[2]
    res[2] = rot[1] * m[1] + rot[0] * m[2]
    return m

# Set up progress prints
t0 = None
def silent(msg):
        pass

def tprint(msg):
        global t0
        if t0 is None:
                t0 = time.time()
        print("%8.2f %s" % (time.time() - t0, msg), file=sys.stderr)



================================================
FILE: pysm/nominal.py
================================================
from __future__ import absolute_import
from .common import read_map, loadtxt
import numpy as np
from healpy import nside2npix
import os

data_dir = os.path.join(os.path.dirname(__file__), 'template')
template = lambda x: os.path.join(data_dir, x)

def models(key, nside, pixel_indices=None, mpi_comm=None):
    model = eval(key)(nside, pixel_indices=pixel_indices, mpi_comm=mpi_comm)
    for m in model:
        m['pixel_indices'] = pixel_indices # include pixel indices in the model dictionary
        m['nside'] = nside
    return model

def d0(nside, pixel_indices=None, mpi_comm=None):
    A_I = read_map(template('dust_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm)
    return [{
        'model': 'modified_black_body',
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': A_I,
        'A_Q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': np.ones(len(A_I)) * 1.54,
        'temp': np.ones(len(A_I)) * 20.,
        'add_decorrelation': False,
    }]

def d1(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'modified_black_body',
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('dust_beta.fits'), nside=nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'temp': read_map(template('dust_temp.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation': False,
    }]

def d2(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'modified_black_body',
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('beta_mean1p59_std0p2.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'temp': read_map(template('dust_temp.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation': False,
    }]

def d3(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'modified_black_body',
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('beta_mean1p59_std0p3.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'temp': read_map(template('dust_temp.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation': False,
    }]

def d4(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'modified_black_body',
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust2comp_I1_ns512_545.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust2comp_Q1_ns512_353.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust2comp_U1_ns512_353.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('dust2comp_beta1_ns512.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'temp': read_map(template('dust2comp_temp1_ns512.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation': False,
    }, {
        'model': 'modified_black_body',
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust2comp_I2_ns512_545.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust2comp_Q2_ns512_353.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust2comp_U2_ns512_353.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('dust2comp_beta2_ns512.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'temp': read_map(template('dust2comp_temp2_ns512.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation': False,
    }]

def d5(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'hensley_draine_2017',
        'draw_uval': True,
        'draw_uval_seed': 4632,
        'fcar': 1.,
        'f_fe': 0.,
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation': False,
    }]

def d6(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'modified_black_body',
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('dust_beta.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'temp': read_map(template('dust_temp.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation': True,
        'corr_len': 5.0
    }]

def d7(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'hensley_draine_2017',
        'draw_uval': True,
        'draw_uval_seed': 4632,
        'fcar': 1.,
        'f_fe': 0.44,
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation' : False,
    }]

def d8(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'hensley_draine_2017',
        'draw_uval': False,
        'uval': 0.2,
        'draw_uval_seed': 4632,
        'fcar': 1.,
        'f_fe': 0.44,
        'nu_0_I': 545.,
        'nu_0_P': 353.,
        'A_I': read_map(template('dust_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'add_decorrelation': False,
    }]

def s0(nside, pixel_indices=None, mpi_comm=None):
    A_I = read_map(template('synch_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm)
    return [{
        'model': 'power_law',
        'nu_0_I': 0.408,
        'nu_0_P': 23.,
        'A_I': A_I,
        'A_Q': read_map(template('synch_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('synch_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': np.ones(len(A_I)) * -3,
    }]

def s1(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'power_law',
        'nu_0_I': 0.408,
        'nu_0_P': 23.,
        'A_I': read_map(template('synch_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('synch_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('synch_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('synch_beta.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
    }]

def s2(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'power_law',
        'nu_0_I': 0.408,
        'nu_0_P': 23.,
        'A_I': read_map(template('synch_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('synch_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('synch_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('beta_latvar.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
    }]

def s3(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'curved_power_law',
        'nu_0_I': 0.408,
        'nu_0_P': 23.,
        'A_I': read_map(template('synch_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('synch_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('synch_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': read_map(template('synch_beta.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_curvature': -0.052,
        'nu_curve': 23.,
    }]

def f1(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'power_law',
        'nu_0_I': 30.,
        'A_I': read_map(template('ff_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'spectral_index': -2.14,
    }]

def c1(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'taylens',
        'cmb_specs': loadtxt(template('camb_lenspotentialCls.dat'), mpi_comm=mpi_comm, unpack=True),
        'delens': False,
        'delensing_ells': loadtxt(template('delens_ells.txt'), mpi_comm=mpi_comm),
        'nside': nside,
        'cmb_seed': 1111
    }]

def c2(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'pre_computed',
        'A_I': read_map(template('lensed_cmb.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_Q': read_map(template('lensed_cmb.fits'), nside, field=1, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'A_U': read_map(template('lensed_cmb.fits'), nside, field=2, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'nside': nside
    }]

def a1(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'spdust',
        'nu_0_I': 22.8,
        'nu_0_P': 22.8,
        'A_I': read_map(template('ame_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'nu_peak_0': 30.,
        'emissivity': loadtxt(template('emissivity.txt'), mpi_comm=mpi_comm, unpack=True),
        'nu_peak': read_map(template('ame_nu_peak_0.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
    }, {
        'model': 'spdust',
        'nu_0_I': 41.0,
        'nu_0_P': 41.0,
        'A_I': read_map(template('ame2_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'nu_peak_0': 30.,
        'emissivity': loadtxt(template('emissivity.txt'), mpi_comm=mpi_comm, unpack=True),
        'nu_peak': 33.35
    }]

def a2(nside, pixel_indices=None, mpi_comm=None):
    return [{
        'model': 'spdust_pol',
        'nu_0_I': 22.8,
        'A_I': read_map(template('ame_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'nu_peak_0': 30.,
        'emissivity': loadtxt(template('emissivity.txt'), mpi_comm=mpi_comm, unpack=True),
        'nu_peak': read_map(template('ame_nu_peak_0.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'pol_frac': 0.02,
        'angle_q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'angle_u': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm)
    }, {
        'model': 'spdust_pol',
        'nu_0_I': 41.0,
        'A_I': read_map(template('ame2_t_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'nu_peak_0': 30.,
        'emissivity': loadtxt(template('emissivity.txt'), mpi_comm=mpi_comm, unpack=True),
        'nu_peak': 33.35,
        'pol_frac': 0.02,
        'angle_q': read_map(template('dust_q_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm),
        'angle_u': read_map(template('dust_u_new.fits'), nside, field=0, pixel_indices=pixel_indices, mpi_comm=mpi_comm)
    }]



================================================
FILE: pysm/pysm.py
================================================
"""                                                                                            
.. module:: pysm
   :platform: Unix
   :synopsis: module containing primary use classes Sky and Instrument.

.. moduleauthor: Ben Thorne <ben.thorne@physics.ox.ac.uk> 
"""

from __future__ import absolute_import, print_function
from scipy import interpolate, integrate
import numpy as np
import healpy as hp
import scipy.constants as constants
import os, sys
from .components import Dust, Synchrotron, Freefree, AME, CMB
from .common import read_key, convert_units, bandpass_convert_units, check_lengths, write_map, build_full_map

class Sky(object):
    """Model sky signal of Galactic foregrounds.

    This class combines the contribtions to the Galactic microwave
    foreground from thermal dust, synchrotron, AME, free-free, and CMB
    emissions.

    Is it inistialised using a dictionary. The keys must be 'cmb', 
    'dust', 'synchrotron', 'freefree', 'ame', and the values must be
    dictionaries with the configuration of the named component, e.g.:
    
    cmb_config = { 
    'model' : 'taylens', 
    'cmb_specs' : np.loadtxt('pysm/template/camb_lenspotentialCls.dat', unpack = True), 
    'delens' : False, 
    'delensing_ells' : np.loadtxt('pysm/template/delens_ells.txt'), 
    'nside' : nside,
    'cmb_seed' : 1111 
    }

    dust_config = {
    'model' : 'modified_black_body',
    'nu_0_I' : 545.,
    'nu_0_P' : 353.,
    'A_I' : pysm.read_map('pysm/template/dust_t_new.fits', nside, field = 0),
    'A_Q' : pysm.read_map('pysm/template/dust_q_new.fits', nside, field = 0),
    'A_U' : pysm.read_map('pysm/template/dust_u_new.fits', nside, field = 0),
    'spectral_index' : 1.5,
    'temp' : pysm.read_map('pysm/template/dust_temp.fits', nside, field = 0)
    }

    sky_config = { 
    'cmb' : cmb_config, 
    'dust': dust_config, 
    }

    """

    def __init__(self, config, mpi_comm=None):
        """Read the configuration dict for Sky

        Implement the configuration file for the Sky instance. Then
        define the getattributes corresponding to the requested
        components.
        """
        self.__config = config
        self.__components = list(config.keys())

        if 'cmb' in self.Components:
            self.cmb = component_adder(CMB, self.Config['cmb'])
        self.Uses_HD17 = False
        if 'dust' in self.Components:
            self.dust = component_adder(Dust, self.Config['dust'], mpi_comm=mpi_comm)
            # Here we add an exception for the HD_17 model. This model requires that for bandpass
            # integration the model be inistialized knowing the bandpass specification, rather than
            # just inidividual frequencies. Therefore we need to be able to call the model directly
            # during the bandpass evaluation.
            if self.Config['dust'][0]['model'] == 'hensley_draine_2017':
                self.Uses_HD17 = True
                self.HD_17_bpass = initialise_hd_dust_model_bandpass(self.dust, mpi_comm=mpi_comm, **self.Config['dust'][0])
        if 'synchrotron' in self.Components:
            self.synchrotron = component_adder(Synchrotron, self.Config['synchrotron'])
        if 'freefree' in self.Components:
            self.freefree = component_adder(Freefree, self.Config['freefree'])
        if 'ame' in self.Components:
            self.ame = component_adder(AME, self.Config['ame'])
        return 

    @property
    def Uses_HD17(self):
        return self.__uses_hd17

    @Uses_HD17.setter
    def Uses_HD17(self, value):
        self.__uses_hd17 = value
    
    @property
    def Config(self):
        try:
            return self.__config
        except AttributeError:
            print("Sky attribute 'Config' not set.")
            sys.exit(1)
            
    @property
    def Components(self):
        try:
            return self.__components
        except AttributeError:
            print("Sky attribute 'Components' not set.")
            sys.exit(1)

    def signal(self, **kwargs):
        """Returns the sky as a function of frequency.

        This returns a function which is the sum of all the requested 
        sky components at the given frequency: (T, Q, U)(nu)."""
        def signal(nu):
            sig = 0.
            for component in self.Components:
                sig += getattr(self, component)(nu, **kwargs)
            return sig
        return signal

    def add_component(self, name, component):
        """Add a already initialized component object to the sky

        Parameters
        ==========
        name : str
            name of the new component, it cannot include spaces or commas
        component : object
            object that provides a signal(nu, **kwargs) function that returns the emission in uK_RJ
        """
        self.__components.append(name)
        setattr(self, name, component.signal)


class Instrument(object):
    """This class contains the attributes and methods required to model
    the instrument observing Sky.

    Instrument contains methods used to perform bandpass integration over an arbitrary bandpass, smooth with a Gaussian beam, and a white Gaussian noise component.
    Instrument is initialised with dictionary, the possible keys are:

    - `frequencies` : frequencies at which to evaluate the Sky model -- numpy.ndarray.
    - `use_smoothing` : whether or not to use smoothing -- bool.
    - `beams` :  Gaussian beam FWHMs in arcmin. Only used if use_smoothing is True. Must be the same length as frequencies.
    - `add_noise` : whether or not to add noise -- bool
    - `sens_I` : sensitivity of intensity in uK_CMBamin. Only used if add_noise is True. Must be same length as frequencies -- numpy.ndarray
    - `sens_P` : sensitivity of polarisation in uK_CMBamin. Only used if add_noise is True. Must be same length as frequencies -- numpy.ndarray
    - `nside` : nside at which to evaluate maps -- int.
    - `noise_seed` : noise seed -- int.
    - `use_bandpass` : whether or not to use bandpass. If this is True `frequencies` is not required -- bool
    - `channels` : frequencies and weights of channels to be calculated as a list of tuples [(frequencies_1, weights_1), (frequencies_2, weights_2) ...] -- list of tuples
    - `channel_names` : list of names used to label the files to which channel maps are written -- string.
    - `output_directory` : directory to which the files will be written -- str.
    - `output_prefix` : prefix for all output files -- str.
    - `output_units` : output units -- str
    
    The use of Instrument is with the :class:`pysm.pysm.Sky` class. Given an instance of Sky we can use the :meth:`pysm.pysm.Instrument.obseve` to apply instrumental effects:
    >>> sky = pysm.Sky(sky_config)
    >>> instrument = pysm.Instrument(instrument_config)
    >>> instrument.observe(sky)
    """
    def __init__(self, config):
        """Specifies the attributes of the Instrument class."""
        for k in config.keys():
            read_key(self, k, config)

        #Get the number of channels of observations.
        if self.Use_Bandpass:
            N_channels = len(self.Channels)
            #Whilst we are here let's normalise the bandpasses.
            self.normalise_bandpass()
        if not self.Use_Bandpass:
            N_channels = len(self.Frequencies)

        #If they are not specified, set the sensitivities and beams
        #to zero, corresponding to noiseless and perfect-resolution
        #observations.
        if not self.Use_Smoothing:
            self.Beams = np.zeros(N_channels)
        if not self.Add_Noise:
            self.Sens_I = np.zeros(N_channels)
            self.Sens_P = np.zeros(N_channels)
        return

    @property
    def Frequencies(self):
        try:
            return self.__frequencies
        except AttributeError:
            print("Instrument attribute 'Frequencies' not set.")
            sys.exit(1)
            
    @property
    def Channels(self):
        try:
            return self.__channels
        except AttributeError:
            print("Instrument attribute 'Channels' not set.")
            sys.exit(1)

    @Channels.setter
    def Channels(self, value):
        self.__channels = value
        
    @property
    def Beams(self):
        try:
            return self.__beams
        except AttributeError:
            print("Instrument attribute 'Beams' not set.")
            sys.exit(1)

    @Beams.setter
    def Beams(self, value):
        self.__beams = value

    @property
    def Sens_I(self):
        try:
            return self.__sens_I
        except AttributeError:
            print("Instrument attribute 'Sens_I' not set.")
            sys.exit(1)

    @Sens_I.setter
    def Sens_I(self, value):
        self.__sens_I = value
            
    @property
    def Sens_P(self):
        try:
            return self.__sens_P
        except AttributeError:
            print("Instrument attribute 'Sens_P' not set.")
            sys.exit(1)

    @Sens_P.setter
    def Sens_P(self, value):
        self.__sens_P = value

    @property
    def Nside(self):
        try:
            return self.__nside
        except AttributeError:
            print("Instrument attribute 'Nside' not set.")
            sys.exit(1)

    @property
    def Noise_Seed(self):
        try:
            return self.__noise_seed
        except AttributeError:
            print("Instrument attribute 'Noise_Seed' not set.")
            sys.exit(1)

    @property
    def Use_Bandpass(self):
        try:
            return self.__use_bandpass
        except AttributeError:
            print("Instrument attribute 'Use_Bandpass' not set.")
            sys.exit(1)

    @property
    def Output_Prefix(self):
        try:
            return self.__output_prefix
        except AttributeError:
            print("Instrument attribute 'Output_Prefix' not set.")
            sys.exit(1)

    @property
    def Output_Directory(self):
        try:
            return self.__output_directory
        except AttributeError:
            print("Instrument attribute 'Output_Directory' not set.")
            sys.exit(1)
            
    @property
    def Channel_Names(self):
        try:
            return self.__channel_names
        except AttributeError:
            print("Instrument attribute 'Channel_Names' not set.")
            sys.exit(1)

    @property 
    def Write_Components(self):
        try:
            return self.__write_components
        except AttributeError:
            print("Instrument attribute 'Write_Components' not set.")
            sys.exit(1)

    @property
    def Add_Noise(self):
        try:
            return self.__add_noise
        except AttributeError:
            print("Instrument attribute 'Add_Noise' not set.")

    @property
    def Use_Smoothing(self):
        try:
            return self.__use_smoothing
        except AttributeError:
            print("Instrument attribute 'Use_Smoothing' not set.")

    @property
    def Output_Units(self):
        try:
            return self.__output_units
        except AttributeError:
            print("Instrument attribute 'Output_Units not set.'")
            
    @property
    def pixel_indices(self):
        try:
            return self.__pixel_indices
        except AttributeError:
            print("Instrument attribute 'pixel_indices' not set.")

    def observe(self, Sky, write_outputs=True):
        """Evaluate and add instrument effects to Sky's signal function.

        This method evaluates the Sky class's signal method at the
        requested frequencies, or over the requested bandpass. Then
        smooths with a Gaussian beam, if requested. Then adds Gaussian
        white noise, if requested. Finally writes the maps to file.

        :param Sky: instance of the :class:`pysm.pysm.Sky` class. 
        :type Sky: class
        :return: no return, writes to file.

        """
        self.print_info()
        signal = Sky.signal()
        output = self.apply_bandpass(signal, Sky) 
        output = self.smoother(output)
        noise = self.noiser()
        output, noise = self.unit_converter(output, noise)
        if write_outputs:
            self.writer(output, noise)
        else:
            return output, noise
        return 
        
    def apply_bandpass(self, signal, Sky):
        """Function to integrate signal over a bandpass.  Frequencies must be
        evenly spaced, if they are not the function will object. Weights
        must be normalisable.

        :param signal: signal function to be integrated of bandpass
        :type param: function
        :return: maps after bandpass integration shape either (N_freqs, 3, Npix) or (N_channels, 3, Npix) -- numpy.ndarray
        
        """
        if not self.Use_Bandpass:
            return signal(self.Frequencies)
        elif self.Use_Bandpass:
            #First need to tell the Sky class that we are using bandpass and if we are using the HD17 model.
            bpass_signal = Sky.signal(use_bandpass = Sky.Uses_HD17)
            # convert to Jysr in order to integrate over bandpass
            signal_Jysr = lambda nu: bpass_signal(nu) * convert_units("uK_RJ", "Jysr", nu)
            bpass_integrated = np.array([bandpass(f, w, signal_Jysr) for (f, w) in self.Channels])
            # We now add an exception in for the case of the HD_17 model. This requires that the model be initialised
            # with the bandpass information in order for the model to be computaitonally efficient. Therefore this is
            # evaluated differently from other models. The function HD_17_bandpass() accepts a tuple (freqs, weights)
            # and returns the integrated signal in units of Jysr. Note that it was initialised when the Instrument
            # class was first instantiated, if use_bandpass = True. Note that the dust signal will still contribute
            # to the bpass_integrated sum in the evaluation above, but will be zero.
            if Sky.Uses_HD17:
                bpass_integrated += np.array(list(map(Sky.HD_17_bpass, self.Channels)))
            return bpass_integrated
        else:
            print("Please set 'Use_Bandpass' for Instrument object.")
            sys.exit(1)

    def normalise_bandpass(self):
        """Function to normalise input bandpasses such that they integrate to one 
        over the stated frequency range.

        """
        self.Channels = [(freqs, weights / np.trapz(weights, freqs * 1.e9)) for (freqs, weights) in self.Channels]
        return 
            
    def smoother(self, map_array):
        """Function to smooth an array of N (T, Q, U) maps with N beams in
        units of arcmin.

        :param map_array:
        :type map_array:
        
        """
        if not self.Use_Smoothing:
            return map_array
        elif self.Use_Smoothing:
            if self.pixel_indices is None:
                full_map = map_array
            else:
                full_map = build_full_map(self.pixel_indices, map_array, self.Nside)
            smoothed_map_array = np.array([hp.smoothing(m, fwhm = np.pi / 180. * b / 60., verbose = False) for (m, b) in zip(full_map, self.Beams)])
            if self.pixel_indices is None:
                return smoothed_map_array
            else:
                assert smoothed_map_array.ndim == 3, \
                    "Assuming map array is 3 dimensional (n_freqs x n_maps x n_pixels)"
                return smoothed_map_array[..., self.pixel_indices]
        else:
            print("Please set 'Use_Smoothing' in Instrument object.")
            sys.exit(1)

    def noiser(self):
        """Calculate white noise maps for given sensitivities.  Returns signal
        + noise, and noise maps at the given nside in (T, Q, U). Input
        sensitivities are expected to be in uK_CMB amin for the rest of
        PySM.

        :param map_array: array of maps to which we add noise. 
        :type map_array: numpy.ndarray.
        :return: map plus noise, and noise -- numpy.ndarray

        """
        try:
            npix = len(self.pixel_indices)
        except TypeError:
            npix = hp.nside2npix(self.Nside)

        if not self.Add_Noise:
            return np.zeros((len(self.Sens_I), 3, npix))
        elif self.Add_Noise:
            # solid angle per pixel in amin2
            pix_amin2 = 4. * np.pi / float(hp.nside2npix(self.Nside)) * (180. * 60. / np.pi) ** 2
            """sigma_pix_I/P is std of noise per pixel. It is an array of length
            equal to the number of input maps."""
            sigma_pix_I = np.sqrt(self.Sens_I ** 2 / pix_amin2)
            sigma_pix_P = np.sqrt(self.Sens_P ** 2 / pix_amin2)
            np.random.seed(seed = self.Noise_Seed)
            noise = np.random.randn(len(self.Sens_I), 3, npix)
            noise[:, 0, :] *= sigma_pix_I[:, None]
            noise[:, 1, :] *= sigma_pix_P[:, None]
            noise[:, 2, :] *= sigma_pix_P[:, None]
            return noise
        else:
            print("Please set 'Add_Noise' in Instrument object.")
            sys.exit(1)

    def unit_converter(self, map_array, noise):
        """Function to handle the conversion of units. 

        If using delta bandpasses just evaluate the unit conversion
        factor normally. If using a bandpass we calculate the
        conversion factor following the Planck HFI definitions.

        :param map_array: signal + noise map to convert units of.
        :type map_array: numpy.ndarray
        :param noise: noise map to conver units of.
        :type noise: numpy.ndarray
        :return: signal + noise map converted to output units, noise map converted to output units -- numpy.ndarray
        """
        if not self.Use_Bandpass:
            #If using a delta bandpass just evaluate the standard unit conversion at
            #the frequencies of interest. All the scaling is done in uK_RJ.
            Uc_signal = np.array(convert_units("uK_RJ", self.Output_Units, self.Frequencies))
        elif self.Use_Bandpass:
            # In the case of a given bandpass we calculate the unit conversion as explained in the documentation
            # of bandpass_convert_units. 
            Uc_signal = np.array([bandpass_convert_units(self.Output_Units, channel) for channel in self.Channels])
        if self.Add_Noise:
            # If noise requested also multiple the calculated noise.
            if not self.Use_Bandpass:
                Uc_noise = np.array(convert_units("uK_CMB", self.Output_Units, self.Frequencies))
            elif self.Use_Bandpass:
                # first convert noise to Jysr then apply the same unit conversion as used for the signal.
                Uc_noise = Uc_signal * np.array([1. / bandpass_convert_units("uK_CMB", channel) for channel in self.Channels])
        elif not self.Add_Noise:
            Uc_noise = np.zeros_like(Uc_signal)
        return Uc_signal[:, None, None] * map_array, Uc_noise[:, None, None] * noise
            
    def file_path(self, channel_name = None, f = None, extra_info = ""):
        """Returns file path for pysm outputs.
        """
        if not self.Use_Bandpass:
            fname = '%s_nu%sGHz_%s_nside%04d.fits'%(self.Output_Prefix, str("%07.2f"%f).replace(".", "p"), extra_info, self.Nside)
        elif self.Use_Bandpass:
            fname = '%s_bandpass_%s_%s_nside%04d.fits'%(self.Output_Prefix, channel_name, extra_info, self.Nside)
        else:
            print("Bandpass set incorrectly.")
            sys.exit(1)
        return os.path.join(self.Output_Directory, fname)

    def writer(self, output, noise):
        """Function to write the total and noise maps to file."""
        if not self.Use_Bandpass:
            if self.Add_Noise:
                for f, o, n in zip(self.Frequencies, output, noise):
                    print(np.std(n, axis = 1))# * np.sqrt(4. * np.pi / float(hp.nside2npix(128)) * (180. * 60. / np.pi) ** 2)
                    print(np.std(o, axis = 1))
                    write_map(self.file_path(f = f, extra_info = "noise"), n, nside=self.Nside, pixel_indices=self.pixel_indices)
                    write_map(self.file_path(f = f, extra_info = "total"), o + n, nside=self.Nside, pixel_indices=self.pixel_indices)
            elif not self.Add_Noise:
                for f, o in zip(self.Frequencies, output):
                    write_map(self.file_path(f = f, extra_info = "total"), o, nside=self.Nside, pixel_indices=self.pixel_indices)
        elif self.Use_Bandpass:
            if self.Add_Noise:
                for c, o, n in zip(self.Channel_Names, output, noise):
                    write_map(self.file_path(channel_name = c, extra_info = "total"), o + n, nside=self.Nside, pixel_indices=self.pixel_indices)
                    write_map(self.file_path(channel_name = c, extra_info = "noise"), n, nside=self.Nside, pixel_indices=self.pixel_indices)
            elif not self.Add_Noise:
                for c, o in zip(self.Channel_Names, output):
                    write_map(self.file_path(channel_name = c, extra_info = "total"), o, nside=self.Nside, pixel_indices=self.pixel_indices)
        return

    def print_info(self):
        """Function to print information about current Instrument
        specifications to screen.

        """
        if not self.Use_Bandpass:
            if not check_lengths(self.Frequencies, self.Beams, self.Sens_I, self.Sens_P):
                print("Check lengths of frequencies, beams, and sensitivities are equal.")
                sys.exit(1)

            print("nu (GHz) | sigma_I (uK_CMB amin) | sigma_P (uK_CMB amin) | FWHM (arcmin) \n")
            for f, s_I, s_P, b in zip(self.Frequencies, self.Sens_I, self.Sens_P, self.Beams):
                print("%07.2f | %05.2f | %05.2f | %05.2f "%(f, s_I, s_P, b))

        elif self.Use_Bandpass:
            print("Channel name | sigma_I (uK_CMB amin) | sigma_P (uK_CMB amin) | FWHM (arcmin) |")
            for cn, s_I, s_P, b in zip(self.Channel_Names, self.Sens_I, self.Sens_P, self.Beams):
                print("%s | %05.2f | %05.2f | %05.2f "%(cn, s_I, s_P, b)) 
        return
    
def bandpass(frequencies, weights, signal):
    """Function to integrate signal over a bandpass.

    Frequencies must be evenly spaced, if they are not the function
    will object. Weights must be able to be normalised to integrate to 1.

    """
    # check that the frequencies are evenly spaced.
    check_bpass_frequencies(frequencies)
    frequency_separation = (frequencies[1] - frequencies[0]) * 1.e9
    # normalise the weights and check that they integrate to 1.
    weights /= np.sum(weights * frequency_separation)
    check_bpass_weights_normalisation(weights, frequency_separation)
    # define the integration: integrand = signal(nu) * w(nu) * d(nu)
    # signal is already in MJysr.
    return sum([signal(nu) * w * frequency_separation for (nu, w) in zip(frequencies, weights)])


def check_bpass_weights_normalisation(weights, spacing):
    """Function that checks the weights of the bandpass were normalised
    properly.

    """
    try:
        np.testing.assert_almost_equal(np.sum(weights * spacing), 1, decimal = 3)
    except AssertionError:
        print("Bandpass weights can not be normalised.")
        sys.exit(1)
    return

def check_bpass_frequencies(frequencies):
    """Function checking the separation of frequencies are even."""
    frequency_separation = frequencies[1] - frequencies[0]
    number_of_frequencies = frequencies.size
    frequency_range = frequencies[-1] - frequencies[0]
    try:
        np.testing.assert_almost_equal(frequency_separation * (number_of_frequencies - 1)/ frequency_range, 1., decimal = 3)
    except AssertionError:
        print("Bandpass frequencies not evenly spaced.")
        sys.exit(1)
    for i in range(frequencies.size - 1):
        spacing = frequencies[i + 1] - frequencies[i]
        try:
            np.testing.assert_almost_equal(spacing / frequency_range, frequency_separation / frequency_range, decimal = 3)
        except AssertionError:
            print("Bandpass frequencies not evenly spaced.")
            sys.exit(1) 
    return

def component_adder(component_class, dictionary_list, **kwargs):
    """This function adds instances of a component class to a Sky
    attribute for that component, e.g. Sky.Dust, thereby allowing for
    multiple populations of that component to be simulated.

    """
    # need this step in order to avoid calling the setup for
    # each scaling law every time the signal is evaluated.
    # each dictionary is a configuration dict used to
    # instantiate the component's class. We then take the
    # signal produced by that population.
    population_signals = [component_class(dic).signal(**kwargs) for dic in dictionary_list]
    # sigs is now a list of functions. Each function is the emission
    # due to a population of the component.
    def total_signal(nu, **kwargs):
        total_component_signal = 0
        # now sum up the contributions of each population at
        # frequency nu. 
        for population_signal in population_signals:
            total_component_signal += population_signal(nu, **kwargs)
        return total_component_signal
    # return the total contribution from all populations
    # as a function of frequency nu. 
    return total_signal

def initialise_hd_dust_model_bandpass(hd_unint_signal, mpi_comm, **kwargs):
    """Function to initialise the bandpass-integrated
    version of the Hensley-Draine 2017 model.
    The keyword arguments are expected to be the initialisation
    dictionary for the HD dust component.

    :param hd_unint_signal: signal of the un-integrated HD17 model. 
    :type hd_unint_signal: function

    """
    #Draw map of uval using Commander dust data.
    uval = Dust.draw_uval(kwargs['draw_uval_seed'], kwargs['nside'], mpi_comm)

    if "pixel_indices" in kwargs and kwargs["pixel_indices"] is not None:
        uval = uval[kwargs["pixel_indices"]]

    #Read in the precomputed dust emission spectra as a function of lambda and U.
    data_sil, data_silfe, data_car, wav, uvec = Dust.read_hd_data()

    c = 2.99792458e10
    fcar = kwargs['fcar']
    f_fe = kwargs['f_fe']

    #Interpolate the dust emission properties in uval and freuency, this is necessary to compute the factor to
    #rescale the dust emission templates to the new model.
    sil_i = interpolate.RectBivariateSpline(uvec,wav,(data_sil[:,3:84]*(wav[:,np.newaxis]*1.e-4/c)*1.e23).T) # to Jy/sr/H
    car_i = interpolate.RectBivariateSpline(uvec,wav,(data_car[:,3:84]*(wav[:,np.newaxis]*1.e-4/c)*1.e23).T) # to Jy/sr/H
    silfe_i = interpolate.RectBivariateSpline(uvec,wav,(data_silfe[:,3:84]*(wav[:,np.newaxis]*1.e-4/c)*1.e23).T) # to Jy/sr/H
    
    sil_p = interpolate.RectBivariateSpline(uvec,wav,(data_sil[:,84:165]*(wav[:,np.newaxis]*1.e-4/c)*1.e23).T) # to Jy/sr/H
    car_p = interpolate.RectBivariateSpline(uvec,wav,(data_car[:,84:165]*(wav[:,np.newaxis]*1.e-4/c)*1.e23).T) # to Jy/sr/H
    silfe_p = interpolate.RectBivariateSpline(uvec,wav,(data_silfe[:,84:165]*(wav[:,np.newaxis]*1.e-4/c)*1.e23).T) # to Jy/sr/H

    nu_to_lambda = lambda x: 1.e-3 * constants.c / x #Note this is in SI units.
    non_int_model_i = lambda nu: (1. - f_fe) * sil_i.ev(uval, nu_to_lambda(nu)) + fcar * car_i.ev(uval, nu_to_lambda(nu)) + f_fe * silfe_i.ev(uval, nu_to_lambda(nu))
    non_int_model_p = lambda nu: (1. - f_fe) * sil_p.ev(uval, nu_to_lambda(nu)) + fcar * car_p.ev(uval, nu_to_lambda(nu)) + f_fe * silfe_p.ev(uval, nu_to_lambda(nu)) 
        
    A_I = kwargs['A_I'] * convert_units("uK_RJ", "Jysr", kwargs['nu_0_I']) / non_int_model_i(kwargs['nu_0_I'])
    A_Q = kwargs['A_Q'] * convert_units("uK_RJ", "Jysr", kwargs['nu_0_P']) / non_int_model_p(kwargs['nu_0_P'])
    A_U = kwargs['A_U'] * convert_units("uK_RJ", "Jysr", kwargs['nu_0_P']) / non_int_model_p(kwargs['nu_0_P'])
    
    def bpass_model(channel):
        """Note that nu is in GHz, and so we have to multipl by 1.e9 in the following functions.
        """
        (nu, t_nu) = channel
                
        # Integrate table over bandpass.
        sil_i_vec = np.zeros(len(uvec))
        car_i_vec = np.zeros(len(uvec))
        silfe_i_vec = np.zeros(len(uvec))
        sil_p_vec = np.zeros(len(uvec))
        car_p_vec = np.zeros(len(uvec))
        silfe_p_vec = np.zeros(len(uvec))
        for i in range(len(uvec)):
            # Note: Table in terms of wavelength in um, increasing
            #       and lambda*I_lambda. Thus we reverse the order
            #       to nu increasing before interpolating to the
            #       bandpass frequencies, then divide by nu to get
            #       I_nu.
            sil_i_vec[i] = np.trapz(t_nu*np.interp(nu*1.e9,c/(wav[::-1]*1.e-4),data_sil[::-1,3+i]*1.e23)/nu*1.e-9, nu*1.e9)
            car_i_vec[i] = np.trapz(t_nu*np.interp(nu*1.e9,c/(wav[::-1]*1.e-4),data_car[::-1,3+i]*1.e23)/nu*1.e-9, nu*1.e9)
            silfe_i_vec[i] = np.trapz(t_nu*np.interp(nu*1.e9,c/(wav[::-1]*1.e-4),data_silfe[::-1,3+i]*1.e23)/nu*1.e-9, nu*1.e9)
        
            sil_p_vec[i] = np.trapz(t_nu*np.interp(nu*1.e9,c/(wav[::-1]*1.e-4),data_sil[::-1,84+i]*1.e23)/nu*1.e-9, nu*1.e9)
            car_p_vec[i] = np.trapz(t_nu*np.interp(nu*1.e9,c/(wav[::-1]*1.e-4),data_car[::-1,84+i]*1.e23)/nu*1.e-9, nu*1.e9)
            silfe_p_vec[i] = np.trapz(t_nu*np.interp(nu*1.e9,c/(wav[::-1]*1.e-4),data_silfe[::-1,84+i]*1.e23)/nu*1.e-9, nu*1.e9)

        # Step 2: Interpolate over U values
        sil_i = interpolate.interp1d(uvec, sil_i_vec)
        car_i = interpolate.interp1d(uvec, car_i_vec)
        silfe_i = interpolate.interp1d(uvec, silfe_i_vec)
        
        sil_p = interpolate.interp1d(uvec, sil_p_vec)
        car_p = interpolate.interp1d(uvec, car_p_vec)
        silfe_p = interpolate.interp1d(uvec, silfe_p_vec)

        #We now compute the final scaling. The integrated quantities sil_i,
        #car_i silfe_i etc.. are in Jy/sr. Therefore we want to convert the
        #templates from uK_RJ to Jy/sr.
        scaling_I = ((1. - f_fe) * sil_i(uval) + fcar * car_i(uval) + f_fe * silfe_i(uval))
        scaling_P = ((1. - f_fe) * sil_p(uval) + fcar * car_p(uval) + f_fe * silfe_p(uval))
        return np.array([scaling_I * A_I, scaling_P * A_Q, scaling_P * A_U])
    return bpass_model



================================================
FILE: pysm/template/__init__.py
================================================
[Empty file]


================================================
FILE: pysm/template/emissivity.txt
================================================
5.023078999999999739e-02 4.609769900000000413e-28
5.069556900000000282e-02 4.865183800000000381e-28
5.116464999999999902e-02 5.134619699999999613e-28
5.163807000000000119e-02 5.418841099999999987e-28
5.211587100000000111e-02 5.718646400000000300e-28
5.259809399999999802e-02 6.034877999999999555e-28
5.308477799999999913e-02 6.368423699999999749e-28
5.357596500000000317e-02 6.720222499999999974e-28
5.407169800000000137e-02 7.091321900000000193e-28
5.457201700000000044e-02 7.482630400000000177e-28
5.507696499999999856e-02 7.895300899999999661e-28
5.558658600000000033e-02 8.330487399999999346e-28
5.610092299999999699e-02 8.789396600000000716e-28
5.662001800000000112e-02 9.273299900000000550e-28
5.714391700000000240e-02 9.783536799999999218e-28
5.767266299999999901e-02 1.032152400000000038e-27
5.820630100000000196e-02 1.088874199999999935e-27
5.874487800000000037e-02 1.148674900000000014e-27
5.928843699999999939e-02 1.211726899999999976e-27
5.983702700000000096e-02 1.278188200000000007e-27
6.039069200000000276e-02 1.348247900000000012e-27
6.094948000000000032e-02 1.422096900000000012e-27
6.151343900000000309e-02 1.499936199999999979e-27
6.208261500000000072e-02 1.581978399999999931e-27
6.265705900000000106e-02 1.668445799999999984e-27
6.323681700000000072e-02 1.759571600000000080e-27
6.382193999999999368e-02 1.855601700000000098e-27
6.441247799999999524e-02 1.956795899999999938e-27
6.500847899999999402e-02 2.063425799999999916e-27
6.560999500000000428e-02 2.175776200000000143e-27
6.621707699999999919e-02 2.294147299999999953e-27
6.682977599999999352e-02 2.418855900000000129e-27
6.744814400000000154e-02 2.550233599999999921e-27
6.807223399999999536e-02 2.688627299999999943e-27
6.870209900000000258e-02 2.834402899999999912e-27
6.933779099999999584e-02 2.987946200000000135e-27
6.997936600000000118e-02 3.149661199999999968e-27
7.062687700000000457e-02 3.319969799999999946e-27
7.128038000000000429e-02 3.499316899999999967e-27
7.193992899999999968e-02 3.688171600000000090e-27
7.260558100000000237e-02 3.887025299999999687e-27
7.327739199999999675e-02 4.096389800000000196e-27
7.395541899999999447e-02 4.316805200000000254e-27
7.463972000000000662e-02 4.548840500000000043e-27
7.533035300000000267e-02 4.793092100000000117e-27
7.602737599999999374e-02 5.050179799999999701e-27
7.673084900000000375e-02 5.320757800000000286e-27
7.744083100000000164e-02 5.605514400000000230e-27
7.815738199999999747e-02 5.905172100000000145e-27
7.888056300000000076e-02 6.220479499999999849e-27
7.961043600000000664e-02 6.552226999999999763e-27
8.034706299999999635e-02 6.901245400000000102e-27
8.109050499999999329e-02 7.268407999999999933e-27
8.184082600000000540e-02 7.654616399999999830e-27
8.259808999999999901e-02 8.060822999999999407e-27
8.336236099999999594e-02 8.488028000000000714e-27
8.413370300000000412e-02 8.937284400000000667e-27
8.491218300000000219e-02 9.409675100000000665e-27
8.569786499999999807e-02 9.906344200000000400e-27
8.649081799999999709e-02 1.042849199999999968e-26
8.729110800000000614e-02 1.097738500000000052e-26
8.809880200000000383e-02 1.155431999999999939e-26
8.891397099999999443e-02 1.216066599999999970e-26
8.973668099999999870e-02 1.279786100000000004e-26
9.056700500000000598e-02 1.346742200000000064e-26
9.140501099999999435e-02 1.417089599999999940e-26
9.225077099999999530e-02 1.490992000000000015e-26
9.310435699999999870e-02 1.568620500000000012e-26
9.396584099999999440e-02 1.650156700000000129e-26
9.483529599999999948e-02 1.735784399999999992e-26
9.571279600000000276e-02 1.825698099999999863e-26
9.659841600000000639e-02 1.920101399999999954e-26
9.749222999999999917e-02 2.019210299999999860e-26
9.839431399999999661e-02 2.123242499999999937e-26
9.930474599999999930e-02 2.232428399999999867e-26
1.002235999999999960e-01 2.347008699999999940e-26
1.011509599999999981e-01 2.467240100000000068e-26
1.020868999999999943e-01 2.593378700000000262e-26
1.030314999999999981e-01 2.866573400000000135e-26
1.039848300000000003e-01 3.010637400000000041e-26
1.049469900000000039e-01 3.161665900000000014e-26
1.059180599999999944e-01 3.319966299999999787e-26
1.068980999999999959e-01 3.485865600000000101e-26
1.078872200000000059e-01 3.659703699999999746e-26
1.088854900000000014e-01 3.841846500000000087e-26
1.098929899999999960e-01 4.032652400000000059e-26
1.109098200000000062e-01 4.232503100000000202e-26
1.119360600000000039e-01 4.441795300000000180e-26
1.129717900000000025e-01 4.845537599999999893e-26
1.140170999999999962e-01 5.081903999999999719e-26
1.150720900000000019e-01 5.329261800000000124e-26
1.161368400000000051e-01 5.588076899999999878e-26
1.172114400000000001e-01 5.858855999999999867e-26
1.182959799999999950e-01 6.142091500000000104e-26
1.193905600000000067e-01 6.438304299999999971e-26
1.204952700000000015e-01 6.748036200000000297e-26
1.216102000000000016e-01 7.071871499999999620e-26
1.227354499999999959e-01 7.410376499999999594e-26
1.238710999999999979e-01 7.764147099999999806e-26
1.250172700000000137e-01 8.377017800000000327e-26
1.261740399999999873e-01 8.772329399999999938e-26
1.273415099999999911e-01 9.185209500000000353e-26
1.285197899999999949e-01 9.616322900000000033e-26
1.297089700000000068e-01 1.006638999999999966e-25
1.309091500000000019e-01 1.053618599999999924e-25
1.321204400000000057e-01 1.102645999999999898e-25
1.333429299999999984e-01 1.153798699999999985e-25
1.345767400000000003e-01 1.207157899999999970e-25
1.358219599999999916e-01 1.262811399999999982e-25
1.370787099999999925e-01 1.320843199999999933e-25
1.383470800000000112e-01 1.412269099999999943e-25
1.396271900000000121e-01 1.476481499999999924e-25
1.409191499999999930e-01 1.543391499999999946e-25
1.422230600000000122e-01 1.613094599999999973e-25
1.435390300000000119e-01 1.685687799999999997e-25
1.448671800000000121e-01 1.761273700000000033e-25
1.462076200000000104e-01 1.839963100000000093e-25
1.475604600000000044e-01 1.921861700000000096e-25
1.489258199999999865e-01 2.007075600000000045e-25
1.503038200000000046e-01 2.131597899999999942e-25
1.516945600000000061e-01 2.225132500000000149e-25
1.530981700000000056e-01 2.322385500000000157e-25
1.545147699999999957e-01 2.423470200000000100e-25
1.559444799999999964e-01 2.528510699999999845e-25
1.573874200000000001e-01 2.637641700000000219e-25
1.588437000000000043e-01 2.750990300000000224e-25
1.603134700000000135e-01 2.868680400000000034e-25
1.617968300000000026e-01 2.990844099999999813e-25
1.632939200000000091e-01 3.158022000000000027e-25
1.648048599999999975e-01 3.291072300000000023e-25
1.663297800000000104e-01 3.429066899999999994e-25
1.678688100000000072e-01 3.572143999999999834e-25
1.694220800000000027e-01 3.720460800000000084e-25
1.709897200000000117e-01 3.874163299999999863e-25
1.725718699999999883e-01 4.033386199999999994e-25
1.741686600000000029e-01 4.198276599999999996e-25
1.757802199999999870e-01 4.368999100000000241e-25
1.774066900000000058e-01 4.590324000000000202e-25
1.790482200000000079e-01 4.774822199999999647e-25
1.807049300000000136e-01 4.965655699999999543e-25
1.823769699999999994e-01 5.162994400000000280e-25
1.840644899999999973e-01 5.366997900000000149e-25
1.857676099999999997e-01 5.577800599999999738e-25
1.874865000000000004e-01 5.795554800000000346e-25
1.892212899999999864e-01 6.020437599999999735e-25
1.909721400000000124e-01 6.252610000000000048e-25
1.927391800000000099e-01 6.492199499999999685e-25
1.945225700000000058e-01 6.739355499999999966e-25
1.963224699999999989e-01 6.994255999999999820e-25
1.981390199999999990e-01 7.257061899999999933e-25
1.999723700000000104e-01 7.527888099999999853e-25
2.018226999999999938e-01 7.806877700000000100e-25
2.036901399999999918e-01 8.094207199999999804e-25
2.055748599999999982e-01 8.390035000000000907e-25
2.074770200000000120e-01 8.694458899999999207e-25
2.093967800000000046e-01 9.007612699999999414e-25
2.113343099999999974e-01 9.329668999999999902e-25
2.132897599999999949e-01 9.660781500000000307e-25
2.152633100000000133e-01 1.000102600000000032e-24
2.172551100000000013e-01 1.035052299999999927e-24
2.192653499999999978e-01 1.070943999999999936e-24
2.212941900000000017e-01 1.107792500000000080e-24
2.233418000000000070e-01 1.145602700000000065e-24
2.254083500000000073e-01 1.184385500000000073e-24
2.274940300000000137e-01 1.224156999999999951e-24
2.295990099999999923e-01 1.264931299999999999e-24
2.317234599999999922e-01 1.306710700000000090e-24
2.338675699999999968e-01 1.349504600000000058e-24
2.360315199999999947e-01 1.393328999999999946e-24
2.382154900000000020e-01 1.438196600000000060e-24
2.404196700000000020e-01 1.484107600000000007e-24
2.426442500000000058e-01 1.531070199999999963e-24
2.448894100000000018e-01 1.579100900000000091e-24
2.471553400000000011e-01 1.628210799999999993e-24
2.494422400000000095e-01 1.678399000000000158e-24
2.517502999999999824e-01 1.729672299999999852e-24
2.540797200000000089e-01 1.782048699999999951e-24
2.564306900000000167e-01 1.835537900000000064e-24
2.588034200000000062e-01 1.890138699999999985e-24
2.611980899999999939e-01 1.945857800000000096e-24
2.636149300000000251e-01 2.002715199999999900e-24
2.660541299999999998e-01 2.060719999999999822e-24
2.685158999999999740e-01 2.119871900000000087e-24
2.710004500000000038e-01 2.180178599999999838e-24
2.735079799999999839e-01 2.241663200000000182e-24
2.760387200000000152e-01 2.304334800000000073e-24
2.785928699999999925e-01 2.368195900000000093e-24
2.811706600000000167e-01 2.433256199999999850e-24
2.837722999999999773e-01 2.499544299999999891e-24
2.863980199999999754e-01 2.567070199999999969e-24
2.890480200000000166e-01 2.635841099999999846e-24
2.917225500000000249e-01 2.705870300000000113e-24
2.944218299999999955e-01 2.777193700000000150e-24
2.971460799999999791e-01 2.849822699999999887e-24
2.998955400000000160e-01 2.923771699999999950e-24
3.026704399999999850e-01 2.999059500000000007e-24
3.054710099999999873e-01 3.075731200000000028e-24
3.082975000000000021e-01 3.153801100000000105e-24
3.111501400000000195e-01 3.233292999999999964e-24
3.140291800000000189e-01 3.314232900000000134e-24
3.169348599999999849e-01 3.396677200000000348e-24
3.198674200000000134e-01 3.480644000000000184e-24
3.228271099999999727e-01 3.566167999999999917e-24
3.258141999999999983e-01 3.653285299999999998e-24
3.288289200000000245e-01 3.742064999999999732e-24
3.318715400000000204e-01 3.832530500000000000e-24
3.349423000000000261e-01 3.924728600000000363e-24
3.380414900000000000e-01 4.018709000000000172e-24
3.411693499999999823e-01 4.114551899999999986e-24
3.443261499999999975e-01 4.212289199999999725e-24
3.475121600000000144e-01 4.311979799999999702e-24
3.507276500000000019e-01 4.413690300000000108e-24
3.539728899999999845e-01 4.517511000000000326e-24
3.572481599999999813e-01 4.623485300000000006e-24
3.605537400000000114e-01 4.731684899999999937e-24
3.638898999999999884e-01 4.842195300000000149e-24
3.672569299999999815e-01 4.955116099999999684e-24
3.706551200000000046e-01 5.070504300000000032e-24
3.740847500000000214e-01 5.188443600000000362e-24
3.775461099999999903e-01 5.309040999999999821e-24
3.810395100000000257e-01 5.432405699999999829e-24
3.845652199999999854e-01 5.558606700000000255e-24
3.881235599999999786e-01 5.687744500000000205e-24
3.917148200000000191e-01 5.819947899999999922e-24
3.953393099999999993e-01 5.955335799999999703e-24
3.989973399999999781e-01 6.093989099999999829e-24
4.026892200000000144e-01 6.236026900000000202e-24
4.064152600000000004e-01 6.381601799999999688e-24
4.101757699999999951e-01 6.530841000000000218e-24
4.139710799999999913e-01 6.683840699999999458e-24
4.178015099999999871e-01 6.840738300000000532e-24
4.216673799999999805e-01 7.001715700000000005e-24
4.255690200000000201e-01 7.166906399999999507e-24
4.295067599999999874e-01 7.336425099999999533e-24
4.334809399999999813e-01 7.510427999999999935e-24
4.374918899999999944e-01 7.689130499999999918e-24
4.415399500000000144e-01 7.872672599999999684e-24
4.456254700000000235e-01 8.061191899999999759e-24
4.497487900000000094e-01 8.254871800000000730e-24
4.539102700000000046e-01 8.453950300000000589e-24
4.581102499999999966e-01 8.658584900000000665e-24
4.623490900000000181e-01 8.868942800000000427e-24
4.666271499999999905e-01 9.085236899999999968e-24
4.709447999999999968e-01 9.307728400000000026e-24
4.753024000000000138e-01 9.536599899999999828e-24
4.797003200000000134e-01 9.772046700000000374e-24
4.841389300000000229e-01 1.001432399999999954e-23
4.886186100000000088e-01 1.026371799999999946e-23
4.931397399999999931e-01 1.052044100000000068e-23
4.977027099999999926e-01 1.078472299999999933e-23
5.023079000000000294e-01 1.105686599999999946e-23
5.069556899999999589e-01 1.133718800000000001e-23
5.116465000000000041e-01 1.162593599999999960e-23
5.163807000000000258e-01 1.192338199999999971e-23
5.211587100000000250e-01 1.222988199999999930e-23
5.259809399999999524e-01 1.254579399999999927e-23
5.308477799999999913e-01 1.287140199999999947e-23
5.357596500000000317e-01 1.320703200000000071e-23
5.407169800000000137e-01 1.355311099999999935e-23
5.457201699999999489e-01 1.391002299999999939e-23
5.507696499999999995e-01 1.427811200000000139e-23
5.558658600000000449e-01 1.465776300000000078e-23
5.610092300000000254e-01 1.504948699999999873e-23
5.662001800000000529e-01 1.545370200000000125e-23
5.714391699999999963e-01 1.587081900000000099e-23
5.767266300000000179e-01 1.630129599999999856e-23
5.820630100000000473e-01 1.674573300000000112e-23
5.874487800000000037e-01 1.720459799999999986e-23
5.928843700000000494e-01 1.767837999999999967e-23
5.983702699999999819e-01 1.816762600000000125e-23
6.039069200000000137e-01 1.867303000000000130e-23
6.094948000000000032e-01 1.919511899999999883e-23
6.151343900000000309e-01 1.973447499999999975e-23
6.208261499999999655e-01 2.029176199999999966e-23
6.265705900000000383e-01 2.086776899999999965e-23
6.323681700000000072e-01 2.146309799999999855e-23
6.382193999999999923e-01 2.207844699999999984e-23
6.441247799999999524e-01 2.271461200000000037e-23
6.500847899999999679e-01 2.337247499999999937e-23
6.560999499999999873e-01 2.405275199999999862e-23
6.621707700000000196e-01 2.475627099999999948e-23
6.682977599999999629e-01 2.548397800000000135e-23
6.744814399999999877e-01 2.623686900000000052e-23
6.807223399999999813e-01 2.701578700000000185e-23
6.870209900000000536e-01 2.782170999999999748e-23
6.933779100000000417e-01 2.865578200000000046e-23
6.997936599999999840e-01 2.951909200000000226e-23
7.062687699999999902e-01 3.041267000000000124e-23
7.128037999999999874e-01 3.133765299999999999e-23
7.193992900000000246e-01 3.229542199999999908e-23
7.260558099999999682e-01 3.328717799999999835e-23
7.327739199999999675e-01 3.431415599999999970e-23
7.395541900000000002e-01 3.537769700000000025e-23
7.463971999999999829e-01 3.647944999999999815e-23
7.533035300000000545e-01 3.762074699999999725e-23
7.602737600000000207e-01 3.880305399999999727e-23
7.673084900000000097e-01 4.002800399999999853e-23
7.744083099999999886e-01 4.129746799999999787e-23
7.815738199999999747e-01 4.261299000000000137e-23
7.888056300000000354e-01 4.397630800000000142e-23
7.961043599999999554e-01 4.538941000000000146e-23
8.034706299999999635e-01 4.685437900000000058e-23
8.109050500000000161e-01 4.837302199999999837e-23
8.184082599999999985e-01 4.994742199999999979e-23
8.259809000000000179e-01 5.157999500000000194e-23
8.336236099999999594e-01 5.327299600000000270e-23
8.413370300000000412e-01 5.502860899999999818e-23
8.491218299999999664e-01 5.684930700000000165e-23
8.569786499999999529e-01 5.873802700000000490e-23
8.649081800000000264e-01 6.069714499999999573e-23
8.729110799999999504e-01 6.272937099999999840e-23
8.809880199999999828e-01 6.483769499999999645e-23
8.891397100000000275e-01 6.702539699999999963e-23
8.973668099999999592e-01 6.929525100000000006e-23
9.056700500000000043e-01 7.165051599999999967e-23
9.140501100000000267e-01 7.409482799999999692e-23
9.225077099999999808e-01 7.663169099999999482e-23
9.310435700000000425e-01 7.926454199999999554e-23
9.396584100000000550e-01 8.199722399999999565e-23
9.483529600000000226e-01 8.483408000000000210e-23
9.571279599999999999e-01 8.777891400000000534e-23
9.659841599999999806e-01 9.083595599999999487e-23
9.749223000000000194e-01 9.400980399999999869e-23
9.839431400000000494e-01 9.730532500000000443e-23
9.930474599999999930e-01 1.007270099999999947e-22
1.002235999999999905e+00 1.042799000000000033e-22
1.011509599999999898e+00 1.079694900000000111e-22
1.020869000000000026e+00 1.118011300000000092e-22
1.030315000000000092e+00 1.157802999999999895e-22
1.039848300000000059e+00 1.199129299999999956e-22
1.049469900000000067e+00 1.242054799999999885e-22
1.059180599999999917e+00 1.286639900000000081e-22
1.068980999999999959e+00 1.332950399999999961e-22
1.078872199999999948e+00 1.381056799999999978e-22
1.088854900000000070e+00 1.431031999999999967e-22
1.098929899999999904e+00 1.482947699999999952e-22
1.109098200000000034e+00 1.536881599999999943e-22
1.119360600000000039e+00 1.592917200000000004e-22
1.129717899999999942e+00 1.651136799999999919e-22
1.140171000000000046e+00 1.711626099999999910e-22
1.150720900000000047e+00 1.774476700000000101e-22
1.161368399999999967e+00 1.839785500000000025e-22
1.172114399999999890e+00 1.907647500000000007e-22
1.182959799999999895e+00 1.978164599999999920e-22
1.193905599999999900e+00 2.051444900000000017e-22
1.204952699999999988e+00 2.127598900000000171e-22
1.216102000000000016e+00 2.206738999999999930e-22
1.227354499999999904e+00 2.288985299999999985e-22
1.238710999999999895e+00 2.374464500000000094e-22
1.250172700000000026e+00 2.463303300000000011e-22
1.261740400000000095e+00 2.555634899999999888e-22
1.273415100000000022e+00 2.651601099999999874e-22
1.285197900000000004e+00 2.751347599999999791e-22
1.297089699999999901e+00 2.855022400000000170e-22
1.309091500000000075e+00 2.962783499999999871e-22
1.321204400000000057e+00 3.074796500000000223e-22
1.333429299999999929e+00 3.191229200000000129e-22
1.345767399999999947e+00 3.312256300000000154e-22
1.358219599999999971e+00 3.438064199999999878e-22
1.370787100000000036e+00 3.568843700000000137e-22
1.383470800000000001e+00 3.704790199999999899e-22
1.396271899999999899e+00 3.846111499999999838e-22
1.409191499999999930e+00 3.993024000000000155e-22
1.422230600000000011e+00 4.145749099999999982e-22
1.435390299999999897e+00 4.304516899999999707e-22
1.448671800000000065e+00 4.469570499999999563e-22
1.462076200000000048e+00 4.641161399999999623e-22
1.475604600000000044e+00 4.819546399999999835e-22
1.489258200000000087e+00 5.004999800000000160e-22
1.503038199999999991e+00 5.197802699999999554e-22
1.516945599999999894e+00 5.398246400000000162e-22
1.530981699999999890e+00 5.606636100000000249e-22
1.545147700000000013e+00 5.823287599999999722e-22
1.559444800000000075e+00 6.048531399999999960e-22
1.573874199999999890e+00 6.282705700000000430e-22
1.588437000000000099e+00 6.526170000000000435e-22
1.603134700000000024e+00 6.779293399999999626e-22
1.617968300000000026e+00 7.042455499999999920e-22
1.632939200000000035e+00 7.316060500000000366e-22
1.648048600000000086e+00 7.600520499999999674e-22
1.663297800000000048e+00 7.896270299999999891e-22
1.678688100000000016e+00 8.203751500000000173e-22
1.694220800000000082e+00 8.523436200000000478e-22
1.709897199999999895e+00 8.855808500000000257e-22
1.725718700000000050e+00 9.201367000000000744e-22
1.741686599999999974e+00 9.560636999999999446e-22
1.757802199999999981e+00 9.934164800000000274e-22
1.774066900000000002e+00 1.032251600000000063e-21
1.790482200000000024e+00 1.072627199999999913e-21
1.807049300000000081e+00 1.114604599999999930e-21
1.823769699999999938e+00 1.158247799999999938e-21
1.840644900000000028e+00 1.203622099999999931e-21
1.857676099999999941e+00 1.250795700000000076e-21
1.874865000000000004e+00 1.299840399999999916e-21
1.892212900000000086e+00 1.350830700000000032e-21
1.909721400000000013e+00 1.403842299999999922e-21
1.927391800000000099e+00 1.458955199999999929e-21
1.945225699999999946e+00 1.516253599999999986e-21
1.963224700000000045e+00 1.575823399999999963e-21
1.981390200000000101e+00 1.637752599999999991e-21
1.999723700000000104e+00 1.702135800000000070e-21
2.018226999999999993e+00 1.769070900000000128e-21
2.036901400000000084e+00 1.838656200000000048e-21
2.055748599999999815e+00 1.910996599999999952e-21
2.074770200000000120e+00 1.986201900000000077e-21
2.093967800000000157e+00 2.064384999999999959e-21
2.113343099999999808e+00 2.145660900000000152e-21
2.132897600000000171e+00 2.230152300000000046e-21
2.152633100000000077e+00 2.317988300000000143e-21
2.172551100000000179e+00 2.409296900000000091e-21
2.192653500000000033e+00 2.504215599999999945e-21
2.212941900000000128e+00 2.602886799999999876e-21
2.233417999999999903e+00 2.705458399999999818e-21
2.254083500000000129e+00 2.812081799999999894e-21
2.274940299999999915e+00 2.922916499999999958e-21
2.295990100000000034e+00 3.038129800000000147e-21
2.317234599999999922e+00 3.157889599999999871e-21
2.338675700000000024e+00 3.282377200000000026e-21
2.360315200000000058e+00 3.411777000000000059e-21
2.382154900000000186e+00 3.546281300000000013e-21
2.404196699999999964e+00 3.686088100000000332e-21
2.426442499999999836e+00 3.831407499999999953e-21
2.448894099999999963e+00 3.982454699999999694e-21
2.471553399999999900e+00 4.139452200000000198e-21
2.494422399999999929e+00 4.302632499999999967e-21
2.517503000000000046e+00 4.472238299999999752e-21
2.540797200000000089e+00 4.648521199999999943e-21
2.564306900000000056e+00 4.831740600000000082e-21
2.588034200000000062e+00 5.022165300000000180e-21
2.611980899999999828e+00 5.220077699999999741e-21
2.636149300000000029e+00 5.425771399999999929e-21
2.660541300000000220e+00 5.639549999999999766e-21
2.685159000000000074e+00 5.861721300000000308e-21
2.710004500000000149e+00 6.092618200000000201e-21
2.735079799999999839e+00 6.332582699999999866e-21
2.760387199999999819e+00 6.581962799999999942e-21
2.785928699999999925e+00 6.841120900000000088e-21
2.811706599999999945e+00 7.110442700000000121e-21
2.837722999999999995e+00 7.390328000000000186e-21
2.863980199999999865e+00 7.681172799999999659e-21
2.890480199999999833e+00 7.983408299999999703e-21
2.917225499999999805e+00 8.297483999999999325e-21
2.944218300000000177e+00 8.623853200000000555e-21
2.971460800000000013e+00 8.962984999999999853e-21
2.998955399999999827e+00 9.315379299999999568e-21
3.026704399999999850e+00 9.681558599999999985e-21
3.054710099999999873e+00 1.006204199999999964e-20
3.082974999999999799e+00 1.045738800000000013e-20
3.111501399999999862e+00 1.086816900000000048e-20
3.140291799999999967e+00 1.129499199999999962e-20
3.169348600000000182e+00 1.173845799999999999e-20
3.198674200000000134e+00 1.219922599999999991e-20
3.228271100000000171e+00 1.267794799999999953e-20
3.258141999999999872e+00 1.317533300000000075e-20
3.288289199999999912e+00 1.369208599999999877e-20
3.318715399999999871e+00 1.422896099999999974e-20
3.349422999999999817e+00 1.478672300000000106e-20
3.380414899999999889e+00 1.536618499999999886e-20
3.411693500000000157e+00 1.596818499999999935e-20
3.443261500000000197e+00 1.659357200000000147e-20
3.475121600000000033e+00 1.724325200000000136e-20
3.507276500000000130e+00 1.791817199999999878e-20
3.539728900000000067e+00 1.861927899999999953e-20
3.572481600000000146e+00 1.934757199999999973e-20
3.605537399999999781e+00 2.010411800000000050e-20
3.638898999999999884e+00 2.089000099999999972e-20
3.672569300000000148e+00 2.170630600000000141e-20
3.706551199999999824e+00 2.255420499999999962e-20
3.740847500000000103e+00 2.343494900000000117e-20
3.775461099999999792e+00 2.434976299999999964e-20
3.810395100000000035e+00 2.529991599999999876e-20
3.845652199999999965e+00 2.628677299999999972e-20
3.881235600000000119e+00 2.731180199999999932e-20
3.917148200000000191e+00 2.837638500000000150e-20
3.953393099999999993e+00 2.948200100000000184e-20
3.989973399999999781e+00 3.063024500000000264e-20
4.026892199999999811e+00 3.182279600000000173e-20
4.064152599999999893e+00 3.306127900000000025e-20
4.101757700000000284e+00 3.434739600000000277e-20
4.139710799999999580e+00 3.568300199999999950e-20
4.178015099999999649e+00 3.706996699999999811e-20
4.216673799999999694e+00 3.851026600000000222e-20
4.255690200000000090e+00 4.000585899999999794e-20
4.295067600000000319e+00 4.155884699999999724e-20
4.334809400000000146e+00 4.317139600000000152e-20
4.374918899999999944e+00 4.484577199999999829e-20
4.415399500000000366e+00 4.658431000000000124e-20
4.456254699999999680e+00 4.838938700000000080e-20
4.497487900000000316e+00 5.026351000000000239e-20
4.539102699999999935e+00 5.220930299999999726e-20
4.581102500000000077e+00 5.422946600000000102e-20
4.623490900000000181e+00 5.632671100000000110e-20
4.666271499999999683e+00 5.850396300000000437e-20
4.709448000000000079e+00 6.076425499999999906e-20
4.753023999999999916e+00 6.311063700000000321e-20
4.797003199999999801e+00 6.554629400000000559e-20
4.841389300000000340e+00 6.807459299999999429e-20
4.886186099999999755e+00 7.069901999999999775e-20
4.931397399999999820e+00 7.342308099999999946e-20
4.977027099999999926e+00 7.625044299999999998e-20
5.023079000000000072e+00 7.918498499999999803e-20
5.069556900000000255e+00 8.223069100000000030e-20
5.116464999999999819e+00 8.539163899999999860e-20
5.163807000000000258e+00 8.867195000000000072e-20
5.211587100000000028e+00 9.207626699999999663e-20
5.259809399999999968e+00 9.560896800000000258e-20
5.308477800000000357e+00 9.927481500000000070e-20
5.357596499999999651e+00 1.030785699999999972e-19
5.407169800000000137e+00 1.070254300000000054e-19
5.457201699999999711e+00 1.111205399999999942e-19
5.507696499999999773e+00 1.153693199999999981e-19
5.558658600000000227e+00 1.197772199999999901e-19
5.610092299999999810e+00 1.243501399999999980e-19
5.662001799999999641e+00 1.290939999999999927e-19
5.714391700000000185e+00 1.340149500000000004e-19
5.767266300000000179e+00 1.391193299999999997e-19
5.820630099999999807e+00 1.444137500000000070e-19
5.874487799999999815e+00 1.499049799999999923e-19
5.928843699999999828e+00 1.556000999999999937e-19
5.983702700000000263e+00 1.615062200000000003e-19
6.039069200000000137e+00 1.676308000000000032e-19
6.094947999999999588e+00 1.739816300000000053e-19
6.151343899999999643e+00 1.805669100000000092e-19
6.208261499999999877e+00 1.873946600000000007e-19
6.265705900000000383e+00 1.944731599999999954e-19
6.323681699999999850e+00 2.018109799999999898e-19
6.382194000000000145e+00 2.094173699999999942e-19
6.441247800000000190e+00 2.173023900000000112e-19
6.500847900000000124e+00 2.254747500000000239e-19
6.560999500000000317e+00 2.339440800000000197e-19
6.621707699999999974e+00 2.427204500000000091e-19
6.682977600000000074e+00 2.518149299999999958e-19
6.744814400000000099e+00 2.612384600000000046e-19
6.807223399999999813e+00 2.710013100000000062e-19
6.870209899999999870e+00 2.811149600000000044e-19
6.933779099999999751e+00 2.915910399999999972e-19
6.997936600000000062e+00 3.024419200000000183e-19
7.062687699999999680e+00 3.136796500000000013e-19
7.128038000000000096e+00 3.253166899999999762e-19
7.193992899999999580e+00 3.373657700000000106e-19
7.260558099999999904e+00 3.498404899999999943e-19
7.327739199999999897e+00 3.627544399999999775e-19
7.395541899999999558e+00 3.761216699999999767e-19
7.463972000000000051e+00 3.899560500000000155e-19
7.533035299999999879e+00 4.042722800000000049e-19
7.602737600000000207e+00 4.190851199999999979e-19
7.673084900000000097e+00 4.344093199999999679e-19
7.744083100000000108e+00 4.502615499999999582e-19
7.815738200000000191e+00 4.666566499999999866e-19
7.888056299999999688e+00 4.836114500000000260e-19
7.961043599999999998e+00 5.011420300000000301e-19
8.034706299999999857e+00 5.192645300000000468e-19
8.109050500000000383e+00 5.379969800000000021e-19
8.184082599999999985e+00 5.573559699999999746e-19
8.259809000000000623e+00 5.773594299999999586e-19
8.336236100000000704e+00 5.980253999999999602e-19
8.413370300000000412e+00 6.193710800000000181e-19
8.491218299999999886e+00 6.414156400000000143e-19
8.569786499999999307e+00 6.641762799999999875e-19
8.649081799999999376e+00 6.876722300000000100e-19
8.729110800000000836e+00 7.119232999999999711e-19
8.809880200000000272e+00 7.369475500000000365e-19
8.891397100000000719e+00 7.627641400000000085e-19
8.973668099999999370e+00 7.893915799999999872e-19
9.056700499999999820e+00 8.168496199999999605e-19
9.140501099999999823e+00 8.451565500000000051e-19
9.225077100000000030e+00 8.743337900000000602e-19
9.310435699999999315e+00 9.044003500000000781e-19
9.396584100000000106e+00 9.353740899999999757e-19
9.483529600000000670e+00 9.672736400000000410e-19
9.571279600000000443e+00 1.000117300000000045e-18
9.659841600000000028e+00 1.033924499999999925e-18
9.749223000000000638e+00 1.068717199999999984e-18
9.839431400000000494e+00 1.104511900000000062e-18
9.930474600000000152e+00 1.141323800000000019e-18
1.002236000000000082e+01 1.179170500000000007e-18
1.011509599999999942e+01 1.218069399999999929e-18
1.020869000000000071e+01 1.258039700000000002e-18
1.030315000000000047e+01 1.299097499999999955e-18
1.039848300000000059e+01 1.341260499999999967e-18
1.049469900000000067e+01 1.384539999999999953e-18
1.059180600000000005e+01 1.428950000000000012e-18
1.068980999999999959e+01 1.474505399999999918e-18
1.078872199999999992e+01 1.521220500000000048e-18
1.088854899999999937e+01 1.569105199999999920e-18
1.098929900000000082e+01 1.618171400000000005e-18
1.109098200000000034e+01 1.668432799999999931e-18
1.119360600000000083e+01 1.719899999999999993e-18
1.129717899999999986e+01 1.772579700000000155e-18
1.140170999999999957e+01 1.826479200000000166e-18
1.150720899999999958e+01 1.881602800000000084e-18
1.161368399999999923e+01 1.937956199999999977e-18
1.172114400000000067e+01 1.995547899999999932e-18
1.182959800000000072e+01 2.054378300000000063e-18
1.193905600000000078e+01 2.114454099999999948e-18
1.204952699999999943e+01 2.175772700000000009e-18
1.216102000000000061e+01 2.238335599999999900e-18
1.227354500000000037e+01 2.302139499999999844e-18
1.238710999999999984e+01 2.367183199999999889e-18
1.250172700000000070e+01 2.433465199999999996e-18
1.261740400000000051e+01 2.500978100000000003e-18
1.273415100000000066e+01 2.569716399999999898e-18
1.285197900000000004e+01 2.639673999999999887e-18
1.297089700000000079e+01 2.710839000000000078e-18
1.309091500000000075e+01 2.783201499999999989e-18
1.321204400000000057e+01 2.856759799999999910e-18
1.333429300000000062e+01 2.931497400000000192e-18
1.345767400000000080e+01 3.007398399999999816e-18
1.358219599999999971e+01 3.084450000000000177e-18
1.370787100000000081e+01 3.162644499999999873e-18
1.383470799999999912e+01 3.241968100000000017e-18
1.396271899999999988e+01 3.322401200000000086e-18
1.409191500000000019e+01 3.403927400000000073e-18
1.422230599999999967e+01 3.486526900000000171e-18
1.435390300000000074e+01 3.570188200000000214e-18
1.448671799999999976e+01 3.654891599999999652e-18
1.462076199999999915e+01 3.740619699999999737e-18
1.475604599999999955e+01 3.827358500000000372e-18
1.489258200000000087e+01 3.915088300000000233e-18
1.503038199999999946e+01 4.003787500000000159e-18
1.516945600000000027e+01 4.093438800000000276e-18
1.530981700000000068e+01 4.184032600000000186e-18
1.545147700000000057e+01 4.275547500000000013e-18
1.559444799999999987e+01 4.367973000000000262e-18
1.573874200000000023e+01 4.461298199999999781e-18
1.588437000000000054e+01 4.555500999999999654e-18
1.603134700000000024e+01 4.650563000000000081e-18
1.617968300000000070e+01 4.746470600000000305e-18
1.632939199999999857e+01 4.843210900000000154e-18
1.648048599999999908e+01 4.940772100000000149e-18
1.663297800000000137e+01 5.039142899999999798e-18
1.678688100000000105e+01 5.138313700000000244e-18
1.694220800000000082e+01 5.238288100000000192e-18
1.709897199999999984e+01 5.339049100000000237e-18
1.725718699999999828e+01 5.440577500000000354e-18
1.741686599999999885e+01 5.542862800000000276e-18
1.757802200000000070e+01 5.645894800000000207e-18
1.774066900000000047e+01 5.749663499999999635e-18
1.790482199999999935e+01 5.854157099999999854e-18
1.807049299999999903e+01 5.959371099999999978e-18
1.823769700000000071e+01 6.065284700000000302e-18
1.840644899999999851e+01 6.171886900000000032e-18
1.857676100000000119e+01 6.279159300000000139e-18
1.874865000000000137e+01 6.387084500000000336e-18
1.892212900000000175e+01 6.495644399999999752e-18
1.909721400000000102e+01 6.604805099999999991e-18
1.927391799999999833e+01 6.714539099999999841e-18
1.945225699999999946e+01 6.824816600000000139e-18
1.963224699999999956e+01 6.935598100000000138e-18
1.981390199999999879e+01 7.046843000000000707e-18
1.999723699999999837e+01 7.158510899999999690e-18
2.018226999999999904e+01 7.270551500000000228e-18
2.036901399999999995e+01 7.382902599999999405e-18
2.055748600000000081e+01 7.495505300000000246e-18
2.074770200000000031e+01 7.608290999999999570e-18
2.093967800000000068e+01 7.721187099999999232e-18
2.113343100000000163e+01 7.834102299999999277e-18
2.132897600000000082e+01 7.946948800000000350e-18
2.152633099999999899e+01 8.059632100000000053e-18
2.172551100000000091e+01 8.172052599999999555e-18
2.192653500000000122e+01 8.284101400000000095e-18
2.212941899999999862e+01 8.395661699999999531e-18
2.233417999999999992e+01 8.506611900000000526e-18
2.254083500000000129e+01 8.616823100000000219e-18
2.274940300000000093e+01 8.726160999999999616e-18
2.295990099999999856e+01 8.834485200000000699e-18
2.317234600000000100e+01 8.941656300000000340e-18
2.338675699999999935e+01 9.047519799999999238e-18
2.360315200000000146e+01 9.151923799999999595e-18
2.382154900000000097e+01 9.254701800000000266e-18
2.404196699999999964e+01 9.355690899999999581e-18
2.426442499999999924e+01 9.454730800000000452e-18
2.448894100000000051e+01 9.551646800000000191e-18
2.471553400000000167e+01 9.646259799999999489e-18
2.494422399999999840e+01 9.738393300000000540e-18
2.517502999999999957e+01 9.827867899999999714e-18
2.540797200000000089e+01 9.914502900000000332e-18
2.564306900000000056e+01 9.998118799999999891e-18
2.588034199999999885e+01 1.007852999999999994e-17
2.611980900000000005e+01 1.015554900000000035e-17
2.636149299999999940e+01 1.022899300000000037e-17
2.660541299999999865e+01 1.029867500000000040e-17
2.685159000000000162e+01 1.036441500000000048e-17
2.710004500000000149e+01 1.042602499999999996e-17
2.735079800000000105e+01 1.048332200000000034e-17
2.760387199999999908e+01 1.053612399999999957e-17
2.785928699999999836e+01 1.058425800000000046e-17
2.811706600000000122e+01 1.062753700000000031e-17
2.837723000000000084e+01 1.066578599999999980e-17
2.863980199999999954e+01 1.069882999999999964e-17
2.890480200000000011e+01 1.072649800000000010e-17
2.917225499999999982e+01 1.074862400000000056e-17
2.944218299999999999e+01 1.076505100000000064e-17
2.971460799999999836e+01 1.077562700000000057e-17
2.998955399999999827e+01 1.078020899999999930e-17
3.026704399999999850e+01 1.077866000000000055e-17
3.054710100000000139e+01 1.077084900000000046e-17
3.082975000000000065e+01 1.075666099999999971e-17
3.111501399999999862e+01 1.073599499999999986e-17
3.140291799999999967e+01 1.070875800000000007e-17
3.169348600000000005e+01 1.067487499999999998e-17
3.198674199999999956e+01 1.063428500000000011e-17
3.228271099999999905e+01 1.058694499999999987e-17
3.258142000000000138e+01 1.053282800000000011e-17
3.288289199999999823e+01 1.047192800000000069e-17
3.318715399999999960e+01 1.040425499999999980e-17
3.349423000000000172e+01 1.032983999999999926e-17
3.380414900000000245e+01 1.024873499999999988e-17
3.411693499999999801e+01 1.016101800000000055e-17
3.443261499999999842e+01 1.006678099999999950e-17
3.475121599999999944e+01 9.966135500000000074e-18
3.507276499999999686e+01 9.859215300000000744e-18
3.539728900000000067e+01 9.746170199999999866e-18
3.572481599999999702e+01 9.627172199999999754e-18
3.605537400000000048e+01 9.502412299999999608e-18
3.638898999999999972e+01 9.372096100000000437e-18
3.672569299999999970e+01 9.236449099999999236e-18
3.706551199999999824e+01 9.095712300000000069e-18
3.740847500000000281e+01 8.950164499999999951e-18
3.775461099999999703e+01 8.800056699999999604e-18
3.810395100000000213e+01 8.645684199999999542e-18
3.845652199999999965e+01 8.487330000000000274e-18
3.881235600000000119e+01 8.325301600000000364e-18
3.917148199999999747e+01 8.159914800000000331e-18
3.953393100000000260e+01 7.991490399999999796e-18
3.989973400000000225e+01 7.820354900000000373e-18
4.026892200000000344e+01 7.646838800000000181e-18
4.064152599999999893e+01 7.471274699999999859e-18
4.101757700000000284e+01 7.293996999999999785e-18
4.139710800000000290e+01 7.115339700000000220e-18
4.178015099999999649e+01 6.935633000000000356e-18
4.216673800000000227e+01 6.755203900000000023e-18
4.255690200000000090e+01 6.574375800000000343e-18
4.295067600000000141e+01 6.393464899999999640e-18
4.334809400000000323e+01 6.212778100000000158e-18
4.374918900000000122e+01 6.032613199999999651e-18
4.415399500000000188e+01 5.853257300000000169e-18
4.456254700000000213e+01 5.674985100000000265e-18
4.497487900000000138e+01 5.498059099999999671e-18
4.539102700000000112e+01 5.322728999999999898e-18
4.581102500000000077e+01 5.149229800000000238e-18
4.623490900000000181e+01 4.977783900000000278e-18
4.666271499999999861e+01 4.808609700000000015e-18
4.709447999999999723e+01 4.641883300000000224e-18
4.753023999999999916e+01 4.477781299999999893e-18
4.797003200000000334e+01 4.316466100000000168e-18
4.841389300000000162e+01 4.158084500000000102e-18
4.886186099999999755e+01 4.002769799999999946e-18
4.931397400000000175e+01 3.850639900000000230e-18
4.977027100000000104e+01 3.701798200000000086e-18
5.023078999999999894e+01 3.556334300000000146e-18
5.069556899999999899e+01 3.414325200000000183e-18
5.116465000000000174e+01 3.275835099999999823e-18
5.163806999999999903e+01 3.140916600000000043e-18
5.211587099999999850e+01 3.009614199999999918e-18
5.259809400000000323e+01 2.881955800000000003e-18
5.308477800000000002e+01 2.757961000000000050e-18
5.357596499999999651e+01 2.637641000000000138e-18
5.407169799999999782e+01 2.520998299999999819e-18
5.457201700000000244e+01 2.408026499999999980e-18
5.507696500000000128e+01 2.298711600000000140e-18
5.558658599999999694e+01 2.193032899999999849e-18
5.610092300000000165e+01 2.090962400000000065e-18
5.662001800000000173e+01 1.992466500000000167e-18
5.714391700000000185e+01 1.897505799999999821e-18
5.767266300000000001e+01 1.806037099999999935e-18
5.820630100000000340e+01 1.718013000000000035e-18
5.874487799999999993e+01 1.633375599999999962e-18
5.928843700000000183e+01 1.552067300000000024e-18
5.983702699999999908e+01 1.474026800000000066e-18
6.039069200000000137e+01 1.399189599999999989e-18
6.094948000000000121e+01 1.327487899999999922e-18
6.151343899999999820e+01 1.258851500000000084e-18
6.208261499999999700e+01 1.193207100000000015e-18
6.265705899999999673e+01 1.130479300000000007e-18
6.323681700000000205e+01 1.070590300000000063e-18
6.382193999999999789e+01 1.013460500000000062e-18
6.441247799999999302e+01 9.590083499999999111e-19
6.500847899999999413e+01 9.071508399999999899e-19
6.560999499999999784e+01 8.578035699999999790e-19
6.621707700000000330e+01 8.108808899999999762e-19
6.682977599999999541e+01 7.662962199999999711e-19
6.744814399999999921e+01 7.239624000000000398e-19
6.807223399999999458e+01 6.837917899999999882e-19
6.870209900000000403e+01 6.456965899999999815e-19
6.933779099999999573e+01 6.095892100000000324e-19
6.997936599999999885e+01 5.753826299999999870e-19
7.062687699999999325e+01 5.429906700000000438e-19
7.128037999999999386e+01 5.123285199999999793e-19
7.193992900000000645e+01 4.833129800000000048e-19
7.260558100000000081e+01 4.558624900000000434e-19
7.327739200000000608e+01 4.298977299999999955e-19
7.395541900000000624e+01 4.053420899999999771e-19
7.463971999999999696e+01 3.821217499999999963e-19
7.533035300000000234e+01 3.601658800000000090e-19
7.602737600000000384e+01 3.394068400000000240e-19
7.673084900000000630e+01 3.197802600000000196e-19
7.744083100000000286e+01 3.012251599999999869e-19
7.815738199999999836e+01 2.836839099999999956e-19
7.888056299999999510e+01 2.671022200000000179e-19
7.961043600000000708e+01 2.514290499999999802e-19
8.034706300000000567e+01 2.366165400000000225e-19
8.109050499999999317e+01 2.226197899999999934e-19
8.184082600000000696e+01 2.093967100000000095e-19
8.259808999999999912e+01 1.969078199999999899e-19
8.336236100000000704e+01 1.851159999999999949e-19
8.413370299999999702e+01 1.739862799999999933e-19
8.491218299999999886e+01 1.634856299999999910e-19
8.569786499999999307e+01 1.535827400000000072e-19
8.649081800000000442e+01 1.442478300000000020e-19
8.729110799999999415e+01 1.354524599999999956e-19
8.809880200000000627e+01 1.271693999999999935e-19
8.891397100000000364e+01 1.193724899999999999e-19
8.973668100000000436e+01 1.120366300000000096e-19
9.056700499999999465e+01 1.051374700000000007e-19
9.140501100000000179e+01 9.865155499999999669e-20
9.225077100000000030e+01 9.255630299999999672e-20
9.310435699999999315e+01 8.682993499999999456e-20
9.396584099999999751e+01 8.145148499999999816e-20
9.483529599999999959e+01 7.640080099999999668e-20
9.571279599999999732e+01 7.165855999999999611e-20
9.659841600000000028e+01 6.720628699999999517e-20
9.749223000000000638e+01 6.302637299999999430e-20
9.839431399999999428e+01 5.910208299999999694e-20
9.930474599999999441e+01 5.541754700000000506e-20
1.002236000000000047e+02 5.195778500000000213e-20
1.011509599999999978e+02 4.870869900000000000e-20
1.020869000000000000e+02 4.565705700000000019e-20
1.030314999999999941e+02 4.279047199999999949e-20
1.039848300000000023e+02 4.009738300000000197e-20
1.049469899999999996e+02 3.756701599999999763e-20
1.059180599999999970e+02 3.518934799999999864e-20
1.068980999999999995e+02 3.295506500000000255e-20
1.078872199999999992e+02 3.085551699999999995e-20
1.088854900000000043e+02 2.888266899999999974e-20
1.098929899999999975e+02 2.702905200000000004e-20
1.109098199999999963e+02 2.528771400000000142e-20
1.119360599999999977e+02 2.365220599999999856e-20
1.129717899999999986e+02 2.211644099999999955e-20
1.140170999999999992e+02 2.067474200000000063e-20
1.150720900000000029e+02 1.932177899999999998e-20
1.161368400000000065e+02 1.805253300000000098e-20
1.172114399999999961e+02 1.686226400000000100e-20
1.182959800000000001e+02 1.574648399999999907e-20
1.193905599999999936e+02 1.470093999999999913e-20
1.204952700000000050e+02 1.372158899999999999e-20
1.216102000000000061e+02 1.280458799999999972e-20
1.227354500000000002e+02 1.194627900000000075e-20
1.238710999999999984e+02 1.114318299999999984e-20
1.250172699999999963e+02 1.039199000000000007e-20
1.261740400000000051e+02 9.689556700000000485e-21
1.273415099999999995e+02 9.032896500000000496e-21
1.285197900000000004e+02 8.419180299999999433e-21
1.297089699999999937e+02 7.845730399999999755e-21
1.309091500000000110e+02 7.310017000000000471e-21
1.321204400000000021e+02 6.809653699999999415e-21
1.333429299999999955e+02 6.342394000000000209e-21
1.345767400000000009e+02 5.906125400000000321e-21
1.358219599999999900e+02 5.498861199999999670e-21
1.370787100000000009e+02 5.118739200000000265e-21
1.383470800000000054e+02 4.764014100000000161e-21
1.396271900000000130e+02 4.433053099999999817e-21
1.409191500000000019e+02 4.124319699999999626e-21
1.422230600000000038e+02 3.836378100000000327e-21
1.435390299999999968e+02 3.567883500000000081e-21
1.448671799999999905e+02 3.317575599999999813e-21
1.462076199999999915e+02 3.084272900000000184e-21
1.475604600000000062e+02 2.866867299999999888e-21
1.489258199999999874e+02 2.664319299999999993e-21
1.503038200000000018e+02 2.475653499999999829e-21
1.516945599999999956e+02 2.299954500000000155e-21
1.530981700000000103e+02 2.136362999999999828e-21
1.545147699999999986e+02 1.984072500000000001e-21
1.559444799999999987e+02 1.842326799999999964e-21
1.573874199999999917e+02 1.710416599999999896e-21
1.588437000000000126e+02 1.587677599999999925e-21
1.603134699999999953e+02 1.473487500000000017e-21
1.617968299999999999e+02 1.367264499999999918e-21
1.632939200000000142e+02 1.268464500000000060e-21
1.648048599999999908e+02 1.176579599999999997e-21
1.663297799999999995e+02 1.091135799999999956e-21
1.678688099999999963e+02 1.011691400000000084e-21
1.694220799999999940e+02 9.378344099999999514e-22
1.709897200000000055e+02 8.691814399999999182e-22
1.725718699999999899e+02 8.053757099999999974e-22
1.741686599999999885e+02 7.460850600000000330e-22
1.757802200000000141e+02 6.909993300000000434e-22
1.774066899999999976e+02 6.398302700000000302e-22
1.790482199999999864e+02 5.923093799999999932e-22
1.807049299999999903e+02 5.481864099999999703e-22
1.823769700000000000e+02 5.072280699999999900e-22
1.840644900000000064e+02 4.692168099999999646e-22
1.857676099999999906e+02 4.339491899999999757e-22
1.874865000000000066e+02 4.012354499999999917e-22
1.892212900000000104e+02 3.708983299999999820e-22
1.909721399999999960e+02 3.427721899999999942e-22
1.927391800000000046e+02 3.167022700000000179e-22
1.945225700000000018e+02 2.925439900000000216e-22
1.963224700000000098e+02 2.701622900000000228e-22
1.981390199999999879e+02 2.494310900000000213e-22
1.999723700000000122e+02 2.302327199999999783e-22
2.018226999999999975e+02 2.124574800000000200e-22
2.036901400000000137e+02 1.960031200000000113e-22
2.055748600000000010e+02 1.807744500000000111e-22
2.074770200000000102e+02 1.666828799999999900e-22
2.093967800000000068e+02 1.536460499999999965e-22
2.113343099999999879e+02 1.415874299999999934e-22
2.132897600000000011e+02 1.304359300000000089e-22
2.152633099999999899e+02 1.201256999999999954e-22
2.172551100000000019e+02 1.105954600000000088e-22
2.192653500000000122e+02 1.017882699999999971e-22
2.212941899999999862e+02 9.365140200000000512e-23
2.233418000000000063e+02 8.613591500000000232e-23
2.254083500000000129e+02 7.919638399999999943e-23
2.274940300000000093e+02 7.279063400000000402e-23
2.295990099999999927e+02 6.687949099999999507e-23
2.317234599999999887e+02 6.142656399999999569e-23
2.338675700000000006e+02 5.639803199999999726e-23
2.360315200000000004e+02 5.176245999999999889e-23
2.382154899999999884e+02 4.749062199999999867e-23
2.404196699999999964e+02 4.355535000000000263e-23
2.426442499999999995e+02 3.993138499999999898e-23
2.448894099999999980e+02 3.659525399999999709e-23
2.471553399999999954e+02 3.352514300000000094e-23
2.494422399999999982e+02 3.070080299999999900e-23
2.517503000000000100e+02 2.810345099999999711e-23
2.540797200000000089e+02 2.571561600000000033e-23
2.564306900000000269e+02 2.352112400000000060e-23
2.588034200000000169e+02 2.150498999999999869e-23
2.611980899999999792e+02 1.965333800000000029e-23
2.636149300000000153e+02 1.795332699999999888e-23
2.660541299999999865e+02 1.639308000000000027e-23
2.685158999999999878e+02 1.496165300000000081e-23
2.710004500000000007e+02 1.364887099999999916e-23
2.735079799999999750e+02 1.244534799999999952e-23
2.760387200000000121e+02 1.134242799999999937e-23
2.785928700000000049e+02 1.033211700000000013e-23
2.811706599999999980e+02 9.407033799999999843e-24
2.837722999999999729e+02 8.560364399999999303e-24
2.863980199999999741e+02 7.785816700000000680e-24
2.890480200000000082e+02 7.077581799999999683e-24
2.917225500000000125e+02 6.430296599999999793e-24
2.944218299999999999e+02 5.839009799999999961e-24
2.971460799999999836e+02 5.299150600000000114e-24
2.998955399999999827e+02 4.806500400000000021e-24
3.026704399999999850e+02 4.357166200000000308e-24
3.054710099999999784e+02 3.947556599999999825e-24
3.082975000000000136e+02 3.574359400000000136e-24
3.111501400000000217e+02 3.234521600000000140e-24
3.140291799999999967e+02 2.925229700000000014e-24
3.169348600000000147e+02 2.643892900000000177e-24
3.198674199999999814e+02 2.388126900000000168e-24
3.228271100000000047e+02 2.155737700000000103e-24
3.258142000000000280e+02 1.944707499999999818e-24
3.288289199999999823e+02 1.753185799999999892e-24
3.318715399999999818e+02 1.579469600000000091e-24
3.349422999999999888e+02 1.422004499999999958e-24
3.380414900000000102e+02 1.279350099999999992e-24
3.411693500000000085e+02 1.150192799999999919e-24
3.443261499999999842e+02 1.033330400000000042e-24
3.475121599999999944e+02 9.276599099999999937e-25
3.507276499999999828e+02 8.321724900000000168e-25
3.539728900000000067e+02 7.459450999999999729e-25
3.572481599999999844e+02 6.681335400000000457e-25
3.605537400000000048e+02 5.979660600000000051e-25
3.638899000000000115e+02 5.347374600000000044e-25
3.672569300000000112e+02 4.778036900000000225e-25
3.706551200000000108e+02 4.265767799999999800e-25
3.740847499999999854e+02 3.805202799999999777e-25
3.775461099999999988e+02 3.391449700000000127e-25
3.810395100000000070e+02 3.020049700000000076e-25
3.845652200000000107e+02 2.686941099999999976e-25
3.881235599999999977e+02 2.388426100000000091e-25
3.917148199999999747e+02 2.121140300000000125e-25
3.953393100000000118e+02 1.882024400000000010e-25
3.989973400000000083e+02 1.668298499999999979e-25
4.026892199999999775e+02 1.477438099999999903e-25
4.064152599999999893e+02 1.307152399999999970e-25
4.101757700000000000e+02 1.155363800000000009e-25
4.139710799999999722e+02 1.020191300000000031e-25
4.178015100000000075e+02 8.999352700000000099e-26
4.216673799999999801e+02 7.930459600000000081e-26
4.255690200000000232e+02 6.981297900000000537e-26
4.295067599999999857e+02 6.139251800000000328e-26
4.334809399999999755e+02 5.393037000000000169e-26
4.374918900000000122e+02 4.732377599999999773e-26
4.415399499999999762e+02 4.148058399999999795e-26
4.456254700000000071e+02 3.631790000000000044e-26
4.497487899999999854e+02 3.176123300000000237e-26
4.539102700000000254e+02 2.774371599999999813e-26
4.581102500000000077e+02 2.420538399999999924e-26
4.623490899999999897e+02 2.109251799999999972e-26
4.666271499999999719e+02 1.835704399999999891e-26
4.709447999999999865e+02 1.595599099999999874e-26
4.753023999999999774e+02 1.385095799999999951e-26
4.797003199999999765e+02 1.200768200000000009e-26
4.841389300000000162e+02 1.039562000000000051e-26
4.886186099999999897e+02 8.987567100000000139e-27
4.931397400000000175e+02 7.759311399999999669e-27
4.977027100000000246e+02 6.689327199999999578e-27



================================================
FILE: pysm/test/__init__.py
================================================
import os
import os.path

def get_testdata(*filename):
    import pysm
    # if a test data dir is set by run-tests.py, use it.
    # otherwise fall back to the test data in source (e.g. if ran directly with py.test)
    reldir = os.path.join(os.path.abspath(os.path.join(os.path.dirname(pysm.__file__), '..')), "test_data")
    testdata = os.environ.get('PYSM_TESTDATA_DIR', reldir)
    return os.path.join(testdata, *filename)



================================================
FILE: pysm/test/test_common.py
================================================
import unittest
import numpy as np
from astropy.io import fits
import scipy.constants as constants
from pysm import common
import os
import matplotlib.pyplot as plt
from . import get_testdata

class CommonTests(unittest.TestCase):

    def testSafeInvert(self):
        np.random.seed(1234)
        l = np.random.randn(3) ** 2
        A = np.random.randn(3, 3)
        # construct positive definite matrix M
        M = np.dot(np.dot(A, np.diag(l)), np.transpose(A))
        # invert M with invert_safe
        M_inv = common.invert_safe(M)
        ident = np.dot(M_inv, M)
        self.assertAlmostEqual(np.sum(ident), 3.)
        
    def testConvert_Units(self):
        a1 = common.convert_units("K_CMB", "uK_RJ", 300.)
        a2 = common.convert_units("uK_RJ", "K_CMB", 300.)
        self.assertAlmostEqual(1., a1 * a2)
        a1 = common.convert_units("K_CMB", "MJysr", 300.)
        a2 = common.convert_units("MJysr", "K_CMB", 300.)
        self.assertAlmostEqual(1., a1 * a2)
        
        """Validation against ECRSC tables.
        https://irsasupport.ipac.caltech.edu/index.php?/Knowledgebase/
        Article/View/181/20/what-are-the-intensity-units-of-the-planck
        -all-sky-maps-and-how-do-i-convert-between-them
        These tables are based on the following tables: 
        h = 6.626176e-26 erg*s
        k = 1.380662e-16 erg/L
        c = 2.997792458e1- cm/s
        T_CMB = 2.726
        The impact of the incorrect CMB temperature is especially impactful
        and limits some comparison to only ~2/3 s.f.
        """

        uK_CMB_2_K_RJ_30 = 9.77074e-7
        uK_CMB_2_K_RJ_143 = 6.04833e-7
        uK_CMB_2_K_RJ_857 = 6.37740e-11

        self.assertAlmostEqual(uK_CMB_2_K_RJ_30, common.convert_units("uK_CMB", "K_RJ", 30.))
        self.assertAlmostEqual(uK_CMB_2_K_RJ_143, common.convert_units("uK_CMB", "K_RJ", 143.))
        self.assertAlmostEqual(uK_CMB_2_K_RJ_857, common.convert_units("uK_CMB", "K_RJ", 857.))

        K_CMB_2_MJysr_30 = 27.6515 
        K_CMB_2_MJysr_143 = 628.272 
        K_CMB_2_MJysr_857 = 22565.1

        self.assertAlmostEqual(K_CMB_2_MJysr_30 / common.convert_units("K_RJ", "MJysr", 30.), 1., places = 4)
        self.assertAlmostEqual(K_CMB_2_MJysr_143 / common.convert_units("K_RJ", "MJysr", 143.), 1., places = 4)
        self.assertAlmostEqual(K_CMB_2_MJysr_857 / common.convert_units("K_RJ", "MJysr", 857.), 1., places = 4)
        
        uK_CMB_2_MJysr_30 = 2.7e-5
        uK_CMB_2_MJysr_143 = 0.0003800
        uK_CMB_2_MJysr_857 = 1.43907e-6

        #Note that the MJysr definition seems to match comparatively poorly. The
        #definitions of h, k, c in the document linked above are in cgs and differ
        #from those on wikipedia. This may conflict with the scipy constants I use.

        self.assertAlmostEqual(uK_CMB_2_MJysr_30 / common.convert_units("uK_CMB", "MJysr", 30.), 1., places = 2)
        self.assertAlmostEqual(uK_CMB_2_MJysr_143 / common.convert_units("uK_CMB", "MJysr", 143.), 1., places = 2)
        self.assertAlmostEqual(uK_CMB_2_MJysr_857 / common.convert_units("uK_CMB", "MJysr", 857.), 1., places = 2)

class test_Bandpass_Unit_Conversion(unittest.TestCase):
    def setUp(self):
        """To test the bandpass unit conversion we use the Planck detector
        averaged bandpasses provided here:
        https://wiki.cosmos.esa.int/planckpla/index.php/The_RIMO. We
        compute the unit conversion factors for these bandpasses and
        compare them to the official Planck factors provided here:
        https://wiki.cosmos.esa.int/planckpla/index.php/UC_CC_Tables

        """
        # Read in the fits file. This contains only the HFI frequencies 100 -> 857.
        planck_HFI_file = get_testdata("HFI_RIMO_R1.10.fits")
        hdu = fits.open(planck_HFI_file)
        bandpasses = [hdu[i].data for i in range(2, 8)]
        # The table contains 4 lists: wavenumber, transmission, 1-sigma uncertainty, flag.
        # We are only interested in wavenumber and transmission.
        channels = []
        for b in bandpasses:
            wavenumber, transmission, _, _ = list(zip(*b))
            frequency = 1e-7 * constants.c * np.array(wavenumber)
            # exclude the element frqeuency[0] = 0
            filt = lambda x: (x[0] > 1.) & (x[0] < 1200)
            freqs, weights = list(zip(*filter(filt, zip(frequency, transmission))))
            channels.append((np.array(freqs), np.array(weights)))
            
        """Planck-provided coefficients for K_CMB to MJysr.
        These should only be taken to the first decimal place.

        """
        self.CMB2MJysr_avg_100_planck = 244.0960
        self.CMB2MJysr_avg_143_planck = 371.7327
        self.CMB2MJysr_avg_217_planck = 483.6874
        self.CMB2MJysr_avg_353_planck = 287.4517
        self.CMB2MJysr_avg_545_planck = 58.0356
        self.CMB2MJysr_avg_857_planck = 2.2681

        """And for MJysr to K_RJ"""
        self.MJysr2KRJ_avg_100_planck = 0.0032548074
        self.MJysr2KRJ_avg_143_planck = 0.0015916707
        self.MJysr2KRJ_avg_217_planck = 0.00069120334
        self.MJysr2KRJ_avg_353_planck = 0.00026120163
        self.MJysr2KRJ_avg_545_planck = 0.00010958025
        self.MJysr2KRJ_avg_857_planck = 4.4316316e-5

        """Do pysm calculation"""
        self.CMB2MJysr_avg_100_pysm = common.bandpass_convert_units("K_CMB", "MJysr", channels[0], nu_c = 100.)
        self.CMB2MJysr_avg_143_pysm = common.bandpass_convert_units("K_CMB", "MJysr", channels[1], nu_c = 143.)
        self.CMB2MJysr_avg_217_pysm = common.bandpass_convert_units("K_CMB", "MJysr", channels[2], nu_c = 217.)
        self.CMB2MJysr_avg_353_pysm = common.bandpass_convert_units("K_CMB", "MJysr", channels[3], nu_c = 353.)
        self.CMB2MJysr_avg_545_pysm = common.bandpass_convert_units("K_CMB", "MJysr", channels[4], nu_c = 545.)
        #self.CMB2MJysr_avg_857_pysm = common.bandpass_convert_units("K_CMB", "MJysr", channels[5], nu_c = 857.)

        self.RJ2MJysr_avg_100_pysm = common.bandpass_convert_units("K_RJ", "MJysr", channels[0], nu_c = 100.)
        self.RJ2MJysr_avg_143_pysm = common.bandpass_convert_units("K_RJ", "MJysr", channels[1], nu_c = 143.)
        self.RJ2MJysr_avg_217_pysm = common.bandpass_convert_units("K_RJ", "MJysr", channels[2], nu_c = 217.)
        self.RJ2MJysr_avg_353_pysm = common.bandpass_convert_units("K_RJ", "MJysr", channels[3], nu_c = 353.)
        self.RJ2MJysr_avg_545_pysm = common.bandpass_convert_units("K_RJ", "MJysr", channels[4], nu_c = 545.)
        #self.RJ2MJysr_avg_857_pysm = common.bandpass_convert_units("K_RJ", "MJysr", channels[5], nu_c = 857.)
        
    def tearDown(self):
        return
    
    # def test_bandpass_unit_conversion_CMB2MJysr(self):
        """Note that the precision is limited by uncertainty on the bandpass central frequency.
        """
        """
        np.testing.assert_almost_equal(self.CMB2MJysr_avg_100_pysm / self.CMB2MJysr_avg_100_planck, 1., decimal = 3)
        np.testing.assert_almost_equal(self.CMB2MJysr_avg_143_pysm / self.CMB2MJysr_avg_143_planck, 1., decimal = 3)
        np.testing.assert_almost_equal(self.CMB2MJysr_avg_217_pysm / self.CMB2MJysr_avg_217_planck, 1., decimal = 3)
        np.testing.assert_almost_equal(self.CMB2MJysr_avg_353_pysm / self.CMB2MJysr_avg_353_planck, 1., decimal = 3)
        np.testing.assert_almost_equal(self.CMB2MJysr_avg_545_pysm / self.CMB2MJysr_avg_545_planck, 1., decimal = 3)
        #np.testing.assert_almost_equal(self.CMB2MJysr_avg_857_pysm / self.CMB2MJysr_avg_857_planck, 1., decimal = 3)

        np.testing.assert_almost_equal(self.RJ2MJysr_avg_100_pysm * self.MJysr2KRJ_avg_100_planck, 1., decimal = 1)
        np.testing.assert_almost_equal(self.RJ2MJysr_avg_143_pysm * self.MJysr2KRJ_avg_143_planck, 1., decimal = 1)
        np.testing.assert_almost_equal(self.RJ2MJysr_avg_217_pysm * self.MJysr2KRJ_avg_217_planck, 1., decimal = 1)
        np.testing.assert_almost_equal(self.RJ2MJysr_avg_353_pysm * self.MJysr2KRJ_avg_353_planck, 1., decimal = 1)
        np.testing.assert_almost_equal(self.RJ2MJysr_avg_545_pysm * self.MJysr2KRJ_avg_545_planck, 1., decimal = 1)
        #np.testing.assert_almost_equal(self.RJ2MJysr_avg_857_pysm * self.MJysr2RJ_avg_857_planck, 1., decimal = 3)
      
        return
        """
    

class test_Check_Lengths(unittest.TestCase):
    def setUp(self):
        self.list1 = [1] * 20
        self.list2 = [2] * 20
        self.list3 = np.ones(20)
        self.list4 = np.ones(40)
        self.list5 = [3] * 40
        self.list6 = [(1, 2), (3, 4), (5, 6), (6, 7), (8, 9)]
        self.list7 = np.random.randn(5) 
        return

    def tearDown(self):
        self.list1 = None
        self.list2 = None
        self.list3 = None
        self.list4 = None
        self.list5 = None
        self.list6 = None
        self.list7 = None
        return

    def test_check_lengths(self):
        self.assertTrue(common.check_lengths(self.list1, self.list2, self.list3))
        self.assertTrue(common.check_lengths(self.list6, self.list7))
        self.assertFalse(common.check_lengths(self.list1, self.list3, self.list4))
        self.assertFalse(common.check_lengths(self.list1, self.list5))
        return

class test_Bandpass_Convert_Units(unittest.TestCase):
    def setUp(self):
        #first check the integration
        nsamples = 50
        nu2 = 40.
        nu1 = 20.

        weights = np.ones(nsamples) / (nu2 - nu1)
        freqs = np.linspace(nu1, nu2, nsamples)
        self.simple_channel = (freqs, weights)
        
        #for a tophat bandpass we can write down the unit conversion factor analytically.
        # For Jysr -> CMB:
        self.UcJysr2CMB = 1.e-26 / (common.B(nu2, 2.7255) - common.B(nu1, 2.7255))        
        return
    
    def tearDown(self):
        return
    
    def test_bandpass_convert_units(self):
        Uc1 = common.bandpass_convert_units("K_CMB", self.simple_channel)
        np.testing.assert_almost_equal(Uc1, self.UcJysr2CMB)
        return

    
def main():
    unittest.main()

if __name__ == '__main__':
    main()



================================================
FILE: pysm/test/test_components.py
================================================
import unittest, os
from pysm import components, common, read_map, convert_units
from pysm import get_template_dir

try:
    from astropy.analytic_functions import blackbody_nu
except ImportError:
    from astropy.modeling.blackbody import blackbody_nu
import numpy as np, healpy as hp
import matplotlib.pyplot as plt
import scipy.constants as constants
from pysm.nominal import models

from . import get_testdata

class ComponentsTests(unittest.TestCase):
    
    def testPower_Law(self):
        scaling1 = components.power_law(120., 30., -0.5)
        scaling2 = components.power_law(1., 27., 1./3.)
        scaling3 = components.power_law(1., 1., 2.)
        self.assertAlmostEqual(scaling1, 0.5, places = 9)
        self.assertAlmostEqual(scaling2, 1. / 3., places = 9)
        self.assertAlmostEqual(scaling3, 1., places = 9)

    def testBlack_Body(self):
        astropy = blackbody_nu(90.e9, 100.) / blackbody_nu(30.e9, 100.)
        pysm = components.black_body(90., 30., 100.)
        self.assertAlmostEqual(astropy, pysm)

class test_Dust(unittest.TestCase):
    def setUp(self):
        data_dir = get_template_dir()
    
        d1_config = models("d1", 64)
        dust = components.Dust(d1_config[0])
        signal = dust.signal()

        dust_1_30GHz = read_map(get_testdata('benchmark', 'check1therm_30p0_64.fits'), 64, field = (0, 1, 2))
        dust_1_100GHz = read_map(get_testdata('benchmark', 'check1therm_100p0_64.fits'), 64, field = (0, 1, 2))
        dust_1_353GHz = read_map(get_testdata('benchmark', 'check1therm_353p0_64.fits'), 64, field = (0, 1, 2))

        self.frac_diff_30GHz = (dust_1_30GHz - signal(30.)) / dust_1_30GHz
        self.frac_diff_100GHz = (dust_1_100GHz - signal(100.)) / dust_1_100GHz
        self.frac_diff_353GHz = (dust_1_353GHz - signal(353.)) / dust_1_353GHz

        d2_config = models("d2", 64)
        dust = components.Dust(d2_config[0])
        signal = dust.signal()
        
        dust_2_30GHz = read_map(get_testdata('benchmark', 'check6therm_30p0_64.fits'), 64, field = (0, 1, 2)) 
        dust_2_100GHz = read_map(get_testdata('benchmark', 'check6therm_100p0_64.fits'), 64, field = (0, 1, 2))
        dust_2_353GHz = read_map(get_testdata('benchmark', 'check6therm_353p0_64.fits'), 64, field = (0, 1, 2))

        self.model_2_frac_diff_30GHz = (dust_2_30GHz - signal(30.)) / dust_1_30GHz
        self.model_2_frac_diff_100GHz = (dust_2_100GHz - signal(100.)) / dust_1_100GHz
        self.model_2_frac_diff_353GHz = (dust_2_353GHz - signal(353.)) / dust_1_353GHz
        
        d3_config = models("d3", 64)
        dust = components.Dust(d3_config[0])
        signal = dust.signal()
        
        dust_3_30GHz = read_map(get_testdata('benchmark', 'check9therm_30p0_64.fits'), 64, field = (0, 1, 2))
        dust_3_100GHz = read_map(get_testdata('benchmark', 'check9therm_100p0_64.fits'), 64, field = (0, 1, 2))
        dust_3_353GHz = read_map(get_testdata('benchmark', 'check9therm_353p0_64.fits'), 64, field = (0, 1, 2))
        
        self.model_3_frac_diff_30GHz = (dust_3_30GHz - signal(30.)) / dust_3_30GHz
        self.model_3_frac_diff_100GHz = (dust_3_100GHz - signal(100.)) / dust_3_100GHz
        self.model_3_frac_diff_353GHz = (dust_3_353GHz - signal(353.)) / dust_3_353GHz

    def test_Dust_model_1(self):
        np.testing.assert_array_almost_equal(self.frac_diff_30GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.frac_diff_100GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.frac_diff_353GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)


    def test_Dust_model_2(self):
        np.testing.assert_array_almost_equal(self.model_2_frac_diff_30GHz, np.zeros_like(self.model_2_frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.model_2_frac_diff_100GHz, np.zeros_like(self.model_2_frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.model_2_frac_diff_353GHz, np.zeros_like(self.model_2_frac_diff_30GHz), decimal = 6)

    def test_Dust_model_3(self):
        np.testing.assert_array_almost_equal(self.model_3_frac_diff_30GHz, np.zeros_like(self.model_3_frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.model_3_frac_diff_100GHz, np.zeros_like(self.model_3_frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.model_3_frac_diff_353GHz, np.zeros_like(self.model_3_frac_diff_30GHz), decimal = 6)

class test_Synchrotron(unittest.TestCase):
    def setUp(self):
        data_dir = get_template_dir()
        
        s1_config = models("s1", 64)
        synchrotron = components.Synchrotron(s1_config[0])
        signal = synchrotron.signal()

        synch_1_30GHz = read_map(get_testdata('benchmark', 'check2synch_30p0_64.fits'), 64, field = (0, 1, 2))
        synch_1_100GHz = read_map(get_testdata('benchmark', 'check2synch_100p0_64.fits'), 64, field = (0, 1, 2))
        synch_1_353GHz = read_map(get_testdata('benchmark', 'check2synch_353p0_64.fits'), 64, field = (0, 1, 2))

        self.model_1_frac_diff = (synch_1_30GHz - signal(30.)) / synch_1_30GHz
        self.model_1_frac_diff = (synch_1_30GHz - signal(30.)) / synch_1_30GHz
        self.model_1_frac_diff = (synch_1_30GHz - signal(30.)) / synch_1_30GHz

        s2_config = models("s2", 64)
        synchrotron = components.Synchrotron(s2_config[0])
        signal = synchrotron.signal()
        
        synch_1_30GHz = read_map(get_testdata('benchmark', 'check7synch_30p0_64.fits'), 64, field = (0, 1, 2))
        synch_1_100GHz = read_map(get_testdata('benchmark', 'check7synch_100p0_64.fits'), 64, field = (0, 1, 2))
        synch_1_353GHz = read_map(get_testdata('benchmark', 'check7synch_353p0_64.fits'), 64, field = (0, 1, 2))

        self.model_2_frac_diff = (synch_2_30GHz - signal(30.)) / synch_2_30GHz
        self.model_2_frac_diff = (synch_2_30GHz - signal(30.)) / synch_2_30GHz
        self.model_2_frac_diff = (synch_2_30GHz - signal(30.)) / synch_2_30GHz

        s3_config = models("s3", 64)
        synchrotron = components.Synchrotron(s3_config[0])
        signal = synchrotron.signal()
        
        synch_3_30GHz = read_map(get_testdata('benchmark', 'check10synch_30p0_64.fits'), 64, field = (0, 1, 2))
        synch_3_100GHz = read_map(get_testdata('benchmark', 'check10synch_100p0_64.fits'), 64, field = (0, 1, 2))
        synch_3_353GHz = read_map(get_testdata('benchmark', 'check10synch_353p0_64.fits'), 64, field = (0, 1, 2))
        
        self.model_1_frac_diff = (synch_3_30GHz - signal(30.)) / synch_1_30GHz
        self.model_1_frac_diff = (synch_3_30GHz - signal(30.)) / synch_1_30GHz
        self.model_1_frac_diff = (synch_3_30GHz - signal(30.)) / synch_1_30GHz

        def test_Synch_model_1(self):
            np.testing.assert_array_almost_equal(self.model_1_frac_diff_30GHz, np.zeros_like(self.model_1_frac_diff_30GHz), decimal = 6)
            np.testing.assert_array_almost_equal(self.model_1_frac_diff_100GHz, np.zeros_like(self.model_1_frac_diff_30GHz), decimal = 6)
            np.testing.assert_array_almost_equal(self.model_1_frac_diff_353GHz, np.zeros_like(self.model_1_frac_diff_30GHz), decimal = 6)

        def test_Synch_model_2(self):
            np.testing.assert_array_almost_equal(self.model_2_frac_diff_30GHz, np.zeros_like(self.model_2_frac_diff_30GHz), decimal = 6)
            np.testing.assert_array_almost_equal(self.model_2_frac_diff_100GHz, np.zeros_like(self.model_2_frac_diff_30GHz), decimal = 6)
            np.testing.assert_array_almost_equal(self.model_2_frac_diff_353GHz, np.zeros_like(self.model_2_frac_diff_30GHz), decimal = 6)

        def test_Synch_model_3(self):
            np.testing.assert_array_almost_equal(self.model_3_frac_diff_30GHz, np.zeros_like(self.model_3_frac_diff_30GHz), decimal = 6)
            np.testing.assert_array_almost_equal(self.model_3_frac_diff_100GHz, np.zeros_like(self.model_3_frac_diff_30GHz), decimal = 6)
            np.testing.assert_array_almost_equal(self.model_3_frac_diff_353GHz, np.zeros_like(self.model_3_frac_diff_30GHz), decimal = 6)
            
class test_AME(unittest.TestCase):
    def setUp(self):
        data_dir = get_template_dir()

        a1_config = models("a1", 64)
        AME1 = components.AME(a1_config[0])
        AME2 = components.AME(a1_config[1])

        signal = lambda nu: AME1.signal()(nu) + AME2.signal()(nu)

        ame_1_30GHz = read_map(get_testdata('benchmark', 'check3spinn_30p0_64.fits'), 64, field = (0, 1, 2)) 
        ame_1_100GHz = read_map(get_testdata('benchmark', 'check3spinn_100p0_64.fits'), 64, field = (0, 1, 2))
        ame_1_353GHz = read_map(get_testdata('benchmark', 'check3spinn_353p0_64.fits'), 64, field = (0, 1, 2))
        
        self.frac_diff_30GHz = (ame_1_30GHz[0] - signal(30.)[0]) / (ame_1_30GHz[0] + 1.e-14)
        self.frac_diff_100GHz = (ame_1_100GHz[0] - signal(100.)[0]) / (ame_1_100GHz[0] + 1e-14)
        self.frac_diff_353GHz = (ame_1_353GHz[0] - signal(353.)[0]) / (ame_1_353GHz[0] +1e-14)

    def tearDown(self):
        self.frac_diff_30GHz = None
        self.frac_diff_100GHz = None
        self.frac_diff_353GHz = None
        
    def test_AME_model_1(self):
        np.testing.assert_array_almost_equal(self.frac_diff_30GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.frac_diff_100GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.frac_diff_353GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
                                
class test_Freefree(unittest.TestCase):
    def setUp(self):
        data_dir = get_template_dir()
                
        f1_config = models("f1", 64)

        freefree = components.Freefree(f1_config[0])
        signal = freefree.signal()

        freefree_1_30GHz = read_map(get_testdata('benchmark', 'check4freef_30p0_64.fits'), 64, field = (0, 1, 2))
        freefree_1_100GHz = read_map(get_testdata('benchmark', 'check4freef_100p0_64.fits'), 64, field = (0, 1, 2))
        freefree_1_353GHz = read_map(get_testdata('benchmark', 'check4freef_353p0_64.fits'), 64, field = (0, 1, 2))
                        
        self.frac_diff_30GHz = (freefree_1_30GHz[0] - signal(30.)[0]) / (freefree_1_30GHz[0] + 1.e-14)
        self.frac_diff_100GHz = (freefree_1_100GHz[0] - signal(100.)[0]) / (freefree_1_100GHz[0] + 1e-14)
        self.frac_diff_353GHz = (freefree_1_353GHz[0] - signal(353.)[0]) / (freefree_1_353GHz[0] +1e-14)

    def tearDown(self):
        self.frac_diff_30GHz = None
        self.frac_diff_100GHz = None
        self.frac_diff_353GHz = None

    def test_Freefree_model_1(self):
        np.testing.assert_array_almost_equal(self.frac_diff_30GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.frac_diff_100GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
        np.testing.assert_array_almost_equal(self.frac_diff_353GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)

class test_models_partial_sky(unittest.TestCase):
    """All models have same implementation, just testing freefree"""

    def test_partial_freefree(self):
        pixel_indices = np.arange(10000, 11000, dtype=np.int)
        f1_config = models("f1", 64, pixel_indices=pixel_indices)
        freefree = components.Freefree(f1_config[0])
        signal = freefree.signal()
        freefree_30_T = signal(30.)[0]
        assert len(freefree_30_T) == 1000
        freefree_1_30GHz = read_map(get_testdata('benchmark', 'check4freef_30p0_64.fits'), 64, field = (0,))
        np.testing.assert_array_almost_equal(freefree_30_T, freefree_1_30GHz[pixel_indices], decimal = 3)

    def test_partial_hensley_draine_2017(self):
        pixel_indices = np.arange(10000, 11000, dtype=np.int)
        f1_config = models("d5", 64, pixel_indices=pixel_indices)
        dust = components.Dust(f1_config[0])
        signal = dust.signal()
        dust_30_T = signal(30.)[0]
        assert len(dust_30_T) == 1000

class test_CMB(unittest.TestCase):
        def setUp(self):
            data_dir = get_template_dir()
        
            self.cmb_config_1 = {
                'model' : 'taylens',
                'cmb_specs' : np.loadtxt(os.path.join(data_dir, 'camb_lenspotentialCls.dat'), unpack = True),
                'delens' : False,
                'delensing_ells' : np.loadtxt(os.path.join(data_dir, 'delens_ells.txt')),
                'nside' : 64,
                'cmb_seed' : 1234
            }

            cmb = components.CMB(self.cmb_config_1)
            signal = cmb.signal()

            self.cmb_1_30GHz = read_map(get_testdata('benchmark', 'check5cmb_30p0_64.fits'), 64, field = (0, 1, 2))
            cmb_1_100GHz = read_map(get_testdata('benchmark', 'check5cmb_100p0_64.fits'), 64, field = (0, 1, 2))
            cmb_1_353GHz = read_map(get_testdata('benchmark', 'check5cmb_353p0_64.fits'), 64, field = (0, 1, 2))

            self.frac_diff_30GHz = (self.cmb_1_30GHz - signal(30.)) / self.cmb_1_30GHz
            self.frac_diff_100GHz = (cmb_1_100GHz - signal(100.)) / cmb_1_100GHz
            self.frac_diff_353GHz = (cmb_1_353GHz - signal(353.)) / cmb_1_353GHz

        def tearDown(self):
            self.frac_diff_30GHz = None
            self.frac_diff_100GHz = None
            self.frac_diff_353GHz = None
            
        def test_CMB_model_1(self):
            
            np.testing.assert_array_almost_equal(self.frac_diff_30GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
            np.testing.assert_array_almost_equal(self.frac_diff_100GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)
            np.testing.assert_array_almost_equal(self.frac_diff_353GHz, np.zeros_like(self.frac_diff_30GHz), decimal = 6)

        def test_CMB_partial_sky(self):
            cmb_config_partial_sky = self.cmb_config_1.copy()
            pixel_indices = np.arange(10000, 11000, dtype=np.int)
            cmb_config_partial_sky["pixel_indices"] = pixel_indices
            cmb = components.CMB(cmb_config_partial_sky)
            signal = cmb.signal()
            signal_30_T = signal(30.)[0]
            assert len(signal_30_T) == len(pixel_indices)
            np.testing.assert_array_almost_equal(signal_30_T, self.cmb_1_30GHz[0][pixel_indices], decimal=3)

def main():
    unittest.main()

if __name__ == '__main__':
    main()



================================================
FILE: pysm/test/test_mpi_readmap.py
================================================
"""Run this test with MPI as:

    mpirun -np 4 python test_mpi.py
"""

import numpy as np
import healpy as hp

import pysm
from pysm.nominal import models

import sys

try:
    from mpi4py import MPI
except ImportError:
    print("Skipping MPI test as mpi4py is missing")
    sys.exit(0)

def is_power2(num):
    return num != 0 and ((num & (num - 1)) == 0)

def build_sky_config(pysm_model, nside, pixel_indices=None, mpi_comm=None):
    """Build a PySM sky configuration dict from a model string"""

    sky_components = [
        'synchrotron',
        'dust',
        'freefree',
        'cmb',
        'ame',
    ]

    sky_config = dict()
    for component_model in pysm_model.split(','):
        full_component_name = [
            each for each in sky_components
            if each.startswith(component_model[0])][0]
        sky_config[full_component_name] = \
            models(component_model, nside=nside, pixel_indices=pixel_indices, mpi_comm=mpi_comm)
    return sky_config

def test_mpi_read():

    comm = MPI.COMM_WORLD

    assert is_power2(comm.size), "Run with a number of MPI processes which is power of 2"

    nside = 64
    npix = hp.nside2npix(nside)

    num_local_pixels = npix // comm.size

    if comm.size == 1:
        pixel_indices = None
        comm = None
    else:
        pixel_indices = np.arange(comm.rank     * num_local_pixels,
                                  (comm.rank+1) * num_local_pixels,
                                  dtype=np.int)

    pysm_model = "s3,d7,f1,c1,a2"

    sky_config = build_sky_config(pysm_model, nside, pixel_indices, comm)

    sky = pysm.Sky(sky_config, mpi_comm=comm)


    instrument_bpass = {
        'use_smoothing': False,
        'nside': nside,
        'add_noise': False,
        'use_bandpass': True,
        'channels': [(np.linspace(20, 25, 10), np.ones(10))],
        'channel_names': ['channel_1'],
        'output_units': 'uK_RJ',
        'output_directory': './',
        'output_prefix': 'test',
        'noise_seed': 1234,
        'pixel_indices': pixel_indices
    }

    instrument = pysm.Instrument(instrument_bpass)
    local_map = instrument.observe(sky, write_outputs=False)

    # Run PySM again locally on each process on the full map

    sky_config = build_sky_config(pysm_model, nside)

    sky = pysm.Sky(sky_config, mpi_comm=comm)

    instrument_bpass["pixel_indices"] = None

    instrument = pysm.Instrument(instrument_bpass)
    complete_map = instrument.observe(sky, write_outputs=False)

    if pixel_indices is None:
        pixel_indices = np.arange(npix)

    np.testing.assert_array_almost_equal(
            local_map[0],
            complete_map[0][:, :, pixel_indices])

if __name__ == "__main__":
    test_mpi_read()



================================================
FILE: pysm/test/test_pysm.py
================================================
import unittest
import numpy as np
import healpy as hp
import scipy.constants as constants
import pysm
from pysm.nominal import models, template
import os
from subprocess import call

import pytest

from . import get_testdata

class BandpassTests(unittest.TestCase):
    def setUp(self):
        self.frequencies = np.linspace(0, 1, 100000)
        self.weights = np.ones_like(self.frequencies)
        self.x = lambda x: x
        self.expected_result_1 = 0.5 
        self.sin = lambda x: np.sin(x)
        self.expected_result_2 = (np.cos(0) - np.cos(1))

    def tearDown(self):
        self.uneven_frequencies = None
        self.weights = None
        self.integrated_map = None
        self.analytic_I = None
        self.analytic_Q = None
        self.analytic_U = None
        
    def test_bandpass_1(self):
        np.testing.assert_almost_equal(pysm.pysm.bandpass(self.frequencies, self.weights, self.x) / self.expected_result_1, 1., decimal = 5)

    def test_bandpass_2(self):
        np.testing.assert_almost_equal(pysm.pysm.bandpass(self.frequencies, self.weights, self.sin) / self.expected_result_2, 1., decimal = 5)

class testCheck_Bandpass_Frequencies(unittest.TestCase):
    def setUp(self):
        self.frequencies_uneven = np.logspace(2, 3, 50)
        self.frequencies_even = np.linspace(100, 130, 30)
        return

    def teatDown(self):
        self.freuqencies_uneven = None
        self.frequencies_even = None
        return

    def test_check_bandpass_frequencies(self):
        with self.assertRaises(SystemExit):
            pysm.pysm.check_bpass_frequencies(self.frequencies_uneven)
        
@pytest.mark.xfail(True, reason="The test file is not created by any routines")
class TestNoise(unittest.TestCase):
    def setUp(self):
        self.nside = 1024
        sigma_T = 4.
        sigma_P = np.sqrt(2.) * sigma_T
        self.instrument_config = {
                'frequencies' : np.array([23.]),
                'sens_I' : np.array([sigma_T]),
                'sens_P' : np.array([sigma_P]),
                'nside' : self.nside,
                'noise_seed' : 1234,
                'use_bandpass' : False,
                'add_noise' : True,
                'output_units' : 'uK_CMB',
                'use_smoothing' : False,
                'output_directory' : os.path.dirname(os.path.abspath(__file__)),
                'output_prefix' : 'test',
            }
        s1 = models("s1", self.nside)
        s1[0]['A_I'] = np.zeros(hp.nside2npix(self.nside))
        s1[0]['A_Q'] = np.zeros(hp.nside2npix(self.nside))
        s1[0]['A_U'] = np.zeros(hp.nside2npix(self.nside))
        sky_config = {'synchrotron' : s1}
        self.sky = pysm.Sky(sky_config)
        
        pix2amin = np.sqrt(4. * np.pi * (180. / np.pi * 60.) ** 2 / float(hp.nside2npix(self.nside)))

        self.expected_T_std = sigma_T / pix2amin
        self.expected_P_std = sigma_P / pix2amin
        self.test_file = get_testdata("test_nu0023p00GHz_noise_nside%04d.fits"%self.nside)

    def tearDown(self):
        try:
            os.remove(self.test_file)
        except: # exception is different on different Python versions
            pass
        
    def test_noise(self):
        instrument = pysm.Instrument(self.instrument_config)
        instrument.observe(self.sky)
        T, Q, U = pysm.read_map(self.test_file, self.nside, field = (0, 1, 2))
        T_std = np.std(T)
        Q_std = np.std(Q)
        U_std = np.std(U)

        np.testing.assert_almost_equal(T_std, self.expected_T_std, decimal = 2)
        np.testing.assert_almost_equal(Q_std, self.expected_P_std, decimal = 2)
        np.testing.assert_almost_equal(U_std, self.expected_P_std, decimal = 2)
        
    def test_noise_partialsky(self):
        local_instrument_config = self.instrument_config.copy()
        local_instrument_config["pixel_indices"] = np.arange(20000, dtype=np.int)
        instrument = pysm.Instrument(local_instrument_config)
        noise = instrument.noiser()

        assert noise[0].shape == (3, len(local_instrument_config["pixel_indices"]))
        np.testing.assert_almost_equal(np.std(noise[0][0]), self.expected_T_std, decimal = 2)
        np.testing.assert_almost_equal(np.std(noise[0][1]), self.expected_P_std, decimal = 2)
        np.testing.assert_almost_equal(np.std(noise[0][2]), self.expected_P_std, decimal = 2)

    def test_noise_write_partialsky(self):
        local_instrument_config = self.instrument_config.copy()
        npix = 20000
        local_instrument_config["pixel_indices"] = np.arange(npix, dtype=np.int)
        instrument = pysm.Instrument(local_instrument_config)
        s1 = models("s1", self.nside, pixel_indices=local_instrument_config["pixel_indices"])
        s1[0]['A_I'] = np.zeros(npix)
        s1[0]['A_Q'] = np.zeros(npix)
        s1[0]['A_U'] = np.zeros(npix)
        sky_config = {'synchrotron' : s1}
        partial_sky = pysm.Sky(sky_config)
        instrument.observe(partial_sky)
        # use masked array to handle partial sky
        T, Q, U = hp.ma(pysm.read_map(self.test_file, self.nside, field = (0, 1, 2)))
        T_std = np.ma.std(T)
        Q_std = np.ma.std(Q)
        U_std = np.ma.std(U)

        np.testing.assert_almost_equal(T_std, self.expected_T_std, decimal = 2)
        np.testing.assert_almost_equal(Q_std, self.expected_P_std, decimal = 2)
        np.testing.assert_almost_equal(U_std, self.expected_P_std, decimal = 2)

class TestSmoothing(unittest.TestCase):

    def setUp(self):

        nside = 64
        self.sky_config = {
            'synchrotron' : models("s1", nside)
            }
        self.synch_1_30GHz = pysm.read_map(get_testdata('benchmark', 'check2synch_30p0_64.fits'), 64, field =(0,1,2))[np.newaxis, :, :]
        self.synch_1_30GHz_smoothed = pysm.read_map(get_testdata('benchmark', 'check2synch_30p0_64_smoothed1deg.fits'), 64, field =0)
        self.instrument_config = {
            'frequencies' : np.array([30., 30.]),
            'beams' : np.array([60., 60.]),
            'nside' : nside,
            'add_noise' : False,
            'output_units' : 'uK_RJ',
            'use_smoothing' : True,
            'use_bandpass' : False,
        }


    def test_no_smoothing(self):
        instrument_config = self.instrument_config
        instrument_config['use_smoothing'] = False
        instrument = pysm.Instrument(instrument_config)
        smoothed = instrument.smoother(self.synch_1_30GHz)
        np.testing.assert_almost_equal(smoothed, self.synch_1_30GHz, decimal=6)

    def test_smoothing(self):
        instrument_config = self.instrument_config
        instrument = pysm.Instrument(instrument_config)
        smoothed = instrument.smoother(self.synch_1_30GHz)
        np.testing.assert_almost_equal(smoothed[0][0], self.synch_1_30GHz_smoothed, decimal=3)

    def test_smoothing_partial_sky(self):
        """Smoothing on a partial sky sets the UNSEEN pixels to zero, so take a large fraction of the sky and check
        only close to the galactic plane"""
        pixel_indices = np.arange(10000, 30000, dtype=np.int)
        instrument_config = self.instrument_config
        instrument_config["pixel_indices"] = pixel_indices
        instrument = pysm.Instrument(instrument_config)
        smoothed = instrument.smoother(self.synch_1_30GHz[..., pixel_indices])
        np.testing.assert_almost_equal(smoothed[0, 0, 10000:10100], self.synch_1_30GHz_smoothed[20000:20100], decimal=1)

def main():
    unittest.main()

if __name__ == "__main__":
    main()





================================================
FILE: Utils_ILC/__init__.py
================================================
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from .smoothingTo import smoothingTo
from .ILC.ILCbase import pixelILC, harmonicILC, NILC, LPILC, PILC




================================================
FILE: Utils_ILC/smoothingTo.py
================================================
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import healpy as hp
import numpy as np

def smoothingTo(inputMap, oriBeam, destBeam):
    '''
    Edited by Si-Yu Li. 2022-05-18
    '''
    npix = np.shape(inputMap)[-1]
    nside = hp.nside2npix(npix)
    lmax = 3*nside+1

    assert(np.ndim(inputMap)<=2)

    haspol = np.ndim(inputMap) == 2

    if not hasattr(oriBeam, '__iter__'):
        oriBeam = float(oriBeam)
        oriBl = hp.gauss_beam(oriBeam, lmax=lmax, pol=haspol)
    else:
        oriBl = oriBeam
    
    if not hasattr(destBeam, '__iter__'):
        destBeam = float(destBeam)
        destBl = hp.gauss_beam(destBeam, lmax=lmax, pol=haspol)
    else:
        destBl = destBeam
    
    ori_lmax_bl = np.shape(oriBl)[-1]
    dest_lmax_bl = np.shape(destBl)[-1]

    lmax = min(ori_lmax_bl, dest_lmax_bl)

    Bl = destBl[:lmax+1] / oriBl[:lmax+1]

    res = hp.smoothing(inputMap, beam_window=Bl, lmax=lmax, pol=haspol)

    return res

#def smoothingTo(inputMap, oriBeam, destBeam):
#    '''in radians'''
#    newfwhm = np.sqrt(destBeam**2 - oriBeam**2)
#    res = hp.smoothing(inputMap, newfwhm)
#    return res

if __name__ == '__main__':
    nside = 128
    npix = hp.nside2npix(nside)
    m = np.random.normal(0,100, npix)
    m1 = hp.smoothing(m, np.deg2rad(0.5))
    m2 = smoothingTo(m1, np.deg2rad(0.5), np.deg2rad(1))

    m3 = hp.smoothing(m, np.deg2rad(1))
    r = hp.anafast(m2)
    r2 = hp.anafast(m3)
    import matplotlib.pyplot as plt
    plt.plot(r)
    plt.plot(r2)
    plt.show()



================================================
FILE: Utils_ILC/ILC/__init__.py
================================================
#!/usr/bin/env python
# -*- coding: utf-8 -*-





================================================
FILE: Utils_ILC/ILC/ILCbase.py
================================================
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import numpy as np
from .. import smoothingTo
import healpy as hp
import os
import pandas as pd
import pymaster as nmt
eps = 1e-15
def myinv(a):
    if np.shape(a) == 2:
        return np.linalg.inv(a)
    res = np.zeros_like(a)
    for i in range(len(a)):
        try:
            res[i] = np.linalg.inv(a[i])
        except:
            pass
    return res

class baseILC:

    def __init__(self, maps, init_beams, final_beam, *, mask=None):
        '''
        Edited by Si-Yu Li.
        beams are in arcmin
        '''
        self.npix = maps.shape[-1]
        self.nside = hp.npix2nside(self.npix)
        self.nmap = maps.shape[0]
        
        if np.ndim(maps) == 3:
            self.ncomp = maps.shape[1]
            pol = True
        else:
            self.ncomp = 1
            pol = False

        # print('ILC instance initialization')
        # print('nmap:', self.nmap)
        # print('npix:', self.npix)
        # print('ncomp:', self.ncomp)
        
        #init_beams = np.deg2rad(init_beams)/60
        #final_beam = np.deg2rad(final_beam)/60
        #diff_beams = init_beams - final_beam
        #is_smooth = ~(np.round(diff_beams, 6) == 0)

        if mask is None:
            mask = np.ones(self.npix)
        self.mask = mask
        new_map = maps*mask

        for i in range(self.nmap):

            m = maps[i]
            inpbeam = init_beams[i]
            
            inpbeam_isarr = hasattr(inpbeam, '__iter__')
            outbeam_isarr = hasattr(final_beam, '__iter__')
        
            outbeam = np.deg2rad(final_beam/60) if not outbeam_isarr else final_beam
            if not inpbeam_isarr: inpbeam = np.deg2rad(inpbeam/60)

            if inpbeam_isarr and not outbeam_isarr:
                ndim = np.ndim(inpbeam)
                lmax = np.shape(inpbeam)[-1]-1
                if ndim == 1:
                    outbeam = hp.gauss_beam(outbeam, lmax=lmax)
                elif ndim == 2:
                    ncomp = np.shape(inpbeam)
                    if ncomp == 1:
                        inpbeam = inpbeam[0]
                        outbeam = hp.gauss_beam(outbeam, lmax=lmax)
                    elif ncomp == 4:
                        outbeam = hp.gauss_beam(outbeam, lmax=lmax, pol=True)
                    else:
                        raise ValueError('Inp or Out beam has wrong shape.')
                else:
                    raise ValueError('Inp beam has wrong dimension.')
            elif not inpbeam_isarr and outbeam_isarr:
                ndim = np.ndim(outbeam)
                lmax = np.shape(outbeam)[-1]-1
                if ndim == 1:
                    inpbeam = hp.gauss_beam(inpbeam, lmax=lmax)
                elif ndim == 2:
                    ncomp = np.shape(outbeam)
                    if ncomp == 1:
                        outbeam = outbeam[0]
                        inpbeam = hp.gauss_beam(inpbeam, lmax=lmax)
                    elif ncomp == 4:
                        inpbeam = hp.gauss_beam(inpbeam, lmax=lmax, pol=True)
                    else:
                        raise ValueError('Inp or Out beam has wrong shape.')
                else:
                    raise ValueError('Out beam has wrong dimension.')
            
            if inpbeam_isarr or outbeam_isarr:
                assert(np.ndim(inpbeam) == np.ndim(outbeam))
                if np.all(np.isclose(inpbeam, outbeam, rtol=1e-3)):
                    continue
            else:
                if np.isclose(inpbeam, outbeam, rtol=1e-3): continue
            
            new_map[i] = smoothingTo(new_map[i], inpbeam, outbeam)
            
        #for index, flag in enumerate(is_smooth):
        #    if not flag:
        #        continue
        #    new_map[index] = smoothingTo(new_map[index], init_beams[index],
        #                                 final_beam)
        self.maps = new_map
        binmask = (mask != 0)
        self.binmask = binmask
        self.maps[...,~binmask] = 0
        self.final_beam = final_beam

    def map2ilc(self, **kwargs):
        pass

    def ilc2map(self, **kwargs):
        pass

    def calc_weight(self, **kwargs):
        pass

    def do_ilc(self, **kwargs):
        self.final_map = None
        self.map2ilc(self.maps, **kwargs)
        if not hasattr(self, 'weights'):
            self.calc_weight()
        self.final_map = self.ilc2map()
        self.final_map[...,~self.binmask] = 0
        return self.final_map

    def set_weight(self, _ilcbase):
        if hasattr(_ilcbase, 'weights'):
            self.weights = _ilcbase.weights
        else:
            self.weights = _ilcbase


class pixelILC(baseILC):
    def __init__(self, maps, init_beams, final_beam, *, mask=None):
        super().__init__(maps, init_beams, final_beam, mask=mask)

    def map2ilc(self, maps, attr='ilcs', **kwargs):
        mask = self.binmask
        setattr(self, attr, maps[...,mask])

    def ilc2map(self, attr='ilcs'):
        final_map = np.zeros(self.npix)
        final_map[self.binmask] = self.weights @ getattr(self, attr)
        return final_map

    def calc_weight(self):
        R = np.cov(self.ilcs)
        invR = np.linalg.inv(R)
        oneVec = np.ones(self.nmap)
        w = (oneVec@invR)/(oneVec@invR@oneVec)
        self.weights = w

class harmonicILC(baseILC):
    def __init__(self, maps, init_beams, final_beam, *, mask=None):
        # super().__init__(maps, init_beams, final_beam, mask=mask)
        self.npix = maps.shape[-1]
        self.nside = hp.npix2nside(self.npix)
        self.nmap = maps.shape[0]
        if mask is None:
            mask = np.ones(self.npix)
        self.mask = mask
        self.maps = maps*mask
        self.binmask = (mask != 0)
        self.final_beam = np.deg2rad(final_beam)/60

        init_beam_rad = np.deg2rad(init_beams)/60
        self.bl = []
        for i in range(self.nmap):
            self.bl.append(hp.gauss_beam(init_beam_rad[i], 2000, True)[:,1])
    def map2ilc(self, maps, attr='ilcs', mask=None, lmax=400):

        final_bl  = hp.gauss_beam(self.final_beam, 2000, True)[:,1]
        ilcs = []
        for i in range(self.nmap):
            cur_alm = hp.map2alm(maps[i], lmax=lmax)
            cur_alm = hp.almxfl(cur_alm, final_bl/self.bl[i])
            ilcs.append(cur_alm)
        self.lmax = lmax
        setattr(self, attr, ilcs)

    def ilc2map(self):
        final_alm = 0
        for i in range(self.nmap):
            weighted_alm = hp.almxfl(self.ilcs[i], self.weights[i])
            final_alm = final_alm + weighted_alm
            # final_alm = final_alm + hp.almxfl(self.ilcs[i], self.weights[i])
        final_map = hp.alm2map(final_alm, self.nside)
        return final_map

    def calc_weight(self):
        R = np.empty((self.lmax+1, self.nmap, self.nmap))
        for i in range(self.nmap):
            for j in range(self.nmap):
                R[:, i, j] = hp.alm2cl(self.ilcs[i], self.ilcs[j])

        #invR = np.linalg.inv(R[2:])
        invR = myinv(R[2:])
        oneVec = np.ones(self.nmap)
        wl_2 = (oneVec@invR).T/(oneVec@invR@oneVec + eps)
        wl = np.zeros((self.nmap, self.lmax + 1))
        wl[:,2:] = wl_2
        self.weights = wl

data_dir =  os.path.join(os.path.dirname(os.path.dirname(__file__)), "needlet_data")
class NILC(baseILC):
    def __init__(self, maps, init_beams, final_beam, *, mask=None):
        super().__init__(maps, init_beams, final_beam, mask=mask)
        self.needlet = pd.read_csv(os.path.join(data_dir, "needlet.csv"))
        self.n_needlet = len(self.needlet)

    def set_needlet(self, path):
        self.needlet = pd.read_csv(path)
        self.n_needlet = len(self.needlet)

    def __calc_hl(self, lmax):
        hl = np.zeros((self.n_needlet, lmax+1))
        for i in range(self.n_needlet):
            nlmax = self.needlet.at[i,'lmax']
            nlmin = self.needlet.at[i,'lmin']
            nlpeak = self.needlet.at[i,'lpeak']
            def funhl(l):
                if l < nlmin or l > nlmax:
                    return 0
                elif l < nlpeak:
                    return np.cos(((nlpeak-l)/(nlpeak-nlmin)) * np.pi/2)
                elif l > nlpeak:
                    return np.cos(((l-nlpeak)/(nlmax-nlpeak)) * np.pi/2)
                else:
                    return 1
            vecHl = np.vectorize(funhl, otypes=[float])
            hl[i] = vecHl(np.arange(lmax+1))
        self.hl = hl

    def map2ilc(self, maps, attr='ilcs', **kwargs):
        lmax = np.max(self.needlet['lmax'])
        #lmax = kwargs.get('lmax', lmax)
        if not hasattr(self, 'hl'):
            self.__calc_hl(lmax)
        hl = self.hl

        alms = []
        for i in range(self.nmap):
            alm = hp.map2alm(maps[i], lmax=lmax)
            alms.append(alm)

        betaList = []
        for j in range(len(self.needlet)):
            curNside = self.needlet.at[j, 'nside']
            curNpix = hp.nside2npix(curNside)
            curBeta = np.zeros((self.nmap, curNpix))
            for i in range(self.nmap):
                curAlm = hp.almxfl(alms[i], hl[j])
                curBeta[i] = hp.alm2map(curAlm, curNside)
            betaList.append(curBeta)

        setattr(self, attr, betaList)

    def __calc_R(self, size = 5):
        betas = self.ilcs
        R = []
        if not hasattr(size, '__len__'):
            size = [size] * self.n_needlet
        for j in range(self.n_needlet):
            curNside = self.needlet.at[j, 'nside']
            curR = np.zeros((hp.nside2npix(curNside), self.nmap, self.nmap))
            for c1 in range(self.nmap):
                for c2 in range(c1+1):
                    #print((c1, c2), end='\t')
                    prodMap = betas[j][c1] * betas[j][c2]
                    RMap = hp.smoothing(prodMap, np.deg2rad(size[j]))
                    curR[:,c1,c2] = RMap
                    curR[:,c2,c1] = RMap
                #print()

            R.append(curR)
        self.R = R
    def calc_weight(self, **kwargs):
        oneVec = np.ones(self.nmap)
        nside = np.array(self.needlet['nside'])
        #size = hp.nside2resol(nside, True) / 60 * 35
        size = 360/(self.needlet['lpeak'] + self.needlet['lmax']) * 10
        #  size = np.arccos(1 - 200/(nside**2))
        #  size = np.rad2deg(size)*0.5
        self.__calc_R(size = size, **kwargs)
        R = self.R
        w = []
        for j in range(self.n_needlet):
            curR = R[j]
            invR = np.linalg.inv(curR)
            curW = (invR@oneVec).T/(oneVec@invR@oneVec)
            w.append(curW)
        self.weights = w

    def ilc2map(self):
        betaNILC = []
        for j in range(self.n_needlet):
            curBeta = self.ilcs[j]
            curW    = self.weights[j]
            curRes  = np.sum(curBeta * curW, axis=0)
            betaNILC.append(curRes)

        resMap = 0
        for j in range(self.n_needlet):
            curAlm = hp.map2alm(betaNILC[j])
            curAlm = hp.almxfl(curAlm, self.hl[j])
            curMap = hp.alm2map(curAlm, self.nside)
            resMap = resMap + curMap
        resMap[~self.binmask] = 0
        return resMap

class LPILC:
    def __init__(self, maps, init_beams, final_beam, *, smoothingmask=None):
        self.npix = maps.shape[-1]
        self.nside = hp.npix2nside(self.npix)
        self.nmap = maps.shape[0]

        if smoothingmask is None:
            smoothingmask = np.ones(self.npix)

        self.init_beams = np.deg2rad(init_beams)/60
        self.final_beam = np.deg2rad(final_beam)/60
        self.smoothed_map = np.zeros_like(maps)
        for i in range(self.nmap):
            self.smoothed_map[i] = smoothingTo(maps[i], self.init_beams[i], self.final_beam)

        self.maps = maps
    def map2ilc(self, maps, mask, attr='ilcs', **kwargs):
        self.ilcmask = mask
        smoothed_mask = nmt.mask_apodization(mask, 3, "C2")
        almes = []
        almbs = []
        for i in range(self.nmap):
            curmap = self.maps[i] * smoothed_mask
            _, alme, almb = hp.map2alm(curmap, lmax=300)
            curbl  = hp.gauss_beam(self.init_beams[i], pol=1)[:,1]
            alme = hp.almxfl(alme, 1/curbl)
            almb = hp.almxfl(almb, 1/curbl)
            almes.append(alme)
            almbs.append(almb)

        almes = np.array(almes)
        almbs = np.array(almbs)
        setattr(self, attr, (almes, almbs))

    def calc_weight(self):
        from scipy.optimize import minimize

        almes, almbs = self.ilcs
        def costFunc(weight):
            wr, wi = np.split(weight, 2)
            outAlmE = wr@almes - wi@almbs
            outAlmB = wr@almbs + wi@almes
            cle = hp.alm2cl(outAlmE)
            clb = hp.alm2cl(outAlmB)
            l = np.arange(cle.size)

            return np.sum(l**2*clb)

        cons = [
            {"type":"eq", "fun":lambda x:np.sum(x[:self.nmap])-1},
            {"type":"eq", "fun":lambda x:np.sum(x[self.nmap:])-0}
        ]
        wGuess = np.ones(2*self.nmap)
        res = minimize(costFunc, wGuess, constraints=cons, callback=lambda x:print(costFunc(x)))

        wr, wi = np.split(res.x, 2)
        self.weights = (wr, wi)

    def ilc2map(self):
        q = self.smoothed_map[:,1,:]
        u = self.smoothed_map[:,2,:]
        wr, wi = self.weights
        xplus = q + 1j*u
        cleanedXplus = (wr+1j*wi)@xplus
        cleanedQ = np.real(cleanedXplus)
        cleanedU = np.imag(cleanedXplus)
        return cleanedQ, cleanedU

    def do_ilc(self, **kwargs):
        self.final_map = None
        self.map2ilc(self.maps, **kwargs)
        if not hasattr(self, 'weights'):
            self.calc_weight()
        self.final_map = self.ilc2map()
        self.final_map[0][~self.ilcmask] = 0
        self.final_map[1][~self.ilcmask] = 0
        return self.final_map

    def set_weight(self, _ilcbase):
        self.weights = _ilcbase.weights

class PILC(baseILC):
    @staticmethod
    def shiftqu(q, u):
        meanq = np.mean(q, 1)
        meanu = np.mean(u, 1)

        shiftedq = q - meanq[...,None]
        shiftedu = u - meanu[...,None]
        return shiftedq, shiftedu

    def __init__(self, maps, init_beams, final_beam, *, mask=None):
        super().__init__(maps, init_beams, final_beam, mask=mask)

    def map2ilc(self, maps, attr='ilcs', **kwargs):
        mask = self.mask
        setattr(self, attr, maps[:,1:])

    def calc_weight(self):
        #q, u = self.shiftqu(self.ilcs[:,0], self.ilcs[:,1])
        q, u = self.ilcs[:,0], self.ilcs[:,1]
        n, npix = self.nmap, self.npix

        cplus  = (q@q.T + u@u.T)/npix
        cminus = (q@u.T - u@q.T)/npix

        C = np.block([[cplus, -cminus], [cminus, cplus]])
        invC = np.linalg.inv(C)

        splus  = np.sum(invC[:n, :n])
        sminus = np.sum(invC[:n, n:])

        lambdaR = 2 * splus/(splus**2 + sminus**2)
        lambdaI = 2 * sminus/(splus**2 + sminus**2)

        wr = np.sum(invC[:n,:n], axis=0) * lambdaR / 2 + np.sum(invC[:n,n:], axis=0) * lambdaI / 2
        wi = np.sum(invC[n:,:n], axis=0) * lambdaR / 2 + np.sum(invC[n:,n:], axis=0) * lambdaI / 2
        self.weights = wr, wi

    def ilc2map(self, attr='ilcs'):
        wr, wi = self.weights
        xplus = self.ilcs[:,0] + 1j*self.ilcs[:,1]
        cleanedXplus = (wr+1j*wi)@xplus
        cleanedQ = np.real(cleanedXplus)
        cleanedU = np.imag(cleanedXplus)
        return np.vstack((cleanedQ, cleanedU))



================================================
FILE: Utils_ILC/ILC/needlet_data/needlet.csv
================================================
Band Index,lmin,lpeak,lmax,nside
1         ,0   ,0    ,50  ,32
2         ,0   ,50   ,100 ,64
3         ,50  ,100  ,150 ,128
4         ,100 ,150  ,250 ,128
5         ,150 ,250  ,350 ,256
6         ,250 ,350  ,550 ,512
7         ,350 ,550  ,650 ,512
8         ,550 ,650  ,800 ,1024
9         ,650 ,800  ,1000,1024



